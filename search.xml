<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[【分布式训练】单机多卡的正确打开方式（三）：PyTorch]]></title>
    <url>%2F2019%2F07%2F23%2Fdistributed-training3%2F</url>
    <content type="text"><![CDATA[拖更拖更了，今天讲一下PyTorch下要如何单机多卡训练。 PyTorch的数据并行相对于TensorFlow而言，要简单的多，主要分成两个API： DataParallel（DP）：Parameter Server模式，一张卡位reducer，实现也超级简单，一行代码。 DistributedDataParallel（DDP）：All-Reduce模式，本意是用来分布式训练，但是也可用于单机多卡。 1. DataParallelDataParallel是基于Parameter server的算法，负载不均衡的问题比较严重，有时在模型较大的时候（比如bert-large），reducer的那张卡会多出3-4g的显存占用。 先简单定义一下数据流和模型。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import torchimport torch.nn as nnfrom torch.autograd import Variablefrom torch.utils.data import Dataset, DataLoaderimport osinput_size = 5output_size = 2batch_size = 30data_size = 30class RandomDataset(Dataset): def __init__(self, size, length): self.len = length self.data = torch.randn(length, size) def __getitem__(self, index): return self.data[index] def __len__(self): return self.lenrand_loader = DataLoader(dataset=RandomDataset(input_size, data_size), batch_size=batch_size, shuffle=True)class Model(nn.Module): # Our model def __init__(self, input_size, output_size): super(Model, self).__init__() self.fc = nn.Linear(input_size, output_size) def forward(self, input): output = self.fc(input) print(" In Model: input size", input.size(), "output size", output.size()) return outputmodel = Model(input_size, output_size)if torch.cuda.is_available(): model.cuda() if torch.cuda.device_count() &gt; 1: print("Let's use", torch.cuda.device_count(), "GPUs!") # 就这一行 model = nn.DataParallel(model) for data in rand_loader: if torch.cuda.is_available(): input_var = Variable(data.cuda()) else: input_var = Variable(data) output = model(input_var) print("Outside: input size", input_var.size(), "output_size", output.size()) 2. DDP官方建议用新的DDP，采用all-reduce算法，本来设计主要是为了多机多卡使用，但是单机上也能用，使用方法如下： 初始化使用nccl后端1torch.distributed.init_process_group(backend=&quot;nccl&quot;) 模型并行化1model=torch.nn.parallel.DistributedDataParallel(model) 需要注意的是：DDP并不会自动shard数据 如果自己写数据流，得根据torch.distributed.get_rank()去shard数据，获取自己应用的一份 如果用Dataset API，则需要在定义Dataloader的时候用DistributedSampler 去shard：12sampler = DistributedSampler(dataset) # 这个sampler会自动分配数据到各个gpu上DataLoader(dataset, batch_size=batch_size, sampler=sampler) 完整的例子：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import torchimport torch.nn as nnfrom torch.autograd import Variablefrom torch.utils.data import Dataset, DataLoaderimport osfrom torch.utils.data.distributed import DistributedSampler# 1) 初始化torch.distributed.init_process_group(backend="nccl")input_size = 5output_size = 2batch_size = 30data_size = 90# 2） 配置每个进程的gpulocal_rank = torch.distributed.get_rank()torch.cuda.set_device(local_rank)device = torch.device("cuda", local_rank)class RandomDataset(Dataset): def __init__(self, size, length): self.len = length self.data = torch.randn(length, size).to('cuda') def __getitem__(self, index): return self.data[index] def __len__(self): return self.lendataset = RandomDataset(input_size, data_size)# 3）使用DistributedSamplerrand_loader = DataLoader(dataset=dataset, batch_size=batch_size, sampler=DistributedSampler(dataset))class Model(nn.Module): def __init__(self, input_size, output_size): super(Model, self).__init__() self.fc = nn.Linear(input_size, output_size) def forward(self, input): output = self.fc(input) print(" In Model: input size", input.size(), "output size", output.size()) return output model = Model(input_size, output_size)# 4) 封装之前要把模型移到对应的gpumodel.to(device) if torch.cuda.device_count() &gt; 1: print("Let's use", torch.cuda.device_count(), "GPUs!") # 5) 封装 model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank) for data in rand_loader: if torch.cuda.is_available(): input_var = data else: input_var = data output = model(input_var) print("Outside: input size", input_var.size(), "output_size", output.size()) 需要通过命令行启动 1CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --nproc_per_node=2 torch_ddp.py 结果：12345678910Let&apos;s use 2 GPUs!Let&apos;s use 2 GPUs! In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])Outside: input size torch.Size([15, 5]) output_size torch.Size([15, 2]) In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])Outside: input size torch.Size([15, 5]) output_size torch.Size([15, 2]) 可以看到有两个进程，log打印了两遍 torch.distributed.launch 会给模型分配一个args.local_rank的参数，也可以通过torch.distributed.get_rank()获取进程id]]></content>
      <categories>
        <category>训练方法</category>
        <category>分布式训练</category>
      </categories>
      <tags>
        <tag>分布式训练</tag>
        <tag>多卡训练</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【分布式训练】单机多卡的正确打开方式（二）：TensorFlow]]></title>
    <url>%2F2019%2F07%2F14%2Fdistributed-training2%2F</url>
    <content type="text"><![CDATA[瓦砾上一篇讲了单机多卡分布式训练的一些入门介绍，后面几篇准备给大家讲讲TensorFlow、PyTorch框架下要怎么实现多卡训练。 这一篇就介绍一下TensorFlow上的分布式训练，尽管从传统的Custom Training Loops到Estimator再到Keras，TF的API换来换去让人猝不及防，但是由于种种原因，TensorFlow还是业务上最成熟的框架，所以Let’s还是do it。（没看过上一篇的读者建议先看一下原理部分：分布式训练的正确打开方式（一）：理论基础，因为算法的理论理解对于后面API的理解还是很重要的。） 这篇博客主要介绍TensorFlow在1.13版本里发布的tf.distribute API，集成了之前tf.contrib.distribute的很多功能，并且大大的简化了使用。官方很良心的放了Google Colab，想要一步步执行看结果的读者可以移步官方教学。 Overviewtf.distribute的核心API是tf.distribute.Strategy，可以简简单单几行代码就实现单机多卡，多机多卡等情况的分布式训练。主要有这几个优势： 简单易用，开箱即用，高性能。 便于各种分布式Strategy切换。 支持Custom Training Loop、Estimator、Keras。 支持eager excution。 123import osos.environ["CUDA_VISIBLE_DEVICES"]="0,1"import tensorflow as tf Strategy的类别tf.distribute.Strategy设计的初衷是能cover不同维度的use cases，目前主要有四个Strategy： MirroredStrategy CentralStorageStrategy MultiWorkerMirroredStrategy ParameterServerStrategy 还有一些策略，例如异步训练等等，后面会逐步支持。 1. MirroredStrategy镜像策略用于单机多卡 数据并行 同步更新的情况，在每个GPU上保存一份模型副本，模型中的每个变量都镜像在所有副本中。这些变量一起形成一个名为MirroredVariable的概念变量。通过apply相同的更新，这些变量保持彼此同步。 镜像策略用了高效的All-reduce算法来实现设备之间变量的传递更新。默认情况下，它使用NVIDIA NCCL作为all-reduce实现。用户还可以在官方提供的其他几个选项之间进行选择。 最简单的创建一个镜像策略的方法： 1mirrored_strategy = tf.distribute.MirroredStrategy() 也可以自己定义要用哪些devices： 1mirrored_strategy = tf.distribute.MirroredStrategy(devices=["/gpu:0", "/gpu:1"]) 官方也提供了其他的一些all-reduce实现： tf.distribute.CrossDeviceOps tf.distribute.HierarchicalCopyAllReduce tf.distribute.ReductionToOneDevice tf.distribute.NcclAllReduce (default) 12mirrored_strategy = tf.distribute.MirroredStrategy( cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()) 2. CentralStorageStrategy中央存储策略，参数被统一存在CPU里，然后复制到所有GPU上，优点是GPU负载均衡了，但是一般情况下CPU和GPU通信代价大，不建议使用。 1central_storage_strategy = tf.distribute.experimental.CentralStorageStrategy() 3. MultiWorkerMirroredStrategy这个API和MirroredStrategy很类似，是其多机多卡分布式的版本，由于我们主要是介绍单机多卡，这里就不展开讲了。 1multiworker_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() 4. ParameterServerStrategy这个API呢，就是被大家普遍嫌弃即将淘汰的PS策略，慢+负载不均衡。（和all-reduce的区别，请看上一篇） 1ps_strategy = tf.distribute.experimental.ParameterServerStrategy() tf.distribute.Strategy在三种API上的使用：Keras、Estimator、Custom Training Loops1. Keras123456789101112# 这里的Strategy可以换成想用的，因为其他三个还是experimental的状态，建议用这个mirrored_strategy = tf.distribute.MirroredStrategy()with mirrored_strategy.scope(): # 定义模型的时候放到镜像策略空间就行 model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))]) model.compile(loss='mse', optimizer='sgd')# 手动做个假数据跑一下dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(1000).batch(10)print('Train:')model.fit(dataset, epochs=2)print('\nEval:')model.evaluate(dataset) 2. Estimator1234567891011121314mirrored_strategy = tf.distribute.MirroredStrategy()# 在config中加入镜像策略config = tf.estimator.RunConfig(train_distribute=mirrored_strategy, eval_distribute=mirrored_strategy)# 把config加到模型里regressor = tf.estimator.LinearRegressor( feature_columns=[tf.feature_column.numeric_column('feats')], optimizer='SGD', config=config)def input_fn(): dataset = tf.data.Dataset.from_tensors((&#123;"feats":[1.]&#125;, [1.])) return dataset.repeat(1000).batch(10)# 正常训练，正常评估regressor.train(input_fn=input_fn, steps=10)regressor.evaluate(input_fn=input_fn, steps=10) 3. Custom Training Loops123456789101112131415161718192021222324252627282930313233343536mirrored_strategy = tf.distribute.MirroredStrategy()# 在mirrored_strategy空间下with mirrored_strategy.scope(): model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))]) optimizer = tf.train.GradientDescentOptimizer(0.1)# 在mirrored_strategy空间下with mirrored_strategy.scope(): dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(1000).batch(global_batch_size) print(dataset) # 这里要分发一下数据 dist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset) print(dist_dataset.__dict__['_cloned_datasets'])def train_step(dist_inputs): def step_fn(inputs): features, labels = inputs logits = model(features) cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2( logits=logits, labels=labels) loss = tf.reduce_sum(cross_entropy) * (1.0 / global_batch_size) train_op = optimizer.minimize(loss) with tf.control_dependencies([train_op]): return tf.identity(loss) # 返回所有gpu的loss per_replica_losses = mirrored_strategy.experimental_run_v2(step_fn, args=(dist_inputs,)) # reduce loss并返回 mean_loss = mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None) return mean_losswith mirrored_strategy.scope(): input_iterator = dist_dataset.make_initializable_iterator() iterator_init = input_iterator.initialize() var_init = tf.global_variables_initializer() loss = train_step(input_iterator.get_next()) with tf.Session() as sess: sess.run([var_init, iterator_init]) for _ in range(2): print(sess.run(loss))]]></content>
      <categories>
        <category>训练方法</category>
        <category>分布式训练</category>
      </categories>
      <tags>
        <tag>分布式训练</tag>
        <tag>多卡训练</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【分布式训练】单机多卡的正确打开方式（一）：理论基础]]></title>
    <url>%2F2019%2F07%2F08%2Fdistributed-training%2F</url>
    <content type="text"><![CDATA[瓦砾由于最近bert-large用的比较多，踩了很多分布式训练的坑，加上在TensorFlow和PyTorch之间更换，算是熟悉了一下各类框架的分布式训练接口，由于集中在一起讲可能比较乱，笔者准备分三到四篇来讲一下深度学习的分布式训练。这一篇先讲一下“分布式训练的类型与算法”。 分布式训练的需求和重要性不需要多说，随着GPT、BERT、xlnet这些预训练模型的出现，普通的16G的显存已经不足以支撑深度学习模型训练的要求了，这时候就需要用到分布式训练来提高效率。 注意：这个系列主要介绍单机多卡的分布式训练情况（这种情况比较常见，土豪和大佬们请忽略）。 总的来说，分布式训练分为这几类： 按照并行方式来分：模型并行 vs 数据并行 按照更新方式来分：同步更新 vs 异步更新 按照算法来分：Parameter Server算法 vs AllReduce算法 模型并行 vs 数据并行假设我们有n张GPU： 模型并行：不同的GPU输入相同的数据，运行模型的不同部分，比如多层网络的不同层； 数据并行：不同的GPU输入不同的数据，运行相同的完整的模型。 当模型非常大，一张GPU已经存不下的时候，可以使用模型并行，把模型的不同部分交给不同的机器负责，但是这样会带来很大的通信开销，而且模型并行各个部分存在一定的依赖，规模伸缩性差。因此，通常一张可以放下一个模型的时候，会采用数据并行的方式，各部分独立，伸缩性好。 同步更新 vs 异步更新对于数据并行来说，由于每个GPU负责一部分数据，那就涉及到如果更新参数的问题，分为同步更新和异步更新两种方式。 同步更新：每个batch所有GPU计算完成后，再统一计算新权值，然后所有GPU同步新值后，再进行下一轮计算。 异步更新：每个GPU计算完梯度后，无需等待其他更新，立即更新整体权值并同步。 同步更新有等待，速度取决于最慢的那个GPU；异步更新没有等待，但是涉及到更复杂的梯度过时，loss下降抖动大的问题。所以实践中，一般使用同步更新的方式。 Parameter Server算法 vs Ring AllReduce算法这里讲一下常用的两种参数同步的算法：PS 和 Ring AllReduce。 假设有5张GPU： Parameter Server：GPU 0将数据分成五份分到各个卡上，每张卡负责自己的那一份mini-batch的训练，得到grad后，返回给GPU 0上做累积，得到更新的权重参数后，再分发给各个卡。 Ring AllReduce：5张以环形相连，每张卡都有左手卡和右手卡，一个负责接收，一个负责发送，循环4次完成梯度累积，再循环4次做参数同步。分为Scatter Reduce和All Gather两个环节。 Parameter Server的思想其实有点类似于MapReduce，以上讲同步异步的时候，都是用的这种算法，但是它存在两个缺点： 每一轮的训练迭代都需要所有卡都将数据同步完做一次Reduce才算结束，并行的卡很多的时候，木桶效应就会很严重，计算效率低。 所有的GPU卡需要和Reducer进行数据、梯度和参数的通信，当模型较大或者数据较大的时候，通信开销很大。 假设有N个GPU，通信一次完整的参数所需时间为K，那么使用PS架构，花费的通信成本为：$$T = 2(N-1)K$$所以我们亟需一种新的算法来提高深度学习模型训练的并行效率。 2017 年 Facebook 发布了《Accurate, large minibatch SGD: Training ImageNet in 1 hour 》验证了大数据并行的高效性，同年百度发表了《Bringing HPC techniques to deep learning 》，验证了全新的梯度同步和权值更新算法的可行性，并提出了一种利用带宽优化环解决通信问题的方法——Ring AllReduce。 Parameter Service最大的问题就是通信成本和GPU的数量线性相关。而Ring AllReduce的通信成本与GPU数量无关。Ring AllReduce分为两个步骤：Scatter Reduce和All Gather。 Scatter Reduce过程：首先，我们将参数分为N份，相邻的GPU传递不同的参数，在传递N-1次之后，可以得到每一份参数的累积（在不同的GPU上）。 All Gather：得到每一份参数的累积之后，再做一次传递，同步到所有的GPU上。 根据这两个过程，我们可以计算到All Reduce的通信成本为：$$T = 2(N-1)\frac{K}{N}$$可以看到通信成本T与GPU数量无关。 由于All Reduce算法在通信成本上的优势，现在几个框架基本上都实现了其对于的官方API，后面几篇，瓦砾会跟大家一起过一遍TF，Torch的分布式训练API具体是怎么用的，有哪些坑。 Reference 是时候放弃Tensorflow，拥抱Horovod了 Tensorflow单机多卡实现 Binging HPC Techniques to Deep Learning Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU &amp; Distributed setups]]></content>
      <categories>
        <category>训练方法</category>
        <category>分布式训练</category>
      </categories>
      <tags>
        <tag>分布式训练</tag>
        <tag>多卡训练</tag>
        <tag>horovod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【优化技巧】指数移动平均（EMA）的原理及PyTorch实现]]></title>
    <url>%2F2019%2F06%2F01%2Fema%2F</url>
    <content type="text"><![CDATA[在深度学习中，经常会使用EMA（指数移动平均）这个方法对模型的参数做平均，以求提高测试指标并增加模型鲁棒。 今天瓦砾准备介绍一下EMA以及它的Pytorch实现代码。 EMA的定义指数移动平均（Exponential Moving Average）也叫权重移动平均（Weighted Moving Average），是一种给予近期数据更高权重的平均方法。 假设我们有n个数据：$[\theta_1, \theta_2, …, \theta_n]$ 普通的平均数：$\overline{v}=\frac{1}{n}\sum_{i=1}^n \theta_i$ EMA：$v_t = \alpha\cdot v_{t-1} + (1-\alpha)\cdot \theta_t$，其中，$ v_t$表示前$t$条的平均值 ($v_0=0$)，$\alpha$ 是加权权重值 (一般设为0.9-0.999)。 Andrew Ng在Course 2 Improving Deep Neural Networks中讲到，EMA可以近似看成过去$1/(1-\alpha)$个时刻$v$值的平均。 普通的过去$n$时刻的平均是这样的：$$v_t =\frac{(n-1)\cdot v_{t-1}+\theta_t}{n}$$类比EMA，可以发现当$\alpha=\frac{n-1}{n}$时，两式形式上相等。需要注意的是，两个平均并不是严格相等的，这里只是为了帮助理解。 实际上，EMA计算时，过去$1/(1-\alpha)$个时刻之前的平均会decay到 $\frac{1}{e}$ ，证明如下。 如果将这里的$v_t$展开，可以得到：$$v_t = \alpha^n v_{t-n} + (1-\alpha)(\alpha^{n-1}\theta_{t-n+1}+ … +\alpha^0\theta_t)$$其中，$n=\frac{1}{1-\alpha}$，代入可以得到$\alpha^n=\alpha^{\frac{1}{1-\alpha}}\approx \frac{1}{e}$。 EMA的偏差修正实际使用中，如果令$v_0=0$，步数较少的情况下，ema的计算结果会有一定偏差。 理想的平均是绿色的，因为初始值为0，所以得到的是紫色的。 因此可以加一个偏差修正（bias correction）。$$v_t = \frac{v_t}{1-\alpha^t}$$显然，当t很大时，修正近似于1。 在深度学习的优化中的EMA上面讲的是广义的ema定义和计算方法，特别的，在深度学习的优化过程中，$\theta_t$ 是t时刻的模型权重weights，$v_t$是t时刻的影子权重（shadow weights）。在梯度下降的过程中，会一直维护着这个影子权重，但是这个影子权重并不会参与训练。基本的假设是，模型权重在最后的n步内，会在实际的最优点处抖动，所以我们取最后n步的平均，能使得模型更加的鲁棒。 EMA为什么有效网上大多数介绍EMA的博客，在介绍其为何有效的时候，只做了一些直觉上的解释，缺少严谨的推理，瓦砾在这补充一下，不喜欢看公式的读者可以跳过。 令第n时刻的模型权重（weights）为$v_n$，梯度为$g_n$，可得：$$\begin{align}\theta_n &amp;= \theta_{n-1}-g_{n-1} \\&amp;=\theta_{n-2}-g_{n-1}-g_{n-2} \\&amp;= … \\&amp;= \theta_1-\sum_{i=1}^{n-1}g_i\end{align}$$令第n时刻EMA的影子权重为$v_n$，可得：$$\begin{align}v_n &amp;= \alpha v_{n-1}+(1-\alpha)\theta_n \\&amp;= \alpha (\alpha v_{n-2}+(1-\alpha)\theta_{n-1})+(1-\alpha)\theta_n \\&amp;= … \\&amp;= \alpha^n v_0+(1-\alpha)(\theta_n+\alpha\theta_{n-1}+\alpha^2\theta_{n-2}+…+\alpha^{n-1}\theta_{1})\end{align}$$ 代入上面$\theta_n$的表达，令$v_0=\theta_1$展开上面的公式，可得：$$\begin{align}v_n &amp;= \alpha^n v_0+(1-\alpha)(\theta_n+\alpha\theta_{n-1}+\alpha^2\theta_{n-2}+…+\alpha^{n-1}\theta_{1})\\&amp;= \alpha^n v_0+(1-\alpha)(\theta_1-\sum_{i=1}^{n-1}g_i+\alpha(\theta_1-\sum_{i=1}^{n-2}g_i)+…+ \alpha^{n-2}(\theta_1-\sum_{i=1}^{1}g_i)+\alpha^{n-1}\theta_{1})\\&amp;= \alpha^n v_0+(1-\alpha)(\frac{1-\alpha^n}{1-\alpha}\theta_1-\sum_{i=1}^{n-1}\frac{1-\alpha^{n-i}}{1-\alpha}g_i) \\&amp;= \alpha^n v_0+(1-\alpha^n)\theta_1 -\sum_{i=1}^{n-1}(1-\alpha^{n-i})g_i\\&amp;= \theta_1 -\sum_{i=1}^{n-1}(1-\alpha^{n-i})g_i\end{align}$$对比两式：$$\begin{align}\theta_n &amp;= \theta_1-\sum_{i=1}^{n-1}g_i \\v_n &amp;= \theta_1 -\sum_{i=1}^{n-1}(1-\alpha^{n-i})g_i\end{align}$$EMA对第i步的梯度下降的步长增加了权重系数$1-\alpha^{n-i}​$，相当于做了一个learning rate decay。 PyTorch实现瓦砾看了网上的一些实现，使用起来都不是特别方便，所以自己写了一个。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class EMA(): def __init__(self, model, decay): self.model = model self.decay = decay self.shadow = &#123;&#125; self.backup = &#123;&#125; def register(self): for name, param in self.model.named_parameters(): if param.requires_grad: self.shadow[name] = param.data.clone() def update(self): for name, param in self.model.named_parameters(): if param.requires_grad: assert name in self.shadow new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name] self.shadow[name] = new_average.clone() def apply_shadow(self): for name, param in self.model.named_parameters(): if param.requires_grad: assert name in self.shadow self.backup[name] = param.data param.data = self.shadow[name] def restore(self): for name, param in self.model.named_parameters(): if param.requires_grad: assert name in self.backup param.data = self.backup[name] self.backup = &#123;&#125;# 初始化ema = EMA(model, 0.999)ema.register()# 训练过程中，更新完参数后，同步update shadow weightsdef train(): optimizer.step() ema.update()# eval前，apply shadow weights；eval之后，恢复原来模型的参数def evaluate(): ema.apply_shadow() # evaluate ema.restore() References 机器学习模型性能提升技巧：指数加权平均（EMA） Exponential Weighted Average for Deep Neutal Networks]]></content>
      <categories>
        <category>Trick</category>
        <category>代码实现</category>
      </categories>
      <tags>
        <tag>torch</tag>
        <tag>ema</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【论文笔记】MT-DNN]]></title>
    <url>%2F2019%2F05%2F23%2Fmt-dnn%2F</url>
    <content type="text"><![CDATA[想写这篇蛮久的，但由于之前一直忙着搞别的事情（好吧就是懒），一直拖着。刚好最近有用这个方面的需求，就又读了一遍论文和github上的一些实现。 大部分的博客都只是粗略翻译论文，然而光看论文，往往会忽略一些实现细节，所以笔者最近在尝试将论文笔记和源码解析结合起来，就从这篇MT-DNN开始吧。希望大家多多提意见。 Paper：Multi-Task Deep Neural Networks for Natural Language Understanding其实作者（Xiaodong Liu）早在15年就写过一篇Multi-task相关的论文，只不过当时还没有bert这样优秀的预训练表达层，在bert横扫各大榜单之后，作者将之前多任务的概念和bert相结合，duang~就出了这一篇在GLUE、SNLI和SciTail创下新的SOTA的论文。 Intuition会滑雪的人，学滑冰要容易的多（笔者试过，反过来不大成立，手动狗头）。我觉得类比成九年义务教育更好，十门功课同步学，数学是你学物理的基础，历史知识能提高你作文的水平，etc。 Motivation 监督学习需要大量监督数据，但正常情况下咱都是没有的。MTL（multi-task learning）可以提高low-resource任务的表现。 MTL能起到正则的作用，减轻模型对特定任务的过拟合。 bert之类的预训练模型充分利用了无监督数据。MTL作为补充，进一步利用了out-domain的监督数据。 Model模型很简单，看一下这个图： 底层share了bert的表达层，输出层为每个任务设计了各自的输入形式和loss计算方式。 任务和loss计算任务分类及数据GLUE 单句分类（Single-Sentence Classification）： CoLA（Corpus of Linguistic Acceptability）：判断英语句子是否语法正确 SST-2（Stanford Sentiment Treebank）：影评情感分类 文本相似度（Text Similarity）： STS-B（Semantic Textual Similarity Bench-mark）：人类标注的1-5的语义相似度数据集。 对句分类（Pairwise Text Classification）： RTE（Recognizing Textual Entailment）：entailment or not_entailment MNLI（Multi-Genre Natural Language Inference）：entailment，contradiction，neural QQP（Quora Question Pairs）：判断两个问题是否问的是同一内容。 MRPC（Microsoft Research Paraphrase Corpus）：判断是否两个句子是语义相同的。 WNLI（Winograd NLI）：Wino-grad Schema dataset得到的推理任务。 相关性排序（Relevance Ranking）： QNLI（Stanford Question Answering）：问答对数据集。 Out-domain SNLI（Stanford Natural Language Inference）：Flickr30里人工标注了hypotheses的推理数据集 SciTail（Science Question Answering Textual Entailment）：科学问题的推理，更难。 loss计算1. 单句分类交叉熵：$$P_r(c|X) = \text{softmax}(W^T_{SST}\cdot x)\\L(\Theta) = -\sum_c{\mathbb{I}(X, c) \log (P_r(c|X)) }$$ 2. 文本相似度均方误差：$$\text{Sim}(X_1, X_2) = \text{sigmoid}(w^T_{STS}\cdot x)\\L(\Theta) = (y-\text{Sim}(X_1, X_2))^2$$ 3. 对句分类前面比较常见，这个对句分类作者处理的方式比较特殊，用了18年作者自己提出的一种叫 SAN（stochastic answer network）的输出层构建方式，推理过程有点繁琐，给大家贴个图。 注意图中的$m$，$n$ 都是sequence length。 总结起来就是，作者得到query和premise分别的token-wise的表达之后，在他们两个之间做了一个attention，然后开辟了一个新的状态维度做RNN，从而得到多次预测结果，再做平均（类似于人推理时，多次思考才能得到最终的判断）。作者在后面证明了 SAN 结构能带来0.1%~0.5%的提升。 loss也是交叉熵：$$L(\Theta) = -\sum_c{\mathbb{I}(X, c) \log (P_r(c|X)) }$$ 4. 相关性排序作者的这个loss设计还是挺有意思的，不用简单的二分类来做这个任务，而是用learning2rank的范式，对于每个query $Q$ 采样 $N$ 个candidates，其中$A^+$是正确答案，其他的都是是错误答案。 负对数似然：$$\text{Rel}(Q,A)=\text{sigmoid}(w^T_{QNLI} \cdot x)\\L(\Theta)=-\frac{\text{exp}(\text{Rel}(Q,A^+))}{\sum_{A’\in{\mathcal{A}}}\text{exp}(Rel(Q,A’))}$$ 训练过程 训练过程就是把所有数据合并在一起，每个batch只有单一任务的数据，同时会带有一个task-type的标志，这样模型就知道该走哪一条loss计算的路径。 论文里并没有提及对于单个任务，之后还要不要再单独Fine-tune一下，不过参考github的FAQ，再FT一下，结果会更好。 实验实现细节Optimizer：Adamax（这个地方跟bert不太一样）lr：5e-5batch size：32max_num of epochs：5SAN steps：5warm-up：0.1clip gradient norm：1max seq length：512 GLUE结果 从Table 2 可以看出来，MT-DNN在每一项都超过了bert，而且数据越少的任务，提升越明显，对于QQP和MNLI来说，提升就没那么明显了。 Table 3中的ST-DNN名字很玄乎，其实与bert不同的就是用了文中的复杂了一点的输出模块和loss的设计，比如SAN，learning2rank这些，单独训练各个任务。可见都是有一定程度的提升。所以MT-DNN相对于bert的提升其实来自于 multi task 和 special output module 两个部分。 SNLI 和 SciTail 结果 在得到mult-task训练后的ckpt后，用这个weights去fine tune新的任务，结果和GLUE的保持一致，都有提升，且小数据集任务的提升更明显。 Domain 适应性结果这个结果比较有趣，笔者认为也是比较重要的点，MT-DNN得到的weights相对于bert的weights能在很少的数据下达到不错的效果，且数据越少，相对bert的提升就越大。(甚至23个训练样本就能达到82.1%的准确率，amazing啊。) Conclusion打个总结： MT-DNN的优点： 数据要求少 泛化能力强，不容易过拟合 MT-DNN的缺点： 实用性：实际应用中也许并不能找到特别合适的，且高质量的多任务 训练慢啊，MT-DNN作者用了4张v100，普通业务要不起这个条件，所以MT-DNN的定位其实类似于bert，训练好了就别乱动了，当pretrain-model用。 作者认为的Further work 更深度的share weights 更有效的训练方法 用更可控的方式融入文本的语言结构（这点个人感觉不适用于现在大刀阔斧搞预训练模型的情况） Code源码地址：https://github.com/namisan/mt-dnn 阅读源码前，我习惯思考一下如果我自己写，会怎么写： 首先咱肯定得分类一下数据，每一类任务对应一个数据流，不能每个任务写一个数据流，太累了。 新建模型的时候得知道有哪些任务，每个任务num_labels是多少，自动生成输出层集合和与id的映射，训练和推理的时候根据任务id选择输出层。 怎么保证一个epoch结束，所有任务数据集都用完了呢？ max seq length、learning rate、batch size这些超参需要根据任务变化吗？不同任务的loss如何scale呢？ 基本上，想到这，心里都有点数了，带着问题看源码实现。 官方的源码是用PyTorch实现的，包括了MT-DNN的训练，和一些下游任务的finetune，同时也提供了训练好的MT-DNN的模型。核心思想和步骤如下： prepro.py：预处理数据，将GLUE的多个任务分成四类，统一处理成 {&#39;uid&#39;: ids, &#39;label&#39;: label, &#39;token_id&#39;: input_ids, &#39;type_id&#39;: type_ids}的形式，方便后面操作。 mt-dnn/batcher.py：自定义的data iterator，将读到的数据处理成Tensor。 mt-dnn/matcher.py：模型，之所以叫matcher，是因为模型有一个ModuleList，存放了不同任务对应的输出层，根据当前batch的任务类型match对应的输出层。 mt-dnn/model.py：这里命名有点混淆，实际的模型是上面的matcher，这里做了一些模型前后的处理工作（ema，predict，save模型之类的）。 重点讲一下mt-dnn/batcher.py和mt-dnn/matcher.py这两个部分。 batcher.py 忽略作者对于batch这个单词疯狂的拼写错误，相比于常规单任务的data_iterator，这个类除了iter数据，还要返回关于这个任务的必要信息，比如这个任务的id，任务的类型。make_baches 实现把数据打包成batch，reset用来在每个epoch之后重新shuffle并打包成batch。 matcher.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class SANBertNetwork(nn.Module): def __init__(self, opt, bert_config=None): super(SANBertNetwork, self).__init__() self.dropout_list = [] self.bert_config = BertConfig.from_dict(opt) self.bert = BertModel(self.bert_config) if opt['update_bert_opt'] &gt; 0: for p in self.bert.parameters(): p.requires_grad = False mem_size = self.bert_config.hidden_size self.decoder_opt = opt['answer_opt'] # 构建ModuleList，index为task_id self.scoring_list = nn.ModuleList() labels = [int(ls) for ls in opt['label_size'].split(',')] task_dropout_p = opt['tasks_dropout_p'] self.bert_pooler = None for task, lab in enumerate(labels): decoder_opt = self.decoder_opt[task] # 不同任务dropout也不一样 dropout = DropoutWrapper(task_dropout_p[task], opt['vb_dropout']) self.dropout_list.append(dropout) if decoder_opt == 1: out_proj = SANClassifier(mem_size, mem_size, lab, opt, prefix='answer', dropout=dropout) self.scoring_list.append(out_proj) else: out_proj = nn.Linear(self.bert_config.hidden_size, lab) self.scoring_list.append(out_proj) self.opt = opt self._my_init() self.set_embed(opt) def forward(self, input_ids, token_type_ids, attention_mask, premise_mask=None, hyp_mask=None, task_id=0): all_encoder_layers, pooled_output = self.bert(input_ids, token_type_ids, attention_mask) sequence_output = all_encoder_layers[-1] if self.bert_pooler is not None: pooled_output = self.bert_pooler(sequence_output) decoder_opt = self.decoder_opt[task_id] if decoder_opt == 1: max_query = hyp_mask.size(1) assert max_query &gt; 0 assert premise_mask is not None assert hyp_mask is not None hyp_mem = sequence_output[:,:max_query,:] # 通过任务id，索引到对应的输出层，搞定。 logits = self.scoring_list[task_id](sequence_output, hyp_mem, premise_mask, hyp_mask) else: pooled_output = self.dropout_list[task_id](pooled_output) logits = self.scoring_list[task_id](pooled_output) return logits 这里笔者删除了其他函数，只保留了__init__和forward，正如我们看源码之前猜想的，作者就是通过构建一个ModuleList，根据各个任务的类型、label数等信息append输出层，index即任务id。]]></content>
      <categories>
        <category>NLP</category>
        <category>论文笔记</category>
        <category>多任务</category>
      </categories>
      <tags>
        <tag>bert</tag>
        <tag>多任务</tag>
        <tag>迁移学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【论文笔记】Attention is all you need]]></title>
    <url>%2F2019%2F01%2F27%2Ftransfomer%2F</url>
    <content type="text"><![CDATA[今年的NLP界被BERT整的明明白白，其中的基本结构 Transformer 一定要了解一下。 Abstract：一般来说，重要的sequence transduction模型都是基于包含encoder，decoder的复杂的rnn和cnn的。最好的模型是通过一个attention机制来连接encoder，decoder。比较普通！我们提出一个只靠attention的。叫Transformer，跟cnn，rnn完全没关系，很炫酷。 在两个翻译任务实验证明：我们的模型又快又好！！！ 高了2个BLEU； 用8个gpu训练3.5天，结果直接超过了当前的SOTA。 我们还证明，Transformer泛化性能贼好，在parsing上大小数据都比别的好 。 划重点：只靠Attention。 1. IntroductionRNN，LSTM，GRU在翻译和 LM（language model）领域搞了很多SOTA，很多研究花了很多心思在push Encoder-Decoder和Recurrent Language Model的边界。 可是，RNN的天性决定了训练的时候并行性差。尤其对长的sequence，内存限制影响batch examples了。很烦！有一些通过分解tricks和条件计算来提高efficiency的related work，后者也提高performance。但是问题依然存在。 Attention机制现在几乎干啥都必须了，可以不顾输入输出的distance地去model依赖。但是，除了极少cases，attention基本都和rnn绑定在了一起，很不机智！ So，咱提出一种不靠rnn，只靠attention的！！！并行性刚刚的，8个p100花了12 hours就达到翻译的new sota。 2. Background一些相关的研究都用了cnn来减少sequence计算。 这些模型里operations的数量随着输入输出距离的线性（convS2S）或是指数级（ByteNet）地增长。使得很难学习到较远的dependency。 在Transformer里，这是一个常数级的操作，虽然是以牺牲一定精度为代价，但是我们用Multi-Head Attention来抵消了，效果很好！ Self-attention，通过relate一个single sequence的不同位置来计算seq的表征。成功用在阅读理解，summarization等等。 基于attention的end2end的memory网络在很多简单QA和LM问题上比seq-aligned rnn（就是s2s吧）要好。但是啊，Transformer是第一个只靠self-attention来计算表征的。下面介绍一波Transformer！ 3. Model Architecture 其实，老瓦看到这个模型的一瞬间，心情是复杂，看着好简单，可是咋和以前那种有个序列，清清楚楚写着$(x_1, x_2, …, x_n)$什么的模型不太一样？不慌，老瓦来盘一盘。 3.1 Encoder and Decoder StacksEncoder：Encoder是由N=6相同的层组成的栈，每层都有两个子层。第一子层是一个multi-head self-attention mechanism（多头自注意力机制），第二子层是一个position-wise的全连接前馈层。每个子层后面都加了一个residual connection（冗余连接）+layer normalization（层正则），也就是说每个子层的输出都是 $\text{LayerNorm}(x+\text{Sublayer}(x))$ 。为了方便做加运算，所有子层的输出维度都是$d_{model}=512$。 Decoder：Decoder也是N=6层的。主要两个区别： 增加了一个子层，将encoder的输出当做输入。 修改了decoder栈的自注意力自层，来防止位置们去关注后续的位置。masking结合output的embedding都右移了一位这个事实，保证了位置i的预测只依赖于比i小的已知位置。 3.2 Attention既然文章名字叫Attention is all you need，attention的结构当然是其中的重中之重，理解了Attention就几乎理解了文章的一大半。 3.2.1 Basic Attention首先，得知道啥是attention。14年Sutskever大神祭出seq2seq之后，紧跟着Bahdanau和Luong就发了两篇attention用在seq2seq的论文，名字也好记，一个叫Bahdanau attention，一个叫Luong attention。这里以Bahdanau Attention为例，讲一讲计算attention的基本套路。 其实，attention机制和普通seq2seq的不同就是，要计算出一个包含上下文信息的context vector作为decoder每个位置的输入。计算attention时，一般有Query(Q)，Key(K)，Value(V) 三个输入。在上面这张图上，Q就是 $(s_0, s_1, … s_m)$ ，K和V就是$(h_0, h_1, …, h_n)$。attention机制一般的套路就是，用Q和K先算出一个权重的向量，再用这个权重的向量去element-wise地乘上V，就能得到Context Vector：$$ConVec(Q, K, V) = softmax(score(Q, K))V$$ 3.2.2 Scaled Dot-Product Attention明白了attention的套路，我们来看看论文里的attention是什么来头，先看看原文中这张无比清晰（但是看起来有点唬人）的图。 这又是Scaled Dot-Product又是Multi-Head的，一开始着实让老瓦感到有点慌，后来仔细一看，其实挺简单。 看过Luong那篇attention的文章的人都知道，score一般有三种算法：$$score(h_t, \overline{h}_s) =\begin{cases}h_t^{\top}\overline{h}_s &amp;dot \\h_t^{\top}W_a\overline{h}_s &amp;general\\v_a^{\top}tanh(W_a[h_t;\overline{h}_s]) &amp;concat\end{cases}$$其实文章里用的就是第一种，因为性价比高（效果还不错速度快），但是有个缺点当Q，K的维度比较大的时候，容易进到softmax的饱和区，作者就scale了一下（除以$\sqrt{d_k}$），解决了这个问题，这个就是所谓的Scaled Dot-Product Attention。粗暴有效。$$Attention(Q, K, V) = softmax(\frac{QK^{\top}}{\sqrt{d_k}})V$$基本上就完事了。 老瓦在看论文的时候一直不明白mask（只有在Decoder的input用到了）到底是具体怎么操作的，其实作者在后面有解释，就是把所有的非法连接的score都设置成负无穷，这样的softmax之后得到的权重向量就是零了($e^{-\infty}$)。完事。 3.2.3 Multi-Head Attention这下就讲到精髓了：多头注意力（不知道这样翻译会不会被打。。。）。可以这么理解，有好多人对attention权重的看法不太一样，所以我们就把这个任务给很多人一起做，最后取大家的共同意见，有点像CNN里好多个kernel的味道。 文章表示，比起直接用$d_{model}$的Q, K, V来说，将Q, K, V用不同的h个线性投影得到的h个$d_v$的context vector，再concat起来，过一个线性层的结果更好，可以综合不同位置的不同表征子空间的信息。$$\text{Multihead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, …, \text{head}_h)W^O\\\text{where head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$其中，$W_i^Q\in\mathbb{R}^{d_{model}\times d_k}$，$W_i^K\in\mathbb{R}^{d_{model}\times d_k}$，$W_i^V\in\mathbb{R}^{d_{model}\times d_v}$，然后$W^O\in\mathbb{R}^{hd_v\times d_{model}}$。 在文章里，设置了h=8个平行注意层（也就是头（head）2333）。对于每个层的$d_k=d_v=d_{\text{model}}/h=64$。因为每个头都减少了dimension，所以整体的computational cost和single-head full dimension的注意力机制是差不多的。 3.2.4 Applications of Attention in our model encoder-decoder attention: 模仿seq2seq模型的注意力机制 encoder 的 self-attention layer decoder 的self-attention layer：和上面的区别就是加了masking 3.3 Position-wise Feed-Forward Networks每个FFN包括两次线性变换，中间是ReLu的激活函数。$$\text{FFN} = \max(0, xW_1+b_1)W_2+b_2$$不同position的FFN是一样的，但是不同层是不同的。输入输出维度都是$d_{model}=512$，中间层的维度是$d_{ff}=2048$。 3.4 Embeddings and Softmax和其他seq transduction模型一样，也得用learned embeddings，learned 线性变换，softmax这些东西。两个embedding的权重是share的。embedding层，会把权重乘$\sqrt{d_{model}}​$。 3.5 Positional Encoding因为模型没有rnn或者cnn，为了用到sequence的顺序，作者引入了positional encoding（$dim=d_{model}$，便于相加）来inject一些相对位置的信息。$$PE(pos, 2i) = sin(pos/10000^{2i/d_{model}})\\PE(pos, 2i+1) = cos(pos/10000^{2i/d_{model}})$$作者测试用学习的方法来得到PE，最终发现效果差不多，所以最后用的是fixed的，而且sinusoidal的可以处理更长的sequence的情况。 用sinusoidal函数的另一个好处是可以用前面位置的值线性表示后面的位置。$$\sin(\alpha+\beta) = \sin\alpha\cos\beta+\cos\alpha\sin\beta\\\cos(\alpha+\beta) = \cos\alpha\cos\beta-\sin\alpha\sin\beta$$ 4. Why Self-Attention 之所以选择self-attention，主要因为三点： 每层的computational complexity； 可以被parallelize的计算量； 网络中long-range dependencies直接的path length（越短越能方便学到 long-range dependencies）。 5. Conclusion优点： 抛弃了RNN和CNN，提出了Transformer，算法的并行性非常好； Transformer的设计最大的带来性能提升的关键是将任意两个单词的距离是1，有效地解决了long dependency的问题。 缺点： Transformer不像CNN那样可以抽取局部特征，RNN + CNN + Transformer的结合可能会带来更好的效果； 位置信息其实在NLP中非常重要，Transformer中用的Position Embedding也不是一个最终的解决方案。]]></content>
      <categories>
        <category>NLP</category>
        <category>论文笔记</category>
        <category>Attention</category>
      </categories>
      <tags>
        <tag>Attention</tag>
        <tag>Transformer</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【论文笔记】A User Simulator for Task-Completion Dialogues]]></title>
    <url>%2F2019%2F01%2F16%2Fuser-simulator%2F</url>
    <content type="text"><![CDATA[基本框架（包含对话系统）： Abstract：做任务型bot的时候，强化学习（RL）很强，但是有一些困难： 需要和环境互动，已有的完整对话训练数据没用； 每个不同的任务都需要各自领域的标注数据； 收集人人对话或者人机对话需要领域知识。 但是啊： ====&gt;建造数据库又贵又花时间 ====&gt;只好模拟 ====&gt;user simulator诞生 ====&gt;Bot（agent）先用simulator去训练，搞定了simulator就可以上线，持续online learning Introduction:对话系统一般如 Figure 1. 所示。Dialongue Policy (DP)是一个任务型bot的核心。 一般来说传统的Dialogue Policy（DP）使用规则编程的，但有缺点： 对于复杂系统，难以设计 最优的policy也会变化，不好维护。因此，一般用强化学习训练DP。 为啥要user simulator ? Supervised Learning（SL）监督学习在任务型的bot里不行： 需要专家来标注大量数据； 大量专业的领域知识需要大量的数据来训练； 即使有大量训练数据，还是会有对话空间没有被搜索到。 相反的，RL很吊，不需要专家生成的数据，给一个奖励信号，agent就可以通过交互优化DP。 但是不幸的是：RL需要从环境来的很多sample，所以和真实用户从零开始训练不实际。 ====&gt; 所以需要User Simulators！ 一般的操作是：先在simulator上训练出一个比较好的效果，再部署到真实场景中，持续online learning。 Related Work很难判断user simulator好不好，没有Metric来判定，so没有标准方法，放手乱做。user simulation主要有这么几个类型： 从粒度上分： 在对话行为（dialogue-act）上进行操作的； 在对话文本（utterance）进行操作的 从方法的角度讲： 基于规则的（rule-based） 基于模型的（model-based） 以前的 bi-gram 模型 $P(a_u |a_m)$，基于上一个系统行为 $a_m$ 去预测下一个用户行为 $a_u$。这就很傻。(user有可能改goal，看的信息太少) 后面两个办法来处理： 看更长的对话历史 把user goal 放到user state modeling中 seq2seq端到端解决闲聊可以，任务型不太行。 本文用的叫 agenda-based user simulation 的架构，类似栈的结构通过进栈出栈来 model 状态转移和用户行为生成。很方便，显性encode了对话历史和用户目标（user goal）。 总结：文章 结合了rule-based和model-based 的方法: 在dialog-act level，用了agenda-based (rule)的方法； 在nlg部分，用了seq2seq (model)的方法。 Task-completion的对话系统（Dialogue system）任务型对话系统(以订票bot为例)，通过nl交互，去获取客户期望的信息，最终实现订票。以： 是否订票成功 是否电影满足要求 为标准，输出一个二进制结果，success or failure，评估系统。 数据：用Amazon Mechanical Turk（众包平台）收集的数据，内部标注，11个intent（i.e., inform，request，confirm-question，confirm-answer，etc），29个slot（i.e., movie-name，start-time，theater，numberofpeople，etc）。 一共标注了280个对话，对话平均11轮。 User SimulatorUser GoalTC Bot的user simulator第一步是生成user goal。agent不知道user goal，但是要帮助user来完成他的goal。user goal的定义分两个部分： inform_slots: 包含了constraints（C） request_slots: unk（R） 又可以分成： 必须有的slots（required slots） 选择有的slots（optional slots，i.e., ticket就是request slots 里的必须项）。 Goal是在labeled dataset里生成的，两个机制： 在第一个用户轮提取所有的slots。对于所有的slots，在所有的用户轮里提取第一次出现的。 每次跑对话的时候，就先sample一个user goal。 User Action第一轮的action sampling：要加一些限制（比如通常是request turn，至少一个inform slot，movie_name必须在，等等）。 如果不用NLG，NLU的话，还要加入噪声来模拟NLG和NLU过程产生的噪音，去训练DM部分。 Dialogue Status三个对化状态： no_outcome_yet success failure 具体情况具体讨论。 NLUIOB-format slots tags + Intent tags 最后的hidden layer判断 intent。 NLG基于Template的NLG：dialog-act 被found在模板中的，套模板句型。基于Model的NLG：没发现的，用model生成。（这一点感觉很扯，都没见过，咋生成，数据量肯定不够啊。） UsagesTask-completing Dialogue Setting：任务型Bot（订票），衡量 agent的Metrics为：1. success rate；2. average reward；3. average turns。KB-InfoBot（简单点，agent和user都只有request和inform）：问答Bot（电影信息） DiscussionRule-based的 user simulation 很safe，但是很耗时，因为要手动制定各种规则。两个优化方向： 包含user goal的改变（已做）实现Model-based 的simulator，优点是泛化性能好，缺点是1. 需要大量数据， 2. 万一有漏洞，RL agent 会抓住这个漏洞，假假的成功，你以为success了，其实都是RL agent抓住了loophole给你的假象。 总结：（根据《Agenda-Based User Simulation for bootstrapping a POMDP Dialogue System》by J. Schatzmann） 语义层的 User Simulation人机对话可以看成是状态转移（state transition）和对话行为（Dialog）的序列：用户根据状态 S (或加上机器人的 $a_m$，第一轮可能没有$a_m$)，采取行动 $a_u$, 把状态转移到$S’$。收到agent行动 $a_m$，再根据 $S’$ ，再把状态转移到 $S’’$。这个user行为可以被分解为三个模型：$P(a_u|S)$ 行为选择，$P(S’|a_u, S)$ 状态转移（用户行为），$P(S’’|a_m,S’)$ 状态转移（系统行为）。 基于Goal和Agent的状态表示用户状态（User State）由用户目标（Goal）及议程（Agenda）构成。 Goal由Constraint（C）（比如要市中心的啤酒酒吧）和Request（R）（比如酒吧电话是多少）构成。 Agenda是一个类似栈的结构，包含了在排队中的，用来引出目标中明确的信息的用户行为。在对话开始的时候，用数据库随机生成一个新的Goal。然后，从Goal里的内容来初始化Agenda，把Goal中的 C 都convert成 Inform动作，把Goal中的 R 都convert成 Request动作，再在最后加一个bye动作。有新的 $a_m​$ 的时候，新的user acts 会被压进栈，不需要的会被出栈。具体实现方法（框架和数据结构），后面分析miulab的simulator源码的时候会写。 上图描述了Agenda随着$a_m$，$a_u$的状态转移过程（进栈和出栈）。 上图描述了无NLU和NLG的训练。]]></content>
      <categories>
        <category>NLP</category>
        <category>论文笔记</category>
        <category>Bot</category>
      </categories>
      <tags>
        <tag>Chatbot</tag>
        <tag>User Simulator</tag>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前言]]></title>
    <url>%2F2019%2F01%2F11%2Ffirst-post%2F</url>
    <content type="text"><![CDATA[欢迎来到瓦特兰蒂斯。笔者是一个相信亚特兰蒂斯曾经存在的五岁抬头团—aka瓦砾。 不定期更新NLP的干货，刚开始写博客，希望自己能坚持下去，请大家多多指教。 试一下图片咋放。2333。]]></content>
  </entry>
</search>
