<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[【比赛分享】刷新CoQA榜单纪录：基于对抗训练和知识蒸馏的机器阅读理解方案解析]]></title>
    <url>%2F2019%2F11%2F06%2Fcoqa%2F</url>
    <content type="text"><![CDATA[本文首发于机器之心微信公众号，专栏留存。 近日，在由斯坦福大学发起的对话式问答挑战赛 CoQA (Conversational Question Answering Challenge）中，追一科技AI Lab团队超越微软团队成为榜单第一[1]，刷新了之前微软等团队创造的CoQA纪录。值得注意的是，团队提交的技术方案中，单模型的各项指标表现首次全面超越人类。 一直以来，机器阅读理解都是自然语言处理界最受关注、进步最快的技术方向之一，主要有两大比赛较受瞩目，分别是注重一问一答的SQuAD[2]和多轮对话问答的CoQA[3]。相比于SQuAD，CoQA更注重模型在对话过程中回答关联问题的能力，答案形式自由，而且数据来自儿童故事、文学、初高中英语考试、新闻、维基百科、Reddit和科学等七个不同的领域，这些改进都极大地提高了挑战赛的难度，对模型的鲁棒性和泛化能力有着更高的要求。我们团队针对CoQA数据集的难点，通过对抗训练（Adversarial training）和知识蒸馏（Knowledge Distillation）等方法，有效地提高了模型的泛化能力，从而依靠单模型刷新了CoQA榜单的记录，并首次单模型超越人工评测指标。在这里分别从模型、训练方法、后处理等方面做一个简单的介绍。 模型Baseline：RoBERTa我们的基线模型以Facebook开源的RoBERTa[4]预训练模型为基础，之所以选择RoBERTa，是因为其相较于BERT[5]在语言模型预训练的过程中用了更多领域的语料，更适合CoQA数据来自不同领域的特点。 在输入端，由于CoQA的数据集是对话式的，每个问题都依赖于历史对话，因此在训练过程中，我们将对话历史拼接到当前问题之前，问题和答案之间用分隔符分开，组成当前轮次的Query，然后将其与Context拼接，作为模型的输入。 在输出端，CoQA数据集的答案有可能是自由文本、Yes、No和Unk。由于头部的自由文本的答案都能在Context中找到相近的片段，我们采取抽取+Yes/No/Unk分类的输出层结构。其中，对于抽取的部分，我们使用Pointer-Network的输出结构得到答案开始和结尾位置的logits；对于分类的部分，则用一个简单的全连接层得到Yes/No/Unk三分类的logits。 在计算损失函数时，我们将预测答案开始和结尾位置的两个logits向量分别与Yes/No/Unk三分类的logits拼接，得到两个最终的logits向量，此时这两个logits对应的label依然是one-hot的，所以我们可以把这两个logits向量过softmax，然后用交叉熵计算开始和结尾的损失，取平均，得到基线模型的损失值。 依据标注辅助任务在CoQA数据集中，每一个回答（除了unknown）都附带了一段Context中的原文片段作为逻辑依据。为了充分利用该信息，我们在Baseline模型的基础上，增加了一个依据标注的任务，同步进行多任务训练。对于Context的每一个token，我们会判断其是否在逻辑依据中（标成1或者0）。这部分的损失函数用二元交叉熵计算，按照一定比例累加到总的loss上。 除此之外，我们发现，Yes/No类型答案的逻辑依据中，常常包含了肯定或否定的语义，可以用来辅助Yes/No/Unk的分类，所以我们在RoBERTa池化输出的基础上又利用注意力机制融合了逻辑依据的输出信息，以提高最后模型的表现。 训练方法除了模型上的修改，为了提高模型的泛化能力以应付CoQA数据集来源丰富、问题类型多样的特点，我们还采用了对抗训练和知识蒸馏等训练方法。 对抗训练对抗训练[6]是一种能有效提高模型鲁棒性和泛化能力的训练手段，其基本原理是通过在原始输入上增加对抗扰动，得到对抗样本，再利用对抗样本进行训练，从而提高模型的表现。由于CoQA数据集对模型的泛化能力较高，我们在训练时，使用了对抗训练来提高模型的表现。 由于自然语言文本是离散的，一般会把对抗扰动添加到嵌入层上。在我们的系统中，为了最大化对抗样本的扰动能力，我们利用梯度上升的方式生成对抗样本。为了避免扰动过大，我们将梯度做了归一化处理。 \begin{align*} {g} &= -\bigtriangledown_ {\mathcal{L}}(y_i|{v}; {\hat{\theta}} ) \\ {v}^* &= {v}+ \epsilon{g} / \|{g}\|_2 \end{align*}其中，v为嵌入向量。实际训练过程中，我们在训练完一个batch的原始输入数据时，保存当前batch对输入词向量的梯度，得到对抗样本后，再使用对抗样本进行对抗训练。 除了对抗训练，我们还利用虚拟对抗训练做半监督训练。 知识蒸馏与对抗训练类似，知识蒸馏也是一种常用的提高模型泛化能力的训练方法。 知识蒸馏[7] 这个概念最早由Hinton在2015年提出。一开始，知识蒸馏通往往应用在模型压缩方面，利用训练好的复杂模型（teacher model）输出作为监督信号去训练另一个简单模型（student model），从而将teacher学习到的知识迁移到student。Tommaso [8]在18年提出，如果student和teacher的模型完全相同，蒸馏后则会对模型的表现有一定程度上的提升。 在我们的训练过程中，我们先用RoBERTa + 对抗训练得到teacher model，再用知识蒸馏的方法得到student模型。训练student时，我们同时采用真实label和teacher的输出来计算损失。 后处理在CoQA数据集中，有一小部分的问题是多选题，比如问题是“How Jack goes to school? Walk or ride? ”，而Context中的片段是“walked”。即使模型抽取到了“walked”，也并不能得分。因此，针对这类问题，我们做了一个简单的后处理。通过一定规则识别到多选题型的问题，然后抽取出问题中出现的选项，找到与我们模型抽取的Context片段语义相似度最高的选项，作为我们系统最终的回答。 结果分析与消融实验最终，我们的单模型在CoQA Leaderboard上超越了微软团队2.6个百分点，并首次超过了人工评测的水平。值得一提的是，与微软和其他团队不同，我们在模型训练的过程，没有用任何CoQA以外的有监督的数据集，进行多任务训练或是数据增强。 为了验证各个技巧的作用，我们进行了消融实验。从实验结果中可以看出，依据标记和对抗训练带来的提升较大，知识蒸馏和后处理也能带来一定程度的提升。 最终可以看到，利用对抗训练、知识蒸馏等方法，我们的单模型在RoBERTa Baseline的基础上提升了1.8个百分点。 总结这次的CoQA挑战赛经历，是我们团队站在巨人肩膀上的一次眺望。在优化模型的过程中，我们发现由于预训练模型已经相当强大，以往一些屡试不爽的优化技巧在RoBERTa上却并不能奏效。这就需要我们在比赛的过程中，大胆地设想，仔细地实验，验证不同方法的有效性和稳定性，从而找到真正行之有效的解决方案。希望我们的这次分享也能给其他团队带来一些经验上的启发。 对方案细节感兴趣的同学可以看英文报告：https://arxiv.org/abs/1909.10772 Reference:[1] CoQA Leaderboard. https://stanfordnlp.github.io/coqa/[2] SQuAD: 100,000+ Questions for Machine Comprehension of Text. https://arxiv.org/abs/1606.05250[3] CoQA: A Conversational Question Answering Challenge. https://arxiv.org/abs/1808.07042[4] RoBERTa: A Robustly Optimized BERT Pretraining Approach. https://arxiv.org/abs/1907.11692[5] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. https://arxiv.org/abs/1810.04805[6] Adversarial Training Methods for Semi-Supervised Text Classification. https://arxiv.org/abs/1605.07725[7] Distilling the Knowledge in a Neural Network. https://arxiv.org/abs/1503.02531[8] Born Again Neural Networks. https://arxiv.org/abs/1805.04770]]></content>
      <categories>
        <category>比赛</category>
        <category>机器阅读理解</category>
      </categories>
      <tags>
        <tag>对抗训练</tag>
        <tag>机器阅读理解</tag>
        <tag>RoBERTa</tag>
        <tag>知识蒸馏</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【训练技巧】功守道：NLP中的对抗训练 + PyTorch实现]]></title>
    <url>%2F2019%2F10%2F15%2Fadversarial-train%2F</url>
    <content type="text"><![CDATA[本文分享一个“万物皆可盘”的NLP对抗训练实现，只需要四行代码即可调用。你值得拥有。 最近，微软的FreeLB-Roberta [1] 靠着对抗训练 (Adversarial Training) 在GLUE榜上超越了Facebook原生的Roberta，追一科技也用到了这个方法仅凭单模型 [2] 就在CoQA榜单中超过了人类，似乎“对抗训练”一下子变成了NLP任务的一把利器。刚好笔者最近也在看这方面的内容，所以开一篇博客，讲一下。 提到“对抗”，相信大多数人的第一反应都是CV中的对抗生成网络 (GAN)，殊不知，其实对抗也可以作为一种防御机制，并且经过简单的修改，便能用在NLP任务上，提高模型的泛化能力。关键是，对抗训练可以写成一个插件的形式，用几行代码就可以在训练中自由地调用，简单有效，使用成本低。不过网上的大多数博客对于NLP中的对抗训练都介绍得比较零散且无代码实现，笔者在这篇博客中，对NLP任务中的对抗训练做了一个简单的综述，并提供了插件形式的PyTorch实现。 本文专注于NLP对抗训练的介绍，对对抗攻击基础感兴趣的读者，可以看这几篇博客及论文 [3] [4] [5]，这里就不赘述了。不想要理解理论细节的读者也可以直接看最后的代码实现。 对抗样本我们常常会听到“对抗样本”、“对抗攻击”、“对抗训练”等等这些令人头秃的概念，为了让大家对“对抗”有个更清晰的认识，我们先把这些概念捋捋清楚。 Szegedy在14年的ICLR中 [6] 提出了对抗样本这个概念。如上图，对抗样本可以用来攻击和防御，而对抗训练其实是“对抗”家族中防御的一种方式，其基本的原理呢，就是通过添加扰动构造一些对抗样本，放给模型去训练，以攻为守，提高模型在遇到对抗样本时的鲁棒性，同时一定程度也能提高模型的表现和泛化能力。 那么，什么样的样本才是好的对抗样本呢？对抗样本一般需要具有两个特点： 相对于原始输入，所添加的扰动是微小的； 能使模型犯错。 下面是一个对抗样本的例子，决定就是你啦，胖达： 对抗训练的基本概念GAN之父Ian Goodfellow在15年的ICLR中 [7] 第一次提出了对抗训练这个概念，简而言之，就是在原始输入样本 $x$ 上加一个扰动 $r_{adv}$ ，得到对抗样本后，用其进行训练。也就是说，问题可以被抽象成这么一个模型： \min_{\theta}-\log P(y|x+r_{adv};\theta)其中，$y$为gold label，$\theta$ 为模型参数。那扰动要如何计算呢？Goodfellow认为，神经网络由于其线性的特点，很容易受到线性扰动的攻击。 This linear behavior suggests that cheap, analytical perturbations of a linear model should also damage neural networks. 于是，他提出了 Fast Gradient Sign Method (FGSM) ，来计算输入样本的扰动。扰动可以被定义为： r_{adv} = \epsilon \cdot \text{sgn}(\triangledown_x L(\theta, x, y))其中，$\text{sgn}$为符号函数，$L$为损失函数。Goodfellow发现，令$\epsilon=0.25$，用这个扰动能给一个单层分类器造成99.9%的错误率。看似这个扰动的发现有点拍脑门，但是仔细想想，其实这个扰动计算的思想可以理解为：将输入样本向着损失上升的方向再进一步，得到的对抗样本就能造成更大的损失，提高模型的错误率。回想我们上一节提到的对抗样本的两个要求，FGSM刚好可以完美地解决。 在 [7] 中，Goodfellow还总结了对抗训练的两个作用： 提高模型应对恶意对抗样本时的鲁棒性； 作为一种regularization，减少overfitting，提高泛化能力。 Min-Max 公式在 [7] 中，对抗训练的理论部分被阐述得还是比较intuitive，Madry在2018年的ICLR中 [8]总结了之前的工作，并从优化的视角，将问题重新定义成了一个找鞍点的问题，也就是大名鼎鼎的Min-Max公式： \min_\theta \mathbb{E}_{(x, y)\sim \mathcal{D}} [\max_{r_{adv} \in \mathcal{S}} L(\theta, x+r_{adv}, y)]该公式分为两个部分，一个是内部损失函数的最大化，一个是外部经验风险的最小化。 内部max是为了找到worst-case的扰动，也就是攻击，其中，$L$ 为损失函数，$\mathcal{S}$ 为扰动的范围空间。 外部min是为了基于该攻击方式，找到最鲁棒的模型参数，也就是防御，其中$\mathcal{D}$是输入样本的分布。 Madry认为，这个公式简单清晰地定义了对抗样本攻防“矛与盾”的两个问题：如何构造足够强的对抗样本？以及，如何使模型变得刀枪不入？剩下的，就是如何求解的问题了。 从 CV 到 NLP以上提到的一些工作都还是停留在CV领域的，那么问题来了，可否将对抗训练迁移到NLP上呢？答案是肯定的，但是，我们得考虑这么几个问题： 首先，CV任务的输入是连续的RGB的值，而NLP问题中，输入是离散的单词序列，一般以one-hot vector的形式呈现，如果直接在raw text上进行扰动，那么扰动的大小和方向可能都没什么意义。Goodfellow在17年的ICLR中 [9] 提出了可以在连续的embedding上做扰动： Because the set of high-dimensional one-hot vectors does not admit inﬁnitesimal perturbation, we deﬁne the perturbation on continuous word embeddings instead of discrete word inputs. 乍一思考，觉得这个解决方案似乎特别完美。然而，对比图像领域中直接在原始输入加扰动的做法，在embedding上加扰动会带来这么一个问题：这个被构造出来的“对抗样本”并不能map到某个单词，因此，反过来在inference的时候，对手也没有办法通过修改原始输入得到这样的对抗样本。我们在上面提到，对抗训练有两个作用，一是提高模型对恶意攻击的鲁棒性，二是提高模型的泛化能力。在CV任务，根据经验性的结论，对抗训练往往会使得模型在非对抗样本上的表现变差，然而神奇的是，在NLP任务中，模型的泛化能力反而变强了，如[1]中所述： While adversarial training boosts the robustness, it is widely accepted by computer vision researchers that it is at odds with generalization, with classiﬁcation accuracy on non-corrupted images dropping as much as 10% on CIFAR-10, and 15% on Imagenet (Madry et al., 2018; Xie et al., 2019). Surprisingly, people observe the opposite result for language models (Miyato et al., 2017; Cheng et al., 2019), showing that adversarial training can improve both generalization and robustness. 因此，在NLP任务中，对抗训练的角色不再是为了防御基于梯度的恶意攻击，反而更多的是作为一种regularization，提高模型的泛化能力。 有了这些“思想准备”，我们来看看NLP对抗训练的常用的几个方法和具体实现吧。 NLP中的两种对抗训练 + PyTorch实现Fast Gradient Method（FGM）上面我们提到，Goodfellow在15年的ICLR [7] 中提出了Fast Gradient Sign Method（FGSM），随后，在17年的ICLR [9]中，Goodfellow对FGSM中计算扰动的部分做了一点简单的修改。假设输入的文本序列的embedding vectors $[v_1, v_2, …, v_T]$为$x$，embedding的扰动为： \begin{align} r_{adv} &= \epsilon \cdot g/||g||_2 \\\\ g &= \triangledown_x L(\theta, x, y) \end{align}实际上就是取消了符号函数，用二范式做了一个scale，需要注意的是：这里的norm计算的是，每个样本的输入序列中出现过的词组成的矩阵的梯度norm。原作者提供了一个TensorFlow的实现 [10]，在他的实现中，公式里的 $x$ 是embedding后的中间结果（batch_size, timesteps, hidden_dim），对其梯度 $g$ 的后面两维计算norm，得到的是一个(batch_size, 1, 1)的向量 $||g||_2$。为了实现插件式的调用，笔者将一个batch抽象成一个样本，一个batch统一用一个norm，由于本来norm也只是一个scale的作用，影响不大。笔者的实现如下： 12345678910111213141516171819202122class FGM(): def __init__(self, model): self.model = model self.backup = &#123;&#125; def attack(self, epsilon=1., emb_name='emb.'): # emb_name这个参数要换成你模型中embedding的参数名 for name, param in self.model.named_parameters(): if param.requires_grad and emb_name in name: self.backup[name] = param.data.clone() norm = torch.norm(param.grad) if norm != 0: r_at = epsilon * param.grad / norm param.data.add_(r_at) def restore(self, emb_name='emb.'): # emb_name这个参数要换成你模型中embedding的参数名 for name, param in self.model.named_parameters(): if param.requires_grad and emb_name in name: assert name in self.backup param.data = self.backup[name] self.backup = &#123;&#125; 需要使用对抗训练的时候，只需要添加五行代码： 1234567891011121314# 初始化fgm = FGM(model)for batch_input, batch_label in data: # 正常训练 loss = model(batch_input, batch_label) loss.backward() # 反向传播，得到正常的grad # 对抗训练 fgm.attack() # 在embedding上添加对抗扰动 loss_adv = model(batch_input, batch_label) loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度 fgm.restore() # 恢复embedding参数 # 梯度下降，更新参数 optimizer.step() model.zero_grad() PyTorch为了节约内存，在backward的时候并不保存中间变量的梯度。因此，如果需要完全照搬原作的实现，需要用register_hook接口[11]将embedding后的中间变量的梯度保存成全局变量，norm后面两维，计算出扰动后，在对抗训练forward时传入扰动，累加到embedding后的中间变量上，得到新的loss，再进行梯度下降。这里不赘述了，感兴趣的读者可以自行实现。 Projected Gradient Descent（PGD）内部max的过程，本质上是一个非凹的约束优化问题，FGM解决的思路其实就是梯度上升，那么FGM简单粗暴的“一步到位”，是不是有可能并不能走到约束内的最优点呢？当然是有可能的。于是，一个很intuitive的改进诞生了：Madry在18年的ICLR中[8]，提出了用Projected Gradient Descent（PGD）的方法，简单的说，就是“小步走，多走几步”，如果走出了扰动半径为$\epsilon$的空间，就映射回“球面”上，以保证扰动不要过大： \begin{align} x_{t+1} &= \Pi_{x+\mathcal{S}}(x_t+\alpha g(x_t)/||g(x_t)||_2) \\\\ g(x_t) &= \triangledown_x L(\theta, x_t, y) \end{align}其中$\mathcal{S}=\{r\in\mathbb{R}^d:||r||_2 \leq \epsilon\}$ 为扰动的约束空间，$\alpha$为小步的步长。 1234567891011121314151617181920212223242526272829303132333435363738394041class PGD(): def __init__(self, model): self.model = model self.emb_backup = &#123;&#125; self.grad_backup = &#123;&#125; def attack(self, epsilon=1., alpha=0.3, emb_name='emb.', is_first_attack=False): # emb_name这个参数要换成你模型中embedding的参数名 for name, param in self.model.named_parameters(): if param.requires_grad and emb_name in name: if is_first_attack: self.emb_backup[name] = param.data.clone() norm = torch.norm(param.grad) if norm != 0: r_at = alpha * param.grad / norm param.data.add_(r_at) param.data = self.project(name, param.data, epsilon) def restore(self, emb_name='emb.'): # emb_name这个参数要换成你模型中embedding的参数名 for name, param in self.model.named_parameters(): if param.requires_grad and emb_name in name: assert name in self.emb_backup param.data = self.emb_backup[name] self.emb_backup = &#123;&#125; def project(self, param_name, param_data, epsilon): r = param_data - self.emb_backup[param_name] if torch.norm(r) &gt; epsilon: r = epsilon * r / torch.norm(r) return param_data + r def backup_grad(self): for name, param in self.model.named_parameters(): if param.requires_grad: self.grad_backup[name] = param.grad def restore_grad(self): for name, param in self.model.named_parameters(): if param.requires_grad: param.grad = self.grad_backup[name] 使用的时候，要麻烦一点： 1234567891011121314151617181920pgd = PGD(model)K = 3for batch_input, batch_label in data: # 正常训练 loss = model(batch_input, batch_label) loss.backward() # 反向传播，得到正常的grad pgd.backup_grad() # 对抗训练 for t in range(K): pgd.attack(is_first_attack=(t==0)) # 在embedding上添加对抗扰动, first attack时备份param.data if t != K-1: model.zero_grad() else: pgd.restore_grad() loss_adv = model(batch_input, batch_label) loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度 pgd.restore() # 恢复embedding参数 # 梯度下降，更新参数 optimizer.step() model.zero_grad() 在[8]中，作者将这一类通过一阶梯度得到的对抗样本称之为“一阶对抗”，在实验中，作者发现，经过PGD训练过的模型，对于所有的一阶对抗都能得到一个低且集中的损失值，如下图所示： 我们可以看到，面对约束空间 $\mathcal{S}$ 内随机采样的十万个扰动，PGD模型能够得到一个非常低且集中的loss分布，因此，在论文中，作者称PGD为“一阶最强对抗”。也就是说，只要能搞定PGD对抗，别的一阶对抗就不在话下了。 实验对照为了说明对抗训练的作用，笔者选了四个GLUE中的任务进行了对照试验。实验代码是用的Huggingface的transfomers/examples/run_glue.py [12]，超参都是默认的，对抗训练用的也是相同的超参。 任务 Metrics BERT-Base FGM PGD MRPC Accuracy 83.6 86.8 85.8 CoLA Matthew’s corr 56.0 56.0 56.8 STS-B Person/Spearman corr. 89.3/88.8 89.3/88.8 89.3/88.9 RTE Accuracy 64.3 65.0 68.2 我们可以看到，对抗训练还是有效的，在MRPC和RTE任务上甚至可以提高三四个百分点。不过，根据我们使用的经验来看，是否有效有时也取决于数据集。毕竟： 缘，妙不可言~ 总结这篇博客梳理了NLP对抗训练发展的来龙去脉，介绍了对抗训练的数学定义，并对于两种经典的对抗训练方法，提供了插件式的实现，做了简单的实验对照。由于笔者接触对抗训练的时间也并不长，如果文中有理解偏差的地方，希望读者不吝指出。 一个彩蛋：Virtual Adversarial Training除了监督训练，对抗训练还可以用在半监督任务中，尤其对于NLP任务来说，很多时候输入的无监督文本多的很，但是很难大规模地进行标注，那么就可以参考[13]中提到的Virtual Adversarial Training进行半监督训练。 首先，我们抽取一个随机标准正态扰动（$d\sim \mathcal{N}(0, I)\in \mathbb{R}^d$），加到embedding上，并用KL散度计算梯度： \begin{align} g &= \triangledown_{x'}D_{KL}(p(\cdot|x;\theta)||p(\cdot|x';\theta)) \\\\ x' &= x + \xi d \end{align}然后，用得到的梯度，计算对抗扰动，并进行对抗训练： \begin{align} \min_\theta & D_{KL}(p(\cdot|x;\theta)||p(\cdot|x^*;\theta)) \\\\ x^* &= x+\epsilon g/||g||_2 \end{align}实现方法跟FGM差不多，这里就不给出了。 Reference[1]：FreeLB: Enhanced Adversarial Training for Language Understanding. https://arxiv.org/abs/1909.11764[2]：Technical report on Conversational Question Answering. https://arxiv.org/abs/1909.10772[3]：EYD与机器学习：对抗攻击基础知识（一）. https://zhuanlan.zhihu.com/p/37260275[4]：Towards a Robust Deep Neural Network in Text Domain A Survey. https://arxiv.org/abs/1902.07285[5]：Adversarial Attacks on Deep Learning Models in Natural Language Processing: A Survey. https://arxiv.org/abs/1901.06796[6]：Intriguing properties of neural networks. https://arxiv.org/abs/1312.6199[7]：Explaining and Harnessing Adversarial Examples. https://arxiv.org/abs/1412.6572[8]：Towards Deep Learning Models Resistant to Adversarial Attacks. https://arxiv.org/abs/1706.06083[9]：Adversarial Training Methods for Semi-Supervised Text Classification. https://arxiv.org/abs/1605.07725[10]：Adversarial Text Classification原作实现. https://github.com/tensorflow/models/blob/e97e22dfcde0805379ffa25526a53835f887a860/research/adversarial_text/adversarial_losses.py[11]：register_hook api. https://www.cnblogs.com/SivilTaram/p/pytorch_intermediate_variable_gradient.html[12]：huggingface的transformers. https://github.com/huggingface/transformers/tree/master/examples[13]：Distributional Smoothing with Virtual Adversarial Training. https://arxiv.org/abs/1507.00677]]></content>
      <categories>
        <category>训练技巧</category>
        <category>对抗训练</category>
      </categories>
      <tags>
        <tag>对抗训练</tag>
        <tag>PyTorch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【参会笔记】2019语言与智能高峰论坛]]></title>
    <url>%2F2019%2F09%2F08%2Flic-2019%2F</url>
    <content type="text"><![CDATA[瓦砾上上周参加了第四届语言与智能高峰论坛，包括周明、刘群、刘知远等很多大拿去做了报告，私以为其中几个报告挺不错的，颇有启发。刚好最近官方放出来PPT，这里做一个总结分享~ PPT的链接：http://tcci.ccf.org.cn/summit/2019/dl.php 总的来说，这次论坛有两个关键词：知识和多模态。 刘群 - 基于深度学习的自然语言处理：边界在哪里？刘群老师结合机器翻译领域，就NLP中DL的边界提出了三个问题： 深度学习解决了哪些NLP的问题？ 还有哪些NLP问题还没被深度学习解决？ 基于DL的NLP：边界在哪里？ 1. 深度学习解决了哪些NLP的问题？ 词语形态问题：以前需要进行词语切分，NMT不需要。 句法结构问题：NMT也不需要句法结构作为输入。 多语言问题：NMT通过中间语言方法解决了多语言翻译的问题。 联合训练问题：NMT端到端，避免了错误传播。 2. 还有哪些NLP问题还没被深度学习解决？ 资源稀缺问题：深度学习需要的数据更多 可解释性问题：nn的不可解释性 可信任性问题：既然不可解释，也就很难信任 可控制性问题：某些人名，希望严格翻译 超长文本问题：长下文没法参考太长，过长翻译不准确。 缺乏常识问题：需要图谱介入，也许能解决 前面两部分，刘群老师结合了机器翻译的发展，对比RBMT、SMT和NMT的区别，说明了DL解决了的以及未解决的NLP的问题，这两部分PPT讲的很详细也没什么需要思考的内容，建议大家看一下原PPT。 3. 边界？这一部分比较引发思考。 1. 数据边界2. 语义边界人工智能之所以能在围棋等项目上获得巨大的成功，一个很重要的原因是：这些问题都是well-defined 的。在这些问题中，我们队客观世界有着精确的建模，系统中所有的操作都在这个世界模型中。类似于智能音箱的产品之所以成功，也是因为它有着明确的任务集。 但是，事实上，大部分的自然语言模型只是在建模词语符号之间的关系，模型的“脑海”中是并没有一个属于自己的客观世界模型的，所以它其实并没有能真正地去理解语言。 一个可能的解决方法是知识图谱，其可以显性地包含大量的知识，避免一些常识性的错误，但是现在知识图谱并没能有一个大规模成功的应用。 刘群老师认为：理想的nlp系统，需要有一个描述客观世界的语义模型，类似于一种隐状态，知识图谱是这种模型的一种可能的形式。 3. 符号边界人类可以利用语言进行逻辑推理，神经网络却无法准确进行逻辑推理。一些简单的利用有限状态自动机可以解决的问题（数词、年份、网址），神经网络却很难准确地学习，这也是实用的nlp系统离不开规则的原因。 4. 因果边界人类对于客观世界发生事情的因果关系，有着明确的理解和判断，但是神经网络，只是基于统计数据找到的“规律”，并不能理解真正的因果关系。 万小军：Recent Advances in NaturalLanguage Generation瓦砾以前做过Question Generation，深感其中的不易，evaluation没有统一的标准，数据常常也不如翻译任务多。不过由于NLG任务本身真的很酷，一直还是对NLG方向很感兴趣。这次万老师讲了一些该领域最近的进展和热点方向，每个方向也都分享了相应的论文干货。 NLG的分类 NLG的趋势 Creative Text Generation （诗歌、比喻） Controllable Text Generation（固定长度、句法） Generating Texts with Special Attributes（风格迁移） Cross-Modal Text Generation（跨模态：图像视频的标题字幕、评论的生成） Question Generation（问题的生成） 万老师的ppt也很详细，由于启发性的东西不多，瓦砾就不复制粘贴了，感兴趣的读者可以看一下ppt，了解一下当下NLG能做什么，未来会怎么发展。 刘知远：知识计算与语言理解微博上一直关注刘老师，仰慕已久，这次听到真人的报告，确实条理清晰，鞭辟入里。 刘老师关于深度学习和知识如何相辅相成，共同驱动NLP任务，做了报告。 深度学习的挑战深度学习能够高效学习语言单元间复杂的语义关联，因此已经能解决很多任务，但是同时，也面临很多挑战，举个例子： 从“夏天像烤箱一样”过渡到“中暑如何治疗”，对有专业知识的人类来说，非常的自然。但对于深度学习模型来说，却有着一道难以逾越的沟壑，而知识也许就是跨越这道沟壑的桥梁。 然后刘老师祭出了一张笔者认为略精髓的图，很清楚地阐述了深度学习和知识图谱之间的关系以及其中涉及的技术方向。刘老师在后面也放了他们实验室做的一些成果，感兴趣的读者可以去看ppt里的paper list。 总结这次短短一天的参会，收获颇丰，尤其是刘群以及刘知远老师的介绍。以后有机会瓦砾也会深挖一些知识图谱方面的技术方向，也希望可以和大家多多交流。]]></content>
  </entry>
  <entry>
    <title><![CDATA[【PyTorch】唯快不破：基于Apex的混合精度加速]]></title>
    <url>%2F2019%2F08%2F26%2Ffp16%2F</url>
    <content type="text"><![CDATA[你想获得双倍训练速度的快感吗？你想让你的显存空间瞬间翻倍吗？如果我告诉你只需要三行代码即可实现，你信不？ 在这篇博客里，瓦砾会详解一下混合精度计算（Mixed Precision），并介绍一款Nvidia开发的基于PyTorch的混合精度训练加速神器—Apex，最近Apex更新了API，可以用短短三行代码就能实现不同程度的混合精度加速，训练时间直接缩小一半。 话不多说，直接先教你怎么用。 PyTorch实现1234from apex import ampmodel, optimizer = amp.initialize(model, optimizer, opt_level="O1") # 这里是“欧一”，不是“零一”with amp.scale_loss(loss, optimizer) as scaled_loss: scaled_loss.backward() 对，就是这么简单，如果你不愿意花时间深入了解，读到这基本就可以直接使用起来了。 但是如果你希望对FP16和Apex有更深入的了解，或是在使用中遇到了各种不明所以的“Nan”的同学，可以接着读下去，后面会有一些有趣的理论知识和瓦砾最近一个月使用Apex遇到的各种bug，不过当你深入理解并解决掉这些bug后，你就可以彻底摆脱“慢吞吞”的FP32啦。 理论部分为了充分理解混合精度的原理，以及API的使用，先补充一点基础的理论知识。 1. 什么是FP16？半精度浮点数是一种计算机使用的二进制浮点数数据类型，使用2字节（16位）存储。 其中，sign位表示正负，exponent位表示指数（$2^{n-15+{1}(n=0)}$），fraction位表示的是分数（$\frac{m}{1024}$）。其中当指数为零的时候，下图加号左边为0，其他情况为1。 2. 为什么需要FP16？在使用FP16之前，我想再赘述一下为什么我们使用FP16。 减少显存占用现在模型越来越大，当你使用Bert这一类的预训练模型时，往往显存就被模型及模型计算占去大半，当想要使用更大的Batch Size的时候会显得捉襟见肘。由于FP16的内存占用只有FP32的一半，自然地就可以帮助训练过程节省一半的显存空间。 加快训练和推断的计算与普通的空间时间Trade-off的加速方法不同，FP16除了能节约内存，还能同时节省模型的训练时间。在大部分的测试中，基于FP16的加速方法能够给模型训练带来多一倍的加速体验（爽感类似于两倍速看肥皂剧）。 张量核心的普及硬件的发展同样也推动着模型计算的加速，随着Nvidia张量核心（Tensor Core）的普及，16bit计算也一步步走向成熟，低精度计算也是未来深度学习的一个重要趋势，再不学习就out啦。 3. FP16带来的问题：量化误差这个部分是整个博客最重要的理论核心。讲了这么多FP16的好处，那么使用FP16的时候有没有什么问题呢？当然有。FP16带来的问题主要有两个：1. 溢出错误；2. 舍入误差。 溢出错误（Grad Overflow / Underflow）由于FP16的动态范围（$6 \times 10^{-8} \sim 65504$）比FP32的动态范围（$1.4 \times 10^{-45} \sim 1.7 \times 10^{38}$）要狭窄很多，因此在计算过程中很容易出现上溢出（Overflow，$g&gt;65504$）和下溢出（Underflow，$g&lt;6\times10^{-8}$）的错误，溢出之后就会出现“Nan”的问题。 在深度学习中，由于激活函数的的梯度往往要比权重梯度小，更易出现下溢出的情况。 舍入误差（Rounding Error）舍入误差指的是当梯度过小，小于当前区间内的最小间隔时，该次梯度更新可能会失败，用一张图清晰地表示： 4. 解决问题的办法：混合精度训练+动态损失放大 混合精度训练（Mixed Precision）混合精度训练的精髓在于“在内存中用FP16做储存和乘法从而加速计算，用FP32做累加避免舍入误差”。混合精度训练的策略有效地缓解了舍入误差的问题。 损失放大（Loss Scaling）即使用了混合精度训练，还是会存在无法收敛的情况，原因是激活梯度的值太小，造成了下溢出（Underflow）。损失放大的思路是： 反向传播前，将损失变化（dLoss）手动增大$2^k$倍，因此反向传播时得到的中间变量（激活函数梯度）则不会溢出； 反向传播后，将权重梯度缩$2^k$倍，恢复正常值。 Apex的新API：Automatic Mixed Precision (AMP)曾经的Apex混合精度训练的api仍然需要手动half模型已经输入的数据，比较麻烦，现在新的api只需要三行代码即可无痛使用：1234from apex import ampmodel, optimizer = amp.initialize(model, optimizer, opt_level="O1") # 这里是“欧一”，不是“零一”with amp.scale_loss(loss, optimizer) as scaled_loss: scaled_loss.backward() opt_level 其中只有一个opt_level需要用户自行配置： O0：纯FP32训练，可以作为accuracy的baseline； O1：混合精度训练（推荐使用），根据黑白名单自动决定使用FP16（GEMM, 卷积）还是FP32（Softmax）进行计算。 O2：“几乎FP16”混合精度训练，不存在黑白名单，除了Batch norm，几乎都是用FP16计算。 O3：纯FP16训练，很不稳定，但是可以作为speed的baseline； 动态损失放大（Dynamic Loss Scaling） AMP默认使用动态损失放大，为了充分利用FP16的范围，缓解舍入误差，尽量使用最高的放大倍数（$2^{24}$），如果产生了上溢出（Overflow），则跳过参数更新，缩小放大倍数使其不溢出，在一定步数后（比如2000步）会再尝试使用大的scale来充分利用FP16的范围： 干货：踩过的那些坑这一部分是整篇博客最干货的部分，是瓦砾在最近在apex使用中的踩过的所有的坑，由于apex报错并不明显，常常debug得让人很沮丧，但只要注意到以下的点，95%的情况都可以畅通无阻了： 判断你的GPU是否支持FP16：构拥有Tensor Core的GPU（2080Ti、Titan、Tesla等），不支持的（Pascal系列）就不建议折腾了。 常数的范围：为了保证计算不溢出，首先要保证人为设定的常数（包括调用的源码中的）不溢出，如各种epsilon，INF等。 Dimension最好是8的倍数：Nvidia官方的文档的2.2条表示，维度都是8的倍数的时候，性能最好。 涉及到sum的操作要小心，很容易溢出，类似Softmax的操作建议用官方API，并定义成layer写在模型初始化里。 模型书写要规范：自定义的Layer写在模型初始化函数里，graph计算写在forward里。 某些不常用的函数，在使用前需要注册：amp.register_float_function(torch, &#39;sigmoid&#39;) 某些函数（如einsum）暂不支持FP16加速，建议不要用的太heavy，xlnet的实现改FP16困扰了我很久。 需要操作模型参数的模块（类似EMA），要使用AMP封装后的model。 需要操作梯度的模块必须在optimizer的step里，不然AMP不能判断grad是否为Nan。 欢迎补充。。。 总结这篇从理论到实践地介绍了混合精度计算以及Apex新API（AMP）的使用方法。瓦砾现在在做深度学习模型的时候，几乎都会第一时间把代码改成混合精度训练的了，速度快，精度还不减，确实是调参炼丹必备神器。目前网上还并没有看到关于AMP以及使用时会遇到的坑的中文博客，所以这一篇也是希望大家在使用的时候可以少花一点时间debug。当然，如果读者们有发现新的坑欢迎交流，我会补充在博客中。 Reference Intel的低精度表示用于深度学习训练与推断 Nvidia官方的混合精度训练文档 Apex官方使用文档 Nvidia-Training Neural Networks with Mixed Precision]]></content>
      <categories>
        <category>训练方法</category>
        <category>混合精度训练</category>
      </categories>
      <tags>
        <tag>深度学习加速</tag>
        <tag>混合精度训练</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【优化器】优化器算法及PyTorch实现（一）：永不磨灭的SGD]]></title>
    <url>%2F2019%2F08%2F10%2Foptimizer_sgd%2F</url>
    <content type="text"><![CDATA[瓦砾准备写一个关于优化器算法的系列，主要面向总是把优化器当做黑盒使用的，或是对优化器算法有些遗忘的读者。希望大家在看完这个系列后，能达成以下几点成就： 对深度学习优化器的进化史，有一个系统的了解； 对应不同的任务，知道应选用哪种优化器； 能够理解每种优化器参数的意义； 能够根据自己的需求，在PyTorch上做一些自定义的优化器的改动。 Gradient Descent1. 梯度下降的三种变体梯度下降有三种变体： 批梯度下降 BGD (Batch gradient descent) 随机梯度下降 SGD (Stochastic gradient descent) 小批梯度下降 MBGD (Mini-batch gradient descent) a. Batch gradient descent (BGD)批梯度下降（Batch gradient descent，又称之为Vanilla gradient descent），顾名思义是用全部的数据样本计算平均loss之后，再得到梯度进行下降： \theta = \theta - \eta \cdot \triangledown_\theta J(\theta)其中$\theta$为模型参数，$\triangledown_\theta J(\theta)$为各个参数的梯度，$\eta$为学习率。但是BGD有两个最大的问题： 容易陷入局部最优点和鞍点。 遍历整个数据集才走一步，这样太耗时，且由于内存的原因，对于较大数据量的几乎不可能实现。 b. Stochastic gradient descent (SGD)随机梯度下降则是另一个极端，每个样本都会计算loss、梯度，然后更新： \theta = \theta - \eta \cdot \triangledown_\theta J(\theta; x^{i};y^{i})SGD完美解决了上面的两点问题，并且可以用来online-learning，但是，SGD做了很多冗余的计算，且不会很快收敛，容易震荡。 c. Mini-batch gradient descent (MBGD)MBGD是BGD和SGD的这种，每次取m个样本，计算平均loss、梯度，然后更新： \theta = \theta - \eta \cdot \triangledown_\theta J(\theta; x^{i: i+m};y^{i:i+m})MBGD避开了前两种的缺点，并且： 减少参数的更新次数，使用更新更稳定； 充分利用gpu的并行计算，提高计算效率。 一般情况，若无特殊说明，SGD优化器指的就是MBGD的方法。（更细节的讲BGD和SGD的优劣的可以看 @袁洋 大佬的这篇：https://zhuanlan.zhihu.com/p/27609238，私以为讲的很好） 2. 挑战梯度下降是所有深度学习优化器的基础。但是想要明白为什么需要优化SGD，首先我们得知道Vanilla SGD优化器面临的挑战： 对于SGD来说，挑选一个学习率（固定）是很很困难的。选小了，收敛太慢；选大了，阻碍收敛甚至容易发散。 如果我们想用Learning rate schedule的方式，随着epoch数去减小学习率（lr退火），也只能用一个事先定义好方式去annealing，不能根据当前batch数据的特点去动态的decay。 所有的参数都用的是一个学习率，不能自适应地调节。 最重要的一个问题是：由于SGD更新的参数仅由学习率和当前batch的梯度决定，模型很容易陷入次优点和鞍点。 下面我们看看后续的改进都是如何一步步优化我们这里提到的四点挑战的。 SGD优化器的几点优化我们先看看在基于SGD的梯度更新上的两点改动： Momentum Nesterov accelerated gradient 1. Momentum动量按吴恩达老师所说的，梯度下降（Gradient Descent）就好比下山，目标则是以最快的速度抵达山下。想象我们的SGD优化器“小明”一路披荆斩棘来到一个峡谷，左右两边极其陡峭，前面有个小土堆，后面是刚刚下山的路，此时根据模型计算的梯度方向，非左即右，“小明”就只能懵逼地左右来回的跑，怎么也无法跨过前面的小土堆了（也就是挑战4的次优点问题）。那么有什么办法让“小明”记住刚刚下山的方向，面向小土堆，跨过它继续下山呢？ 一个最简单的Intuition就是我们把“小明”搓成一个球，让他滚下山坡，这样他就保持了一定的惯性，可以顺利的滚过小土堆了。因此1964年，Polyak提出了Momentum动量这个概念，公式如下： v_t = \gamma v_{t-1}+\eta\triangledown_\theta J(\theta) \\\\ \theta = \theta - v_t深度学习中，$\gamma$一般设置为0.9（或者0.99，更严谨的选择参考L. Vandenberghe的EE236C的课件，这里不展开了）。其实就是在下降的时候不仅考虑当前batch计算的梯度，也考虑到之前实际下降的梯度，从而跨过局部最优。 此外，动量的存在还可以扩大可收敛的学习率范围，缓解挑战1的问题，并加速模型收敛的速度。 当然，这里的解释只是为了帮助理解，并不严谨，感兴趣的同学可以看Gabriel的这篇Why Momentum Really Works，讲的很详细，还有有趣的可视化插件可以玩（可以对比一下有无Momentum模型下降的节奏）。2333。 2. Nesterov accelerated gradient (NAG)细心的读者可能已经发现了上面的参数更新的一个很大的“bug”，实际上梯度则是根据$\theta$算出来的，然而参数是在$\theta-\gamma v_{t-1}$的基础上更新的，这跟我们SGD的“搁哪算，搁哪降”的原则就相违背了。因此，Nesterov在1983年提出了NAG，让模型先预见性地往前走完惯性导致的一步，再根据当前点计算真实的梯度，再更新。 \theta' = \theta-\gamma v_{t-1} \\\\ v_t = \gamma v_{t-1}+\eta\triangledown_\theta J(\theta') \\\\ \theta = \theta - v_tIlya Sutskever在2013年的博士论文Training Recurrent Neural Networks 中给了更多的细节。 3. PyTorch实现PyTorch官方的实现：https://github.com/fyubang/pytorch_optimizers 核心代码： 12345678910111213141516for p in group['params']: d_p = p.grad.data if weight_decay != 0: d_p.add_(weight_decay, p.data) if momentum != 0: param_state = self.state[p] if 'momentum_buffer' not in param_state: buf = param_state['momentum_buffer'] = torch.clone(d_p).detach() else: buf = param_state['momentum_buffer'] buf.mul_(momentum).add_(1 - dampening, d_p) if nesterov: d_p = d_p.add(momentum, buf) else: d_p = buf p.data.add_(-group['lr'], d_p) a. Momentum这里要注意，PyTorch官方的Momentum实现在Sutskever的公式上做了一点优化： v_t = \gamma v_{t-1}+\triangledown_\theta J(\theta) \\\\ \theta = \theta - \eta v_t当学习率固定时，两种更新方法是等价的(issue 1099，读者可以令$v_0=0$自己推导一下，很简单)，但是如果学习率是会随着step衰减的话，PyTorch的实现的Momentum可以立即随之衰减，而原来的实现需要较长的时间才能衰减到正常的水平。 b. NAG用数学公式表示NAG的思想是非常简单且严谨的，然而在计算框架上要如何实现呢？PyTorch的框架是自动求导的，也就意味着：我们在forward之前就必须把参数走完动量的那一步从而得到$\theta’$，并只维护这一个参数，那么更新公式变为： \begin{align} \text{一步迭代：}\\\\ \theta_{t-1}' &= \theta_{t-1}-\eta\gamma v_{t-1} \\\\ v_t &= \gamma v_{t-1}+\triangledown_\theta J(\theta_{t-1}') \\\\ \theta_t &= \theta_{t-1} - \eta v_t \\\\ &= \theta_{t-1}' + \eta\gamma v_{t-1} - \eta v_t \\\\ \text{下一步迭代：}\\\\ \theta_t' &= \theta_t-\eta\gamma v_{t} \\\\ &= \theta_{t-1}' + \eta\gamma v_{t-1} - \eta v_t - \eta\gamma v_{t} \\\\ &= \theta_{t-1}' - \eta \triangledown_\theta J(\theta_{t-1}') -\eta \gamma v_t \\\\ v_{t+1} &= \gamma v_{t}+\triangledown_\theta J(\theta_{t}') \\\\ & ... \end{align}抹掉$\theta$参数，就可以得到： \begin{align} v_t &= \gamma v_{t-1}+\triangledown_\theta J(\theta_{t-1}') \\\\ \theta_t' &= \theta_{t-1}' - \eta (\triangledown_\theta J(\theta_{t-1}') + \gamma v_t) \end{align}这样我们就能把PyTorch的代码和NAG的更新公式统一了。虽然最后得到的是“多跨了一步动量”的参数，但是由于到最后靠近极值点的时候动量已经很小了，所以有一点误差也并无大碍。 我们现在已经能一定程度上缓解挑战1和挑战4的问题了，但是我们还是希望能针对不同参数用不同的更新幅度，该如何实现呢？ 下一章，我们介绍一些在SGD的变体：Adagrad、Adadelta、RMSprop等。 Reference: Polyak, B.T. (1964) Some methods of speeding up the convergence of iteration methods Nesterov, Y. (1983) A Method for Solving a Convex Programming Problem with Convergence Rate O(1/K2) Sutskever, I. (2013) Training Recurrent neural Networks. PhD Thesis. Sutskever, I. (2013) On the importance of initialization and momentum in deep learning Hinton, G. (2014) Neural Networks for Machine Learning—-Lecture 6a Overview of mini-batch gradient descent Ruder, S. (2018) An overview of gradient descent optimization algorithms]]></content>
      <categories>
        <category>优化</category>
        <category>深度学习优化器</category>
      </categories>
      <tags>
        <tag>优化器</tag>
        <tag>代码实现</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【论文笔记】寻找让机器心动的“信号”]]></title>
    <url>%2F2019%2F08%2F03%2Fsignal%2F</url>
    <content type="text"><![CDATA[不知道大家有没有这样的疑惑，在BERT横空出世之后，NLP算法工程师的工作开始变得越来越无聊了，想要用Train-from-Scratch的模型战胜芝麻街家族，几乎已无可能，往往绞尽脑汁可能还不如BERT加一个输出层Fine-Tune。那么，既然迁移学习的大势无法阻挡，作为算法工程师的我们如何能在BERT后时代里找到新的乐趣呢？ 瓦砾最近读到了一篇不错的博客：https://dawn.cs.stanford.edu/2019/03/22/glue/，从“信号”的角度分析了后BERT时代Fine-Tune模型时的发力点，颇有启发，故此翻译总结并分享。 Let’s do it。 机器学习的三个要素当解决一个NLP监督学习任务时，我们需要考虑三个要素： 模型 硬件，也就是计算资源 数据 其中，由于Google，Huggingface等大佬的开源贡献，想构建SOTA模型几乎只需要pip install xxx便唾手可得；加上各种云服务的诞生，算力也并不是很大的问题；然而，NLP领域中，高质量的监督数据，仍然是处于一种 low-resource 的状态，这也成为了大多数应用场景的瓶颈，所以，NLPer们开始寻找一些迂回的方式“围魏救赵”，给他们的模型注入一些间接的监督信号。 因此，我们想要搭建一套以监督信号为王的框架，使其能更好地利用如今潜在的一些监督信号，包括：传统的监督、迁移学习、多任务学习、弱监督等。我们称之为Massive Multi-Task Learning (MMTL)。我们以RTE（Recognizing Textual Entailment）任务为例，展示了如何通过逐渐增加“信号”，一步步地提高模型的表现。 RTE是一个经典的NLI任务，一共有2.5k个训练数据，任务的目标是判断第二个句子是否由第一个句子推断而来，数据量不大很符合我们实际的应用场景。 瓦砾在这用高考举例，带着读者walk through一遍给模型寻找信号的过程！ 做俩例题：传统的监督传统监督信号当然不能落下，这就好像高考前得做些例题，刷点“五三”一样。“老师，我还能刷两本黄冈！” 使用标准的Bi-LSTM训练，在该任务上能获得57.4的准确率，只比随机高了一点，即使我们加上ElMo embedding和注意力机制，也只能将分数提高到58.9。很不幸，无论我们的模型结构有多么的fancy，从2.5k监督信号中，That’s all we can learn。 我们需要更多信号！💪 读万卷书：迁移学习迁移学习就好比读书，一个只读过几篇满分作文的考生是写不出《杯中窥人》的，要想“作文”写的好，先得大量地阅读，即使不甚相关，也是一种积累，厚积才能薄发。 2018年被称为“NLP的ImageNet时刻”，因为这是迁移学习真正起飞，给NLP界带来飞速提升的一年。我们所熟知的ULMFit，GPT以及拥有三亿参数量的BERT在这一年大放异彩。这些模型都是用一些语言模型任务，佐以大量的数据训练而成，因此学习过更多更广泛的信号，在下游任务中游刃有余，鲁棒而又有效。 在实验中，我们使用简单的BERT上加一个线性变换来Fine-Tune RTE数据集，对比Baseline的58.9的得分，立即飙升了17.6的点，来到了76.5。虽然RTE数据量很小，但由于模型已经“看过山和大海，穿越过人山人海”，简单推理任务已经是一点就通，举一反三了。 我们又需要更多信号！💪 九门功课同步学：多任务学习作为目标清北的高考考生，怎能偏科？更何况数理化不分家，政史地生相互裨益。偏科？不，你只是过拟合了同学。 语言模型可以教会模型很多东西，但教不会它所有，比如做RTE任务，我们可以找一些类似的NLI任务，用multi-task learning的方式同步去学习。 Multi-Task Learning（MTL）是一种共享表达层，分离输出层从而去学习不同任务信号的学习方法。因为我们相信，重要的是“信号”，而不是模型结构，我们在BERT的基础上为每个任务只添加一个线性变换。我们使用MT-DNN中的训练方法：每个batch只包含一个任务，多任务之后再单独在各个任务上Fine-Tune一下。最终，RTE再次提高了6.9个百分点，达到了83.4的高分。 我们又双需要更多信号！💪 开个小灶：大量多任务学习高考路遥遥，不仅需要低头赶路，也得抬头看看天，问自己：“我哪里还不够优秀呢？” 有的放矢，做到哪里不会点哪里。 日常实验或业务中，Error Analysis是一个很重要的针对性地提高模型表现的方法，发个比方：当你发现Badcase大多来自于对文本语法的理解错误，就可以考虑增加一个句法解析的辅助任务；又或是，当你发现文本中存在诸多指代，则可尝试增加一个指代消解的辅助任务。需要的label则可以用开源的系统去生成，不可避免这样会引入一些错误，但模型依然还是可以利用这些信号，去针对性地补全缺失的能力。 我们又双叒需要更多信号！💪 整理错题集：数据切分在哪里跌倒，就得在哪里爬起来。做了错题不要紧，准备一本错题集，记录下来，分而治之，就怼它，死磕它。 在实验中，我们发现在RTE数据集中，有一些特殊的数据子集表现很差，比如：当我们的模型获得了83.4的准确率时，它在有罕见标点的数据中表现仅有76.7，在代词较多的数据中仅有58.3，这意味着，这些问题确实难，容易错。 面对上述的这两类问题，我们用一些规则识别出这类问题后，单独增加一个线性输出层去解决它，表达层仍是共享的，使得一小部分网络专注于解决这类问题，从而提高这部分数据的表现。用了这个方法，76.7提高至79.3，58.3提高至75.0，最终总得分达到84.1。赞。 我们又双叒叕需要更多信号！💪 九九归一：Ensembling高考的至高境界：人格分裂去高考，九人同行，一人高中。在每一门科目中，派出你不同的人格去高考，考语文时，你是张爱玲；考数学时，你是高斯；考英语时，你是韩梅梅。 Ensemble是大家的老朋友了，在后BERT时代，Ensemble也尤其重要，比如Cased和Uncased的模型，常常就擅长不同的任务，毕竟，三个臭皮匠，顶个诸葛亮；三个诸葛亮，顶九个臭皮匠。 打个总结其中本文的“干货”并没有那么多，更多地是一种思想和方法论，即：监督信号要重要于模型结构的些微调整，背后其实是算力与算法的权衡。瓦砾认为，这个观点在bert时代尤为适用。当然，并不是否定算法的作用，而是认为算法和算力应当互相补充，这里引用XLNet作者杨植麟的一段采访时说的话： 依靠算力解决问题是当前研究 AI 的王道：让计算机去做它的强项——计算；如果算力解决不了的问题，再用算法去做。 把算力推到极致的好处是知晓当前算法的边界，避免在算力可以解决的问题上做一些不必要的算法创新，让大家关注最重要的研究问题。但同时大算力带来的弊端是提升了研究门槛，比如一般的学校和实验室可能没有资源做预训练。这个问题我觉得短时间内要通过不同的分工来解决，资源多的研究者利用资源做大算力研究，资源少的研究者做基于小算力的研究。 至此，我们的“信号”寻找之旅告一段落啦，但百尺竿头更进一步，各位NLPer们继续加油，去寻找那些浩瀚星辰中未被发现的“信号”吧。 PS：写这篇博客真的是激起了瓦砾无数关于高考的回忆，其实，往往机器学习中很多的 Intuition 都是可以从生活中来的。最后，大家不要因为我这篇鸡汤写的多就取关啊。Orz。]]></content>
      <categories>
        <category>论文笔记</category>
        <category>预训练</category>
      </categories>
      <tags>
        <tag>多任务</tag>
        <tag>迁移学习</tag>
        <tag>BERT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【分布式训练】单机多卡的正确打开方式（四）：Horovod]]></title>
    <url>%2F2019%2F07%2F26%2Fdistributed-training4%2F</url>
    <content type="text"><![CDATA[讲完了单机多卡的分布式训练的理论、TensorFlow和PyTorch分别的实现后，今天瓦砾讲一个强大的第三方插件：Horovod。 Horovod是Uber开源的跨平台的分布式训练工具，名字来自于俄国传统民间舞蹈，舞者手牵手围成一个圈跳舞，与Horovod设备之间的通信模式很像，有以下几个特点： 兼容TensorFlow、Keras和PyTorch机器学习框架。 使用Ring-AllReduce算法，对比Parameter Server算法，有着无需等待，负载均衡的优点。 实现简单，五分钟包教包会。（划重点） Uber官方在git上给了很详细的例子： https://github.com/horovod/horovod/tree/master/examples，所以这里只简单讲一下大概的使用方法： TensorFlow以TF的Custom Training Loop API为例：12345678910111213141516171819202122232425import tensorflow as tfimport horovod.tensorflow as hvd# 1. 初始化horovodhvd.init()# 2. 给当前进程分配对应的gpu，local_rank()返回的是当前是第几个进程config = tf.ConfigProto()config.gpu_options.visible_device_list = str(hvd.local_rank())# 3. Scale学习率，封装优化器opt = tf.train.AdagradOptimizer(0.01 * hvd.size())opt = hvd.DistributedOptimizer(opt)# 4. 定义初始化的时候广播参数的hook，这个是为了在一开始的时候同步各个gpu之间的参数hooks = [hvd.BroadcastGlobalVariablesHook(0)]# 搭建model，定义lossloss = ...train_op = opt.minimize(loss)# 5. 只保存一份ckpt就行checkpoint_dir = '/tmp/train_logs' if hvd.rank() == 0 else None# 7. 用MonitoredTrainingSession实现初始化，读写ckptwith tf.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir, config=config, hooks=hooks) as mon_sess: while not mon_sess.should_stop(): # Perform synchronous training. mon_sess.run(train_op) 具体的代码看tensorflow_mnist.py：https://github.com/horovod/horovod/blob/master/examples/tensorflow_mnist.py 单机双卡训练输入以下命令：1CUDA_VISIBLE_DEVICES=6,7 horovodrun -np 2 -H localhost:2 python tensorflow_mnist.py 这里 -np指的是进程的数量。 执行之后可以看到如下的结果，因为多线程，每个step都打印了两遍。 1234567891011[1,0]&lt;stderr&gt;:INFO:tensorflow:loss = 0.13126025, step = 300 (0.191 sec)[1,1]&lt;stderr&gt;:INFO:tensorflow:loss = 0.01396352, step = 310 (0.177 sec)[1,0]&lt;stderr&gt;:INFO:tensorflow:loss = 0.063738815, step = 310 (0.182 sec)[1,1]&lt;stderr&gt;:INFO:tensorflow:loss = 0.044452004, step = 320 (0.215 sec)[1,0]&lt;stderr&gt;:INFO:tensorflow:loss = 0.028987963, step = 320 (0.212 sec)[1,0]&lt;stderr&gt;:INFO:tensorflow:loss = 0.09094897, step = 330 (0.206 sec)[1,1]&lt;stderr&gt;:INFO:tensorflow:loss = 0.11366991, step = 330 (0.210 sec)[1,0]&lt;stderr&gt;:INFO:tensorflow:loss = 0.08559138, step = 340 (0.200 sec)[1,1]&lt;stderr&gt;:INFO:tensorflow:loss = 0.037002128, step = 340 (0.201 sec)[1,0]&lt;stderr&gt;:INFO:tensorflow:loss = 0.15422738, step = 350 (0.181 sec)[1,1]&lt;stderr&gt;:INFO:tensorflow:loss = 0.06424393, step = 350 (0.179 sec) PyTorchTorch下也是类似的套路，但是由于PyTorch本身单机多卡训练已经够简单了，API也稳定，所以笔者一般做的时候就是直接用Torch自己的DP和DDP了。 1234567891011121314151617181920212223242526272829303132import torchimport horovod.torch as hvd# 1. 初始化horovodhvd.init()# 2. 给当前进程分配对应的gpu，local_rank()返回的是当前是第几个进程torch.cuda.set_device(hvd.local_rank())# Define dataset...train_dataset = ...# 3. 用DistributedSampler给各个worker分数据train_sampler = torch.utils.data.distributed.DistributedSampler( train_dataset, num_replicas=hvd.size(), rank=hvd.rank())train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=..., sampler=train_sampler)# Build model...model = ...model.cuda()# 4. 封装优化器optimizer = optim.SGD(model.parameters())optimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())# 5. 初始化的时候广播参数，这个是为了在一开始的时候同步各个gpu之间的参数hvd.broadcast_parameters(model.state_dict(), root_rank=0)# 训练for epoch in range(100): for batch_idx, (data, target) in enumerate(train_loader): optimizer.zero_grad() output = model(data) loss = F.nll_loss(output, target) loss.backward() optimizer.step() if batch_idx % args.log_interval == 0: print('Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125;]\tLoss: &#123;&#125;'.format( epoch, batch_idx * len(data), len(train_sampler), loss.item())) 速度瓦砾还没有来得及做一个全面的Horovod、tf.distribute和 Torch的单机多卡训练速度的横向对比，不过大家可以参考这两篇： Horovod: fast and easy distributed deep learning in TensorFlow Goodbye Horovod, Hello CollectiveAllReduce 总体而言，用了All-Reduce算法的API，速度应该都差不多，如果你是土豪，拥有NVLINK（卡间通信极快）的话，那忘了我说的这几篇“废话”吧朋友。Orz。 总结终于结束了单机多卡系列的最后一章，由于博客本身的限制，给的例子整体还是比较简单，以入门为主，大家具体使用的时候肯定还是会遇到一些坑，这里瓦砾把踩过的一些坑和解决办法列举在这，以避免大家以后重复踩坑： tf.contrib.distributed.MirroredStrategy 需要optimizer支持merge_call（bert实现的optimizer是直接修改apply_gradient的，所以会报错），这个时候就需要正确地修改optimizer里的_apply_dense、_apply_sparse(参考Issue 23986 和 JayYip)。或者用horovod，就可以避免这个问题。 Effective batch size，不同的多卡工具对输入的batch size的操作不一样，要确定最后进模型的effective batch size才有意义。一般来说，多进程的batch size指的是每张卡的batch size。 Learning rate scale，学习率要根据effective batch size调整。 All-Reduce由于是多进程的，数据流各自独立，为了防止同一个step多gpu的batch重叠，最好的的办法是在每个进程里根据local_rank设置shard的数据，保证各个gpu采样的数据不重叠。 为了使用horovod，新建docker container时，要加—privileged，否则会疯狂报warning，虽然没影响，但是看着难受。 Pytorch的DP多卡要注意最后一个batch的batch size不能小于gpu的数量，否则会报错，最保险的做法是drop_last，扔掉最后的batch。 并不是所有情况下All-Reduce都比PS好，比如当卡间通信用的是NVLink的时候，在gpu数量不多的情况下，数据传输的时间不是瓶颈，All-Reduce的提升就几乎没有了。 DP和DDP有一个区别在于BatchNorm。 DDP封装model后不能再改动model。 待补充。。。 Reference Horovod的官方给的一些例子。 Uber：如何用Horovod实现bert的单机多卡训练 Goodbye Horovod, Hello CollectiveAllReduce Horovod: fast and easy distributed deep learning in TensorFlow]]></content>
      <categories>
        <category>训练方法</category>
        <category>分布式训练</category>
      </categories>
      <tags>
        <tag>分布式训练</tag>
        <tag>多卡训练</tag>
        <tag>Horovod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【分布式训练】单机多卡的正确打开方式（三）：PyTorch]]></title>
    <url>%2F2019%2F07%2F23%2Fdistributed-training3%2F</url>
    <content type="text"><![CDATA[拖更拖更了，今天讲一下PyTorch下要如何单机多卡训练。 PyTorch的数据并行相对于TensorFlow而言，要简单的多，主要分成两个API： DataParallel（DP）：Parameter Server模式，一张卡位reducer，实现也超级简单，一行代码。 DistributedDataParallel（DDP）：All-Reduce模式，本意是用来分布式训练，但是也可用于单机多卡。 1. DataParallelDataParallel是基于Parameter server的算法，负载不均衡的问题比较严重，有时在模型较大的时候（比如bert-large），reducer的那张卡会多出3-4g的显存占用。 先简单定义一下数据流和模型。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import torchimport torch.nn as nnfrom torch.autograd import Variablefrom torch.utils.data import Dataset, DataLoaderimport osinput_size = 5output_size = 2batch_size = 30data_size = 30class RandomDataset(Dataset): def __init__(self, size, length): self.len = length self.data = torch.randn(length, size) def __getitem__(self, index): return self.data[index] def __len__(self): return self.lenrand_loader = DataLoader(dataset=RandomDataset(input_size, data_size), batch_size=batch_size, shuffle=True)class Model(nn.Module): # Our model def __init__(self, input_size, output_size): super(Model, self).__init__() self.fc = nn.Linear(input_size, output_size) def forward(self, input): output = self.fc(input) print(" In Model: input size", input.size(), "output size", output.size()) return outputmodel = Model(input_size, output_size)if torch.cuda.is_available(): model.cuda() if torch.cuda.device_count() &gt; 1: print("Let's use", torch.cuda.device_count(), "GPUs!") # 就这一行 model = nn.DataParallel(model) for data in rand_loader: if torch.cuda.is_available(): input_var = Variable(data.cuda()) else: input_var = Variable(data) output = model(input_var) print("Outside: input size", input_var.size(), "output_size", output.size()) 2. DDP官方建议用新的DDP，采用all-reduce算法，本来设计主要是为了多机多卡使用，但是单机上也能用，使用方法如下： 初始化使用nccl后端1torch.distributed.init_process_group(backend=&quot;nccl&quot;) 模型并行化1model=torch.nn.parallel.DistributedDataParallel(model) 需要注意的是：DDP并不会自动shard数据 如果自己写数据流，得根据torch.distributed.get_rank()去shard数据，获取自己应用的一份 如果用Dataset API，则需要在定义Dataloader的时候用DistributedSampler 去shard：12sampler = DistributedSampler(dataset) # 这个sampler会自动分配数据到各个gpu上DataLoader(dataset, batch_size=batch_size, sampler=sampler) 完整的例子：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import torchimport torch.nn as nnfrom torch.autograd import Variablefrom torch.utils.data import Dataset, DataLoaderimport osfrom torch.utils.data.distributed import DistributedSampler# 1) 初始化torch.distributed.init_process_group(backend="nccl")input_size = 5output_size = 2batch_size = 30data_size = 90# 2） 配置每个进程的gpulocal_rank = torch.distributed.get_rank()torch.cuda.set_device(local_rank)device = torch.device("cuda", local_rank)class RandomDataset(Dataset): def __init__(self, size, length): self.len = length self.data = torch.randn(length, size).to('cuda') def __getitem__(self, index): return self.data[index] def __len__(self): return self.lendataset = RandomDataset(input_size, data_size)# 3）使用DistributedSamplerrand_loader = DataLoader(dataset=dataset, batch_size=batch_size, sampler=DistributedSampler(dataset))class Model(nn.Module): def __init__(self, input_size, output_size): super(Model, self).__init__() self.fc = nn.Linear(input_size, output_size) def forward(self, input): output = self.fc(input) print(" In Model: input size", input.size(), "output size", output.size()) return output model = Model(input_size, output_size)# 4) 封装之前要把模型移到对应的gpumodel.to(device) if torch.cuda.device_count() &gt; 1: print("Let's use", torch.cuda.device_count(), "GPUs!") # 5) 封装 model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[local_rank], output_device=local_rank) for data in rand_loader: if torch.cuda.is_available(): input_var = data else: input_var = data output = model(input_var) print("Outside: input size", input_var.size(), "output_size", output.size()) 需要通过命令行启动 1CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --nproc_per_node=2 torch_ddp.py 结果：12345678910Let&apos;s use 2 GPUs!Let&apos;s use 2 GPUs! In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])Outside: input size torch.Size([15, 5]) output_size torch.Size([15, 2]) In Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2]) In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])Outside: input size torch.Size([15, 5]) output_size torch.Size([15, 2]) 可以看到有两个进程，log打印了两遍 torch.distributed.launch 会给模型分配一个args.local_rank的参数，也可以通过torch.distributed.get_rank()获取进程id]]></content>
      <categories>
        <category>训练方法</category>
        <category>分布式训练</category>
      </categories>
      <tags>
        <tag>PyTorch</tag>
        <tag>分布式训练</tag>
        <tag>多卡训练</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【分布式训练】单机多卡的正确打开方式（二）：TensorFlow]]></title>
    <url>%2F2019%2F07%2F14%2Fdistributed-training2%2F</url>
    <content type="text"><![CDATA[瓦砾上一篇讲了单机多卡分布式训练的一些入门介绍，后面几篇准备给大家讲讲TensorFlow、PyTorch框架下要怎么实现多卡训练。 这一篇就介绍一下TensorFlow上的分布式训练，尽管从传统的Custom Training Loops到Estimator再到Keras，TF的API换来换去让人猝不及防、又爱又恨，但是由于种种原因，TensorFlow还是业务上最成熟的框架，所以Let’s还是do it。（没看过上一篇的读者建议先看一下原理部分：分布式训练的正确打开方式（一）：理论基础，因为算法的理论理解对于后面API的理解还是很重要的。） 这篇博客主要介绍TensorFlow在1.13版本里发布的tf.distribute API，集成了之前tf.contrib.distribute的很多功能，并且大大的简化了使用。官方很良心的放了Google Colab，想要一步步执行看结果的读者可以移步官方教学。 Overviewtf.distribute的核心API是tf.distribute.Strategy，可以简简单单几行代码就实现单机多卡，多机多卡等情况的分布式训练。主要有这几个优势： 简单易用，开箱即用，高性能。 便于各种分布式Strategy切换。 支持Custom Training Loop、Estimator、Keras。 支持eager excution。 123import osos.environ["CUDA_VISIBLE_DEVICES"]="0,1"import tensorflow as tf Strategy的类别tf.distribute.Strategy设计的初衷是能cover不同维度的use cases，目前主要有四个Strategy： MirroredStrategy CentralStorageStrategy MultiWorkerMirroredStrategy ParameterServerStrategy 还有一些策略，例如异步训练等等，后面会逐步支持。 1. MirroredStrategy镜像策略用于单机多卡 数据并行 同步更新的情况，在每个GPU上保存一份模型副本，模型中的每个变量都镜像在所有副本中。这些变量一起形成一个名为MirroredVariable的概念变量。通过apply相同的更新，这些变量保持彼此同步。 镜像策略用了高效的All-reduce算法来实现设备之间变量的传递更新。默认情况下，它使用NVIDIA NCCL作为all-reduce实现。用户还可以在官方提供的其他几个选项之间进行选择。 最简单的创建一个镜像策略的方法： 1mirrored_strategy = tf.distribute.MirroredStrategy() 也可以自己定义要用哪些devices： 1mirrored_strategy = tf.distribute.MirroredStrategy(devices=["/gpu:0", "/gpu:1"]) 官方也提供了其他的一些all-reduce实现： tf.distribute.CrossDeviceOps tf.distribute.HierarchicalCopyAllReduce tf.distribute.ReductionToOneDevice tf.distribute.NcclAllReduce (default) 12mirrored_strategy = tf.distribute.MirroredStrategy( cross_device_ops=tf.distribute.HierarchicalCopyAllReduce()) 2. CentralStorageStrategy中央存储策略，参数被统一存在CPU里，然后复制到所有GPU上，优点是GPU负载均衡了，但是一般情况下CPU和GPU通信代价大，不建议使用。 1central_storage_strategy = tf.distribute.experimental.CentralStorageStrategy() 3. MultiWorkerMirroredStrategy这个API和MirroredStrategy很类似，是其多机多卡分布式的版本，由于我们主要是介绍单机多卡，这里就不展开讲了。 1multiworker_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() 4. ParameterServerStrategy这个API呢，就是被大家普遍嫌弃即将淘汰的PS策略，慢+负载不均衡。（和all-reduce的区别，请看上一篇） 1ps_strategy = tf.distribute.experimental.ParameterServerStrategy() tf.distribute.Strategy在三种API上的使用：Keras、Estimator、Custom Training Loops1. Keras123456789101112# 这里的Strategy可以换成想用的，因为其他三个还是experimental的状态，建议用这个mirrored_strategy = tf.distribute.MirroredStrategy()with mirrored_strategy.scope(): # 定义模型的时候放到镜像策略空间就行 model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))]) model.compile(loss='mse', optimizer='sgd')# 手动做个假数据跑一下dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(1000).batch(10)print('Train:')model.fit(dataset, epochs=2)print('\nEval:')model.evaluate(dataset) 2. Estimator1234567891011121314mirrored_strategy = tf.distribute.MirroredStrategy()# 在config中加入镜像策略config = tf.estimator.RunConfig(train_distribute=mirrored_strategy, eval_distribute=mirrored_strategy)# 把config加到模型里regressor = tf.estimator.LinearRegressor( feature_columns=[tf.feature_column.numeric_column('feats')], optimizer='SGD', config=config)def input_fn(): dataset = tf.data.Dataset.from_tensors((&#123;"feats":[1.]&#125;, [1.])) return dataset.repeat(1000).batch(10)# 正常训练，正常评估regressor.train(input_fn=input_fn, steps=10)regressor.evaluate(input_fn=input_fn, steps=10) 3. Custom Training Loops123456789101112131415161718192021222324252627282930313233343536mirrored_strategy = tf.distribute.MirroredStrategy()# 在mirrored_strategy空间下with mirrored_strategy.scope(): model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))]) optimizer = tf.train.GradientDescentOptimizer(0.1)# 在mirrored_strategy空间下with mirrored_strategy.scope(): dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat(1000).batch(global_batch_size) print(dataset) # 这里要分发一下数据 dist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset) print(dist_dataset.__dict__['_cloned_datasets'])def train_step(dist_inputs): def step_fn(inputs): features, labels = inputs logits = model(features) cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2( logits=logits, labels=labels) loss = tf.reduce_sum(cross_entropy) * (1.0 / global_batch_size) train_op = optimizer.minimize(loss) with tf.control_dependencies([train_op]): return tf.identity(loss) # 返回所有gpu的loss per_replica_losses = mirrored_strategy.experimental_run_v2(step_fn, args=(dist_inputs,)) # reduce loss并返回 mean_loss = mirrored_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses, axis=None) return mean_losswith mirrored_strategy.scope(): input_iterator = dist_dataset.make_initializable_iterator() iterator_init = input_iterator.initialize() var_init = tf.global_variables_initializer() loss = train_step(input_iterator.get_next()) with tf.Session() as sess: sess.run([var_init, iterator_init]) for _ in range(2): print(sess.run(loss))]]></content>
      <categories>
        <category>训练方法</category>
        <category>分布式训练</category>
      </categories>
      <tags>
        <tag>分布式训练</tag>
        <tag>多卡训练</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【分布式训练】单机多卡的正确打开方式（一）：理论基础]]></title>
    <url>%2F2019%2F07%2F08%2Fdistributed-training%2F</url>
    <content type="text"><![CDATA[瓦砾由于最近bert-large用的比较多，踩了很多分布式训练的坑，加上在TensorFlow和PyTorch之间更换，算是熟悉了一下各类框架的分布式训练接口，由于集中在一起讲可能比较乱，笔者准备分三到四篇来讲一下深度学习的分布式训练。这一篇先讲一下“分布式训练的类型与算法”。 分布式训练的需求和重要性不需要多说，随着GPT、BERT、xlnet这些预训练模型的出现，普通的16G的显存已经不足以支撑深度学习模型训练的要求了，这时候就需要用到分布式训练来提高效率。 注意：这个系列主要介绍单机多卡的分布式训练情况（这种情况比较常见，土豪和大佬们请忽略）。 总的来说，分布式训练分为这几类： 按照并行方式来分：模型并行 vs 数据并行 按照更新方式来分：同步更新 vs 异步更新 按照算法来分：Parameter Server算法 vs AllReduce算法 模型并行 vs 数据并行假设我们有n张GPU： 模型并行：不同的GPU输入相同的数据，运行模型的不同部分，比如多层网络的不同层； 数据并行：不同的GPU输入不同的数据，运行相同的完整的模型。 当模型非常大，一张GPU已经存不下的时候，可以使用模型并行，把模型的不同部分交给不同的机器负责，但是这样会带来很大的通信开销，而且模型并行各个部分存在一定的依赖，规模伸缩性差。因此，通常一张可以放下一个模型的时候，会采用数据并行的方式，各部分独立，伸缩性好。 同步更新 vs 异步更新对于数据并行来说，由于每个GPU负责一部分数据，那就涉及到如果更新参数的问题，分为同步更新和异步更新两种方式。 同步更新：每个batch所有GPU计算完成后，再统一计算新权值，然后所有GPU同步新值后，再进行下一轮计算。 异步更新：每个GPU计算完梯度后，无需等待其他更新，立即更新整体权值并同步。 同步更新有等待，速度取决于最慢的那个GPU；异步更新没有等待，但是涉及到更复杂的梯度过时，loss下降抖动大的问题。所以实践中，一般使用同步更新的方式。 Parameter Server算法 vs Ring AllReduce算法这里讲一下常用的两种参数同步的算法：PS 和 Ring AllReduce。 假设有5张GPU： Parameter Server：GPU 0将数据分成五份分到各个卡上，每张卡负责自己的那一份mini-batch的训练，得到grad后，返回给GPU 0上做累积，得到更新的权重参数后，再分发给各个卡。 Ring AllReduce：5张以环形相连，每张卡都有左手卡和右手卡，一个负责接收，一个负责发送，循环4次完成梯度累积，再循环4次做参数同步。分为Scatter Reduce和All Gather两个环节。 Parameter Server的思想其实有点类似于MapReduce，以上讲同步异步的时候，都是用的这种算法，但是它存在两个缺点： 每一轮的训练迭代都需要所有卡都将数据同步完做一次Reduce才算结束，并行的卡很多的时候，木桶效应就会很严重，计算效率低。 所有的GPU卡需要和Reducer进行数据、梯度和参数的通信，当模型较大或者数据较大的时候，通信开销很大。 假设有N个GPU，通信一次完整的参数所需时间为K，那么使用PS架构，花费的通信成本为： T = 2(N-1)K所以我们亟需一种新的算法来提高深度学习模型训练的并行效率。 2017 年 Facebook 发布了《Accurate, large minibatch SGD: Training ImageNet in 1 hour 》验证了大数据并行的高效性，同年百度发表了《Bringing HPC techniques to deep learning 》，验证了全新的梯度同步和权值更新算法的可行性，并提出了一种利用带宽优化环解决通信问题的方法——Ring AllReduce。 Parameter Service最大的问题就是通信成本和GPU的数量线性相关。而Ring AllReduce的通信成本与GPU数量无关。Ring AllReduce分为两个步骤：Scatter Reduce和All Gather。 Scatter Reduce过程：首先，我们将参数分为N份，相邻的GPU传递不同的参数，在传递N-1次之后，可以得到每一份参数的累积（在不同的GPU上）。 All Gather：得到每一份参数的累积之后，再做一次传递，同步到所有的GPU上。 根据这两个过程，我们可以计算到All Reduce的通信成本为： T = 2(N-1)\frac{K}{N}可以看到通信成本T与GPU数量无关。 由于All Reduce算法在通信成本上的优势，现在几个框架基本上都实现了其对于的官方API，后面几篇，瓦砾会跟大家一起过一遍TF，Torch的分布式训练API具体是怎么用的，有哪些坑。 Reference 是时候放弃Tensorflow，拥抱Horovod了 Tensorflow单机多卡实现 Binging HPC Techniques to Deep Learning Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU &amp; Distributed setups]]></content>
      <categories>
        <category>训练方法</category>
        <category>分布式训练</category>
      </categories>
      <tags>
        <tag>分布式训练</tag>
        <tag>多卡训练</tag>
        <tag>horovod</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【训练技巧】指数移动平均（EMA）的原理及PyTorch实现]]></title>
    <url>%2F2019%2F06%2F01%2Fema%2F</url>
    <content type="text"><![CDATA[在深度学习中，经常会使用EMA（指数移动平均）这个方法对模型的参数做平均，以求提高测试指标并增加模型鲁棒。 今天瓦砾准备介绍一下EMA以及它的Pytorch实现代码。 EMA的定义指数移动平均（Exponential Moving Average）也叫权重移动平均（Weighted Moving Average），是一种给予近期数据更高权重的平均方法。 假设我们有n个数据：$[\theta_1, \theta_2, …, \theta_n]$ 普通的平均数：$\overline{v}=\frac{1}{n}\sum_{i=1}^n \theta_i$ EMA：$vt = \alpha\cdot v{t-1} + (1-\alpha)\cdot \theta_t$，其中，$ v_t$表示前$t$条的平均值 ($v_0=0$)，$\alpha$ 是加权权重值 (一般设为0.9-0.999)。 Andrew Ng在Course 2 Improving Deep Neural Networks中讲到，EMA可以近似看成过去$1/(1-\alpha)$个时刻$v$值的平均。 普通的过去$n$时刻的平均是这样的： v_t =\frac{(n-1)\cdot v_{t-1}+\theta_t}{n}类比EMA，可以发现当$\alpha=\frac{n-1}{n}$时，两式形式上相等。需要注意的是，两个平均并不是严格相等的，这里只是为了帮助理解。 实际上，EMA计算时，过去$1/(1-\alpha)$个时刻之前的平均会decay到 $\frac{1}{e}$ ，证明如下。 如果将这里的$v_t$展开，可以得到： v_t = \alpha^n v_{t-n} + (1-\alpha)(\alpha^{n-1}\theta_{t-n+1}+ ... +\alpha^0\theta_t)其中，$n=\frac{1}{1-\alpha}$，代入可以得到$\alpha^n=\alpha^{\frac{1}{1-\alpha}}\approx \frac{1}{e}$。 EMA的偏差修正实际使用中，如果令$v_0=0$，步数较少的情况下，ema的计算结果会有一定偏差。 理想的平均是绿色的，因为初始值为0，所以得到的是紫色的。 因此可以加一个偏差修正（bias correction）。 v_t = \frac{v_t}{1-\alpha^t}显然，当t很大时，修正近似于1。 在深度学习的优化中的EMA上面讲的是广义的ema定义和计算方法，特别的，在深度学习的优化过程中，$\theta_t$ 是t时刻的模型权重weights，$v_t$是t时刻的影子权重（shadow weights）。在梯度下降的过程中，会一直维护着这个影子权重，但是这个影子权重并不会参与训练。基本的假设是，模型权重在最后的n步内，会在实际的最优点处抖动，所以我们取最后n步的平均，能使得模型更加的鲁棒。 EMA为什么有效网上大多数介绍EMA的博客，在介绍其为何有效的时候，只做了一些直觉上的解释，缺少严谨的推理，瓦砾在这补充一下，不喜欢看公式的读者可以跳过。 令第n时刻的模型权重（weights）为$v_n$，梯度为$g_n$，可得： \begin{align} \theta_n &= \theta_{n-1}-g_{n-1} \\\\ &=\theta_{n-2}-g_{n-1}-g_{n-2} \\\\ &= ... \\\\ &= \theta_1-\sum_{i=1}^{n-1}g_i \end{align}令第n时刻EMA的影子权重为$v_n$，可得： \begin{align} v_n &= \alpha v_{n-1}+(1-\alpha)\theta_n \\\\ &= \alpha (\alpha v_{n-2}+(1-\alpha)\theta_{n-1})+(1-\alpha)\theta_n \\\\ &= ... \\\\ &= \alpha^n v_0+(1-\alpha)(\theta_n+\alpha\theta_{n-1}+\alpha^2\theta_{n-2}+...+\alpha^{n-1}\theta_{1}) \end{align}代入上面$\theta_n$的表达，令$v_0=\theta_1$展开上面的公式，可得： \begin{align} v_n &= \alpha^n v_0+(1-\alpha)(\theta_n+\alpha\theta_{n-1}+\alpha^2\theta_{n-2}+...+\alpha^{n-1}\theta_{1})\\\\ &= \alpha^n v_0+(1-\alpha)(\theta_1-\sum_{i=1}^{n-1}g_i+\alpha(\theta_1-\sum_{i=1}^{n-2}g_i)+...+ \alpha^{n-2}(\theta_1-\sum_{i=1}^{1}g_i)+\alpha^{n-1}\theta_{1})\\\\ &= \alpha^n v_0+(1-\alpha)(\frac{1-\alpha^n}{1-\alpha}\theta_1-\sum_{i=1}^{n-1}\frac{1-\alpha^{n-i}}{1-\alpha}g_i) \\\\ &= \alpha^n v_0+(1-\alpha^n)\theta_1 -\sum_{i=1}^{n-1}(1-\alpha^{n-i})g_i\\\\ &= \theta_1 -\sum_{i=1}^{n-1}(1-\alpha^{n-i})g_i \end{align}对比两式： \begin{align} \theta_n &= \theta_1-\sum_{i=1}^{n-1}g_i \\\\ v_n &= \theta_1 -\sum_{i=1}^{n-1}(1-\alpha^{n-i})g_i \end{align}EMA对第i步的梯度下降的步长增加了权重系数$1-\alpha^{n-i}$，相当于做了一个learning rate decay。 PyTorch实现瓦砾看了网上的一些实现，使用起来都不是特别方便，所以自己写了一个。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class EMA(): def __init__(self, model, decay): self.model = model self.decay = decay self.shadow = &#123;&#125; self.backup = &#123;&#125; def register(self): for name, param in self.model.named_parameters(): if param.requires_grad: self.shadow[name] = param.data.clone() def update(self): for name, param in self.model.named_parameters(): if param.requires_grad: assert name in self.shadow new_average = (1.0 - self.decay) * param.data + self.decay * self.shadow[name] self.shadow[name] = new_average.clone() def apply_shadow(self): for name, param in self.model.named_parameters(): if param.requires_grad: assert name in self.shadow self.backup[name] = param.data param.data = self.shadow[name] def restore(self): for name, param in self.model.named_parameters(): if param.requires_grad: assert name in self.backup param.data = self.backup[name] self.backup = &#123;&#125;# 初始化ema = EMA(model, 0.999)ema.register()# 训练过程中，更新完参数后，同步update shadow weightsdef train(): optimizer.step() ema.update()# eval前，apply shadow weights；eval之后，恢复原来模型的参数def evaluate(): ema.apply_shadow() # evaluate ema.restore() References 机器学习模型性能提升技巧：指数加权平均（EMA） Exponential Weighted Average for Deep Neutal Networks]]></content>
      <categories>
        <category>训练技巧</category>
        <category>指数移动平均</category>
      </categories>
      <tags>
        <tag>torch</tag>
        <tag>ema</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【论文笔记】MT-DNN]]></title>
    <url>%2F2019%2F05%2F23%2Fmt-dnn%2F</url>
    <content type="text"><![CDATA[想写这篇蛮久的，但由于之前一直忙着搞别的事情（好吧就是懒），一直拖着。刚好最近有用这个方面的需求，就又读了一遍论文和github上的一些实现。 大部分的博客都只是粗略翻译论文，然而光看论文，往往会忽略一些实现细节，所以笔者最近在尝试将论文笔记和源码解析结合起来，就从这篇MT-DNN开始吧。希望大家多多提意见。 Paper：Multi-Task Deep Neural Networks for Natural Language Understanding其实作者（Xiaodong Liu）早在15年就写过一篇Multi-task相关的论文，只不过当时还没有bert这样优秀的预训练表达层，在bert横扫各大榜单之后，作者将之前多任务的概念和bert相结合，duang~就出了这一篇在GLUE、SNLI和SciTail创下新的SOTA的论文。 Intuition会滑雪的人，学滑冰要容易的多（笔者试过，反过来不大成立，手动狗头）。我觉得类比成九年义务教育更好，十门功课同步学，数学是你学物理的基础，历史知识能提高你作文的水平，etc。 Motivation 监督学习需要大量监督数据，但正常情况下咱都是没有的。MTL（multi-task learning）可以提高low-resource任务的表现。 MTL能起到正则的作用，减轻模型对特定任务的过拟合。 bert之类的预训练模型充分利用了无监督数据。MTL作为补充，进一步利用了out-domain的监督数据。 Model模型很简单，看一下这个图： 底层share了bert的表达层，输出层为每个任务设计了各自的输入形式和loss计算方式。 任务和loss计算任务分类及数据GLUE 单句分类（Single-Sentence Classification）： CoLA（Corpus of Linguistic Acceptability）：判断英语句子是否语法正确 SST-2（Stanford Sentiment Treebank）：影评情感分类 文本相似度（Text Similarity）： STS-B（Semantic Textual Similarity Bench-mark）：人类标注的1-5的语义相似度数据集。 对句分类（Pairwise Text Classification）： RTE（Recognizing Textual Entailment）：entailment or not_entailment MNLI（Multi-Genre Natural Language Inference）：entailment，contradiction，neural QQP（Quora Question Pairs）：判断两个问题是否问的是同一内容。 MRPC（Microsoft Research Paraphrase Corpus）：判断是否两个句子是语义相同的。 WNLI（Winograd NLI）：Wino-grad Schema dataset得到的推理任务。 相关性排序（Relevance Ranking）： QNLI（Stanford Question Answering）：问答对数据集。 Out-domain SNLI（Stanford Natural Language Inference）：Flickr30里人工标注了hypotheses的推理数据集 SciTail（Science Question Answering Textual Entailment）：科学问题的推理，更难。 loss计算1. 单句分类交叉熵： P_r(c|X) = \text{softmax}(W^T_{SST}\cdot x)\\\\ L(\Theta) = -\sum_c{\mathbb{I}(X, c) \log (P_r(c|X)) }2. 文本相似度均方误差： \text{Sim}(X_1, X_2) = \text{sigmoid}(w^T_{STS}\cdot x)\\\\ L(\Theta) = (y-\text{Sim}(X_1, X_2))^23. 对句分类前面比较常见，这个对句分类作者处理的方式比较特殊，用了18年作者自己提出的一种叫 SAN（stochastic answer network）的输出层构建方式，推理过程有点繁琐，给大家贴个图。 注意图中的$m$，$n$ 都是sequence length。 总结起来就是，作者得到query和premise分别的token-wise的表达之后，在他们两个之间做了一个attention，然后开辟了一个新的状态维度做RNN，从而得到多次预测结果，再做平均（类似于人推理时，多次思考才能得到最终的判断）。作者在后面证明了 SAN 结构能带来0.1%~0.5%的提升。 loss也是交叉熵： L(\Theta) = -\sum_c{\mathbb{I}(X, c) \log (P_r(c|X)) }4. 相关性排序作者的这个loss设计还是挺有意思的，不用简单的二分类来做这个任务，而是用learning2rank的范式，对于每个query $Q$ 采样 $N$ 个candidates，其中$A^+$是正确答案，其他的都是是错误答案。 负对数似然： \text{Rel}(Q,A)=\text{sigmoid}(w^T_{QNLI} \cdot x)\\\\ L(\Theta)=-\frac{\text{exp}(\text{Rel}(Q,A^+))}{\sum_{A'\in{\mathcal{A}}}\text{exp}(Rel(Q,A'))}训练过程 训练过程就是把所有数据合并在一起，每个batch只有单一任务的数据，同时会带有一个task-type的标志，这样模型就知道该走哪一条loss计算的路径。 论文里并没有提及对于单个任务，之后还要不要再单独Fine-tune一下，不过参考github的FAQ，再FT一下，结果会更好。 实验实现细节Optimizer：Adamax（这个地方跟bert不太一样）lr：5e-5batch size：32max_num of epochs：5SAN steps：5warm-up：0.1clip gradient norm：1max seq length：512 GLUE结果 从Table 2 可以看出来，MT-DNN在每一项都超过了bert，而且数据越少的任务，提升越明显，对于QQP和MNLI来说，提升就没那么明显了。 Table 3中的ST-DNN名字很玄乎，其实与bert不同的就是用了文中的复杂了一点的输出模块和loss的设计，比如SAN，learning2rank这些，单独训练各个任务。可见都是有一定程度的提升。所以MT-DNN相对于bert的提升其实来自于 multi task 和 special output module 两个部分。 SNLI 和 SciTail 结果 在得到mult-task训练后的ckpt后，用这个weights去fine tune新的任务，结果和GLUE的保持一致，都有提升，且小数据集任务的提升更明显。 Domain 适应性结果这个结果比较有趣，笔者认为也是比较重要的点，MT-DNN得到的weights相对于bert的weights能在很少的数据下达到不错的效果，且数据越少，相对bert的提升就越大。(甚至23个训练样本就能达到82.1%的准确率，amazing啊。) Conclusion打个总结： MT-DNN的优点： 数据要求少 泛化能力强，不容易过拟合 MT-DNN的缺点： 实用性：实际应用中也许并不能找到特别合适的，且高质量的多任务 训练慢啊，MT-DNN作者用了4张v100，普通业务要不起这个条件，所以MT-DNN的定位其实类似于bert，训练好了就别乱动了，当pretrain-model用。 作者认为的Further work 更深度的share weights 更有效的训练方法 用更可控的方式融入文本的语言结构（这点个人感觉不适用于现在大刀阔斧搞预训练模型的情况） Code源码地址：https://github.com/namisan/mt-dnn 阅读源码前，我习惯思考一下如果我自己写，会怎么写： 首先咱肯定得分类一下数据，每一类任务对应一个数据流，不能每个任务写一个数据流，太累了。 新建模型的时候得知道有哪些任务，每个任务num_labels是多少，自动生成输出层集合和与id的映射，训练和推理的时候根据任务id选择输出层。 怎么保证一个epoch结束，所有任务数据集都用完了呢？ max seq length、learning rate、batch size这些超参需要根据任务变化吗？不同任务的loss如何scale呢？ 基本上，想到这，心里都有点数了，带着问题看源码实现。 官方的源码是用PyTorch实现的，包括了MT-DNN的训练，和一些下游任务的finetune，同时也提供了训练好的MT-DNN的模型。核心思想和步骤如下： prepro.py：预处理数据，将GLUE的多个任务分成四类，统一处理成 {&#39;uid&#39;: ids, &#39;label&#39;: label, &#39;token_id&#39;: input_ids, &#39;type_id&#39;: type_ids}的形式，方便后面操作。 mt-dnn/batcher.py：自定义的data iterator，将读到的数据处理成Tensor。 mt-dnn/matcher.py：模型，之所以叫matcher，是因为模型有一个ModuleList，存放了不同任务对应的输出层，根据当前batch的任务类型match对应的输出层。 mt-dnn/model.py：这里命名有点混淆，实际的模型是上面的matcher，这里做了一些模型前后的处理工作（ema，predict，save模型之类的）。 重点讲一下mt-dnn/batcher.py和mt-dnn/matcher.py这两个部分。 batcher.py 忽略作者对于batch这个单词疯狂的拼写错误，相比于常规单任务的data_iterator，这个类除了iter数据，还要返回关于这个任务的必要信息，比如这个任务的id，任务的类型。make_baches 实现把数据打包成batch，reset用来在每个epoch之后重新shuffle并打包成batch。 matcher.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class SANBertNetwork(nn.Module): def __init__(self, opt, bert_config=None): super(SANBertNetwork, self).__init__() self.dropout_list = [] self.bert_config = BertConfig.from_dict(opt) self.bert = BertModel(self.bert_config) if opt['update_bert_opt'] &gt; 0: for p in self.bert.parameters(): p.requires_grad = False mem_size = self.bert_config.hidden_size self.decoder_opt = opt['answer_opt'] # 构建ModuleList，index为task_id self.scoring_list = nn.ModuleList() labels = [int(ls) for ls in opt['label_size'].split(',')] task_dropout_p = opt['tasks_dropout_p'] self.bert_pooler = None for task, lab in enumerate(labels): decoder_opt = self.decoder_opt[task] # 不同任务dropout也不一样 dropout = DropoutWrapper(task_dropout_p[task], opt['vb_dropout']) self.dropout_list.append(dropout) if decoder_opt == 1: out_proj = SANClassifier(mem_size, mem_size, lab, opt, prefix='answer', dropout=dropout) self.scoring_list.append(out_proj) else: out_proj = nn.Linear(self.bert_config.hidden_size, lab) self.scoring_list.append(out_proj) self.opt = opt self._my_init() self.set_embed(opt) def forward(self, input_ids, token_type_ids, attention_mask, premise_mask=None, hyp_mask=None, task_id=0): all_encoder_layers, pooled_output = self.bert(input_ids, token_type_ids, attention_mask) sequence_output = all_encoder_layers[-1] if self.bert_pooler is not None: pooled_output = self.bert_pooler(sequence_output) decoder_opt = self.decoder_opt[task_id] if decoder_opt == 1: max_query = hyp_mask.size(1) assert max_query &gt; 0 assert premise_mask is not None assert hyp_mask is not None hyp_mem = sequence_output[:,:max_query,:] # 通过任务id，索引到对应的输出层，搞定。 logits = self.scoring_list[task_id](sequence_output, hyp_mem, premise_mask, hyp_mask) else: pooled_output = self.dropout_list[task_id](pooled_output) logits = self.scoring_list[task_id](pooled_output) return logits 这里笔者删除了其他函数，只保留了__init__和forward，正如我们看源码之前猜想的，作者就是通过构建一个ModuleList，根据各个任务的类型、label数等信息append输出层，index即任务id。]]></content>
      <categories>
        <category>论文笔记</category>
        <category>多任务</category>
      </categories>
      <tags>
        <tag>多任务</tag>
        <tag>bert</tag>
        <tag>迁移学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【论文笔记】Attention is all you need]]></title>
    <url>%2F2019%2F01%2F27%2Ftransfomer%2F</url>
    <content type="text"><![CDATA[今年的NLP界被BERT整的明明白白，其中的基本结构 Transformer 一定要了解一下。 Abstract：一般来说，重要的sequence transduction模型都是基于包含encoder，decoder的复杂的rnn和cnn的。最好的模型是通过一个attention机制来连接encoder，decoder。比较普通！我们提出一个只靠attention的。叫Transformer，跟cnn，rnn完全没关系，很炫酷。 在两个翻译任务实验证明：我们的模型又快又好！！！ 高了2个BLEU； 用8个gpu训练3.5天，结果直接超过了当前的SOTA。 我们还证明，Transformer泛化性能贼好，在parsing上大小数据都比别的好 。 划重点：只靠Attention。 1. IntroductionRNN，LSTM，GRU在翻译和 LM（language model）领域搞了很多SOTA，很多研究花了很多心思在push Encoder-Decoder和Recurrent Language Model的边界。 可是，RNN的天性决定了训练的时候并行性差。尤其对长的sequence，内存限制影响batch examples了。很烦！有一些通过分解tricks和条件计算来提高efficiency的related work，后者也提高performance。但是问题依然存在。 Attention机制现在几乎干啥都必须了，可以不顾输入输出的distance地去model依赖。但是，除了极少cases，attention基本都和rnn绑定在了一起，很不机智！ So，咱提出一种不靠rnn，只靠attention的！！！并行性刚刚的，8个p100花了12 hours就达到翻译的new sota。 2. Background一些相关的研究都用了cnn来减少sequence计算。 这些模型里operations的数量随着输入输出距离的线性（convS2S）或是指数级（ByteNet）地增长。使得很难学习到较远的dependency。 在Transformer里，这是一个常数级的操作，虽然是以牺牲一定精度为代价，但是我们用Multi-Head Attention来抵消了，效果很好！ Self-attention，通过relate一个single sequence的不同位置来计算seq的表征。成功用在阅读理解，summarization等等。 基于attention的end2end的memory网络在很多简单QA和LM问题上比seq-aligned rnn（就是s2s吧）要好。但是啊，Transformer是第一个只靠self-attention来计算表征的。下面介绍一波Transformer！ 3. Model Architecture 其实，老瓦看到这个模型的一瞬间，心情是复杂，看着好简单，可是咋和以前那种有个序列，清清楚楚写着$(x_1, x_2, …, x_n)$什么的模型不太一样？不慌，老瓦来盘一盘。 3.1 Encoder and Decoder StacksEncoder：Encoder是由N=6相同的层组成的栈，每层都有两个子层。第一子层是一个multi-head self-attention mechanism（多头自注意力机制），第二子层是一个position-wise的全连接前馈层。每个子层后面都加了一个residual connection（冗余连接）+layer normalization（层正则），也就是说每个子层的输出都是 $\text{LayerNorm}(x+\text{Sublayer}(x))$ 。为了方便做加运算，所有子层的输出维度都是$d_{model}=512$。 Decoder：Decoder也是N=6层的。主要两个区别： 增加了一个子层，将encoder的输出当做输入。 修改了decoder栈的自注意力自层，来防止位置们去关注后续的位置。masking结合output的embedding都右移了一位这个事实，保证了位置i的预测只依赖于比i小的已知位置。 3.2 Attention既然文章名字叫Attention is all you need，attention的结构当然是其中的重中之重，理解了Attention就几乎理解了文章的一大半。 3.2.1 Basic Attention首先，得知道啥是attention。14年Sutskever大神祭出seq2seq之后，紧跟着Bahdanau和Luong就发了两篇attention用在seq2seq的论文，名字也好记，一个叫Bahdanau attention，一个叫Luong attention。这里以Bahdanau Attention为例，讲一讲计算attention的基本套路。 其实，attention机制和普通seq2seq的不同就是，要计算出一个包含上下文信息的context vector作为decoder每个位置的输入。计算attention时，一般有Query(Q)，Key(K)，Value(V) 三个输入。在上面这张图上，Q就是 $(s_0, s_1, … s_m)$ ，K和V就是$(h_0, h_1, …, h_n)$。attention机制一般的套路就是，用Q和K先算出一个权重的向量，再用这个权重的向量去element-wise地乘上V，就能得到Context Vector： ConVec(Q, K, V) = softmax(score(Q, K))V3.2.2 Scaled Dot-Product Attention明白了attention的套路，我们来看看论文里的attention是什么来头，先看看原文中这张无比清晰（但是看起来有点唬人）的图。 这又是Scaled Dot-Product又是Multi-Head的，一开始着实让老瓦感到有点慌，后来仔细一看，其实挺简单。 看过Luong那篇attention的文章的人都知道，score一般有三种算法： score(h_t, \overline{h}_s) = \begin{cases} h_t^{\top}\overline{h}_s &dot \\\\ h_t^{\top}W_a\overline{h}_s &general\\\\ v_a^{\top}tanh(W_a[h_t;\overline{h}_s]) &concat \end{cases}其实文章里用的就是第一种，因为性价比高（效果还不错速度快），但是有个缺点当Q，K的维度比较大的时候，容易进到softmax的饱和区，作者就scale了一下（除以$\sqrt{d_k}$），解决了这个问题，这个就是所谓的Scaled Dot-Product Attention。粗暴有效。 Attention(Q, K, V) = softmax(\frac{QK^{\top}}{\sqrt{d_k}})V基本上就完事了。 老瓦在看论文的时候一直不明白mask（只有在Decoder的input用到了）到底是具体怎么操作的，其实作者在后面有解释，就是把所有的非法连接的score都设置成负无穷，这样的softmax之后得到的权重向量就是零了($e^{-\infty}$)。完事。 3.2.3 Multi-Head Attention这下就讲到精髓了：多头注意力（不知道这样翻译会不会被打。。。）。可以这么理解，有好多人对attention权重的看法不太一样，所以我们就把这个任务给很多人一起做，最后取大家的共同意见，有点像CNN里好多个kernel的味道。 文章表示，比起直接用$d_{model}$的Q, K, V来说，将Q, K, V用不同的h个线性投影得到的h个$d_v$的context vector，再concat起来，过一个线性层的结果更好，可以综合不同位置的不同表征子空间的信息。 \text{Multihead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, ..., \text{head}_h)W^O\\\\ \text{where head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)其中，$Wi^Q\in\mathbb{R}^{d{model}\times dk}$，$W_i^K\in\mathbb{R}^{d{model}\times dk}$，$W_i^V\in\mathbb{R}^{d{model}\times dv}$，然后$W^O\in\mathbb{R}^{hd_v\times d{model}}$。 在文章里，设置了h=8个平行注意层（也就是头（head）2333）。对于每个层的$dk=d_v=d{\text{model}}/h=64$。因为每个头都减少了dimension，所以整体的computational cost和single-head full dimension的注意力机制是差不多的。 3.2.4 Applications of Attention in our model encoder-decoder attention: 模仿seq2seq模型的注意力机制 encoder 的 self-attention layer decoder 的self-attention layer：和上面的区别就是加了masking 3.3 Position-wise Feed-Forward Networks每个FFN包括两次线性变换，中间是ReLu的激活函数。 \text{FFN} = \max(0, xW_1+b_1)W_2+b_2不同position的FFN是一样的，但是不同层是不同的。输入输出维度都是$d{model}=512$，中间层的维度是$d{ff}=2048$。 3.4 Embeddings and Softmax和其他seq transduction模型一样，也得用learned embeddings，learned 线性变换，softmax这些东西。两个embedding的权重是share的。embedding层，会把权重乘$\sqrt{d_{model}}$。 3.5 Positional Encoding因为模型没有rnn或者cnn，为了用到sequence的顺序，作者引入了positional encoding（$dim=d_{model}$，便于相加）来inject一些相对位置的信息。 PE(pos, 2i) = sin(pos/10000^{2i/d_{model}})\\\\ PE(pos, 2i+1) = cos(pos/10000^{2i/d_{model}})作者测试用学习的方法来得到PE，最终发现效果差不多，所以最后用的是fixed的，而且sinusoidal的可以处理更长的sequence的情况。 用sinusoidal函数的另一个好处是可以用前面位置的值线性表示后面的位置。 \sin(\alpha+\beta) = \sin\alpha\cos\beta+\cos\alpha\sin\beta\\\\ \cos(\alpha+\beta) = \cos\alpha\cos\beta-\sin\alpha\sin\beta4. Why Self-Attention 之所以选择self-attention，主要因为三点： 每层的computational complexity； 可以被parallelize的计算量； 网络中long-range dependencies直接的path length（越短越能方便学到 long-range dependencies）。 5. Conclusion优点： 抛弃了RNN和CNN，提出了Transformer，算法的并行性非常好； Transformer的设计最大的带来性能提升的关键是将任意两个单词的距离是1，有效地解决了long dependency的问题。 缺点： Transformer不像CNN那样可以抽取局部特征，RNN + CNN + Transformer的结合可能会带来更好的效果； 位置信息其实在NLP中非常重要，Transformer中用的Position Embedding也不是一个最终的解决方案。]]></content>
      <categories>
        <category>论文笔记</category>
        <category>Attention</category>
      </categories>
      <tags>
        <tag>Attention</tag>
        <tag>Transformer</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【论文笔记】A User Simulator for Task-Completion Dialogues]]></title>
    <url>%2F2019%2F01%2F16%2Fuser-simulator%2F</url>
    <content type="text"><![CDATA[基本框架（包含对话系统）： Abstract：做任务型bot的时候，强化学习（RL）很强，但是有一些困难： 需要和环境互动，已有的完整对话训练数据没用； 每个不同的任务都需要各自领域的标注数据； 收集人人对话或者人机对话需要领域知识。 但是啊： ====&gt;建造数据库又贵又花时间 ====&gt;只好模拟 ====&gt;user simulator诞生 ====&gt;Bot（agent）先用simulator去训练，搞定了simulator就可以上线，持续online learning Introduction:对话系统一般如 Figure 1. 所示。Dialongue Policy (DP)是一个任务型bot的核心。 一般来说传统的Dialogue Policy（DP）使用规则编程的，但有缺点： 对于复杂系统，难以设计 最优的policy也会变化，不好维护。因此，一般用强化学习训练DP。 为啥要user simulator ? Supervised Learning（SL）监督学习在任务型的bot里不行： 需要专家来标注大量数据； 大量专业的领域知识需要大量的数据来训练； 即使有大量训练数据，还是会有对话空间没有被搜索到。 相反的，RL很吊，不需要专家生成的数据，给一个奖励信号，agent就可以通过交互优化DP。 但是不幸的是：RL需要从环境来的很多sample，所以和真实用户从零开始训练不实际。 ====&gt; 所以需要User Simulators！ 一般的操作是：先在simulator上训练出一个比较好的效果，再部署到真实场景中，持续online learning。 Related Work很难判断user simulator好不好，没有Metric来判定，so没有标准方法，放手乱做。user simulation主要有这么几个类型： 从粒度上分： 在对话行为（dialogue-act）上进行操作的； 在对话文本（utterance）进行操作的 从方法的角度讲： 基于规则的（rule-based） 基于模型的（model-based） 以前的 bi-gram 模型 $P(a_u |a_m)$，基于上一个系统行为 $a_m$ 去预测下一个用户行为 $a_u$。这就很傻。(user有可能改goal，看的信息太少) 后面两个办法来处理： 看更长的对话历史 把user goal 放到user state modeling中 seq2seq端到端解决闲聊可以，任务型不太行。 本文用的叫 agenda-based user simulation 的架构，类似栈的结构通过进栈出栈来 model 状态转移和用户行为生成。很方便，显性encode了对话历史和用户目标（user goal）。 总结：文章 结合了rule-based和model-based 的方法: 在dialog-act level，用了agenda-based (rule)的方法； 在nlg部分，用了seq2seq (model)的方法。 Task-completion的对话系统（Dialogue system）任务型对话系统(以订票bot为例)，通过nl交互，去获取客户期望的信息，最终实现订票。以： 是否订票成功 是否电影满足要求 为标准，输出一个二进制结果，success or failure，评估系统。 数据：用Amazon Mechanical Turk（众包平台）收集的数据，内部标注，11个intent（i.e., inform，request，confirm-question，confirm-answer，etc），29个slot（i.e., movie-name，start-time，theater，numberofpeople，etc）。 一共标注了280个对话，对话平均11轮。 User SimulatorUser GoalTC Bot的user simulator第一步是生成user goal。agent不知道user goal，但是要帮助user来完成他的goal。user goal的定义分两个部分： inform_slots: 包含了constraints（C） request_slots: unk（R） 又可以分成： 必须有的slots（required slots） 选择有的slots（optional slots，i.e., ticket就是request slots 里的必须项）。 Goal是在labeled dataset里生成的，两个机制： 在第一个用户轮提取所有的slots。对于所有的slots，在所有的用户轮里提取第一次出现的。 每次跑对话的时候，就先sample一个user goal。 User Action第一轮的action sampling：要加一些限制（比如通常是request turn，至少一个inform slot，movie_name必须在，等等）。 如果不用NLG，NLU的话，还要加入噪声来模拟NLG和NLU过程产生的噪音，去训练DM部分。 Dialogue Status三个对化状态： no_outcome_yet success failure 具体情况具体讨论。 NLUIOB-format slots tags + Intent tags 最后的hidden layer判断 intent。 NLG基于Template的NLG：dialog-act 被found在模板中的，套模板句型。基于Model的NLG：没发现的，用model生成。（这一点感觉很扯，都没见过，咋生成，数据量肯定不够啊。） UsagesTask-completing Dialogue Setting：任务型Bot（订票），衡量 agent的Metrics为：1. success rate；2. average reward；3. average turns。KB-InfoBot（简单点，agent和user都只有request和inform）：问答Bot（电影信息） DiscussionRule-based的 user simulation 很safe，但是很耗时，因为要手动制定各种规则。两个优化方向： 包含user goal的改变（已做）实现Model-based 的simulator，优点是泛化性能好，缺点是1. 需要大量数据， 2. 万一有漏洞，RL agent 会抓住这个漏洞，假假的成功，你以为success了，其实都是RL agent抓住了loophole给你的假象。 总结：（根据《Agenda-Based User Simulation for bootstrapping a POMDP Dialogue System》by J. Schatzmann） 语义层的 User Simulation人机对话可以看成是状态转移（state transition）和对话行为（Dialog）的序列：用户根据状态 S (或加上机器人的 $a_m$，第一轮可能没有$a_m$)，采取行动 $a_u$, 把状态转移到$S’$。收到agent行动 $a_m$，再根据 $S’$ ，再把状态转移到 $S’’$。这个user行为可以被分解为三个模型：$P(a_u|S)$ 行为选择，$P(S’|a_u, S)$ 状态转移（用户行为），$P(S’’|a_m,S’)$ 状态转移（系统行为）。 基于Goal和Agent的状态表示用户状态（User State）由用户目标（Goal）及议程（Agenda）构成。 Goal由Constraint（C）（比如要市中心的啤酒酒吧）和Request（R）（比如酒吧电话是多少）构成。 Agenda是一个类似栈的结构，包含了在排队中的，用来引出目标中明确的信息的用户行为。在对话开始的时候，用数据库随机生成一个新的Goal。然后，从Goal里的内容来初始化Agenda，把Goal中的 C 都convert成 Inform动作，把Goal中的 R 都convert成 Request动作，再在最后加一个bye动作。有新的 $a_m$ 的时候，新的user acts 会被压进栈，不需要的会被出栈。具体实现方法（框架和数据结构），后面分析miulab的simulator源码的时候会写。 上图描述了Agenda随着$a_m$，$a_u$的状态转移过程（进栈和出栈）。 上图描述了无NLU和NLG的训练。]]></content>
      <categories>
        <category>论文笔记</category>
        <category>Bot</category>
      </categories>
      <tags>
        <tag>Chatbot</tag>
        <tag>User Simulator</tag>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前言]]></title>
    <url>%2F2019%2F01%2F11%2Ffirst-post%2F</url>
    <content type="text"><![CDATA[欢迎来到瓦特兰蒂斯。笔者是一个相信亚特兰蒂斯曾经存在的五岁抬头团—-aka瓦砾。 不定期更新NLP的干货，刚开始写博客，希望自己能坚持下去，请大家多多指教。 试一下图片咋放。2333。]]></content>
  </entry>
</search>
