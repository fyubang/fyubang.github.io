<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[【论文笔记】MT-DNN]]></title>
    <url>%2F2019%2F05%2F23%2Fmt-dnn%2F</url>
    <content type="text"><![CDATA[瓦砾想写这篇蛮久的，但由于之前一直忙着搞别的事情（好吧就是懒），一直拖着。刚好最近有用这个方面的需求，就又读了一遍论文和github上的一些实现。 其实作者（Xiaodong Liu）早在15年就写过一篇Multi-task相关的论文，只不过当时还没有bert这样优秀的预训练表达层，在bert横扫各大榜单之后，作者将之前多任务的概念和bert相结合，duang~就出了这一篇在GLUE、SNLI和SciTail创下新的SOTA的论文。 Intuition会滑雪的人，学滑冰要容易的多。（笔者试过，反过来不大成立，手动狗头） Motivation 监督学习需要大量监督数据，但正常情况下咱都是没有的。MTL（multi-task learning）可以提高low-resource任务的表现。 MTL能起到正则的作用，减轻模型对特定任务的过拟合。 bert之类的预训练模型充分利用了无监督数据。MTL作为补充，进一步利用了out-domain的监督数据。 Model模型很简单，看一下这个图： 底层share了bert的表达层，输出层为每个任务设计了各自的输入形式和loss计算方式。 任务和loss计算任务分类及数据GLUE 单句分类（Single-Sentence Classification）： CoLA（Corpus of Linguistic Acceptability）：判断英语句子是否语法正确 SST-2（Stanford Sentiment Treebank）：影评情感分类 文本相似度（Text Similarity）： STS-B（Semantic Textual Similarity Bench-mark）：人类标注的1-5的语义相似度数据集。 对句分类（Pairwise Text Classification）： RTE（Recognizing Textual Entailment）：entailment or not_entailment MNLI（Multi-Genre Natural Language Inference）：entailment，contradiction，neural QQP（Quora Question Pairs）：判断两个问题是否问的是同一内容。 MRPC（Microsoft Research Paraphrase Corpus）：判断是否两个句子是语义相同的。 WNLI（Winograd NLI）：Wino-grad Schema dataset得到的推理任务。 相关性排序（Relevance Ranking）： QNLI（Stanford Question Answering）：问答对数据集。 Out-domain SNLI（Stanford Natural Language Inference）：Flickr30里人工标注了hypotheses的推理数据集 SciTail（Science Question Answering Textual Entailment）：科学问题的推理，更难。 loss计算1. 单句分类交叉熵：$$P_r(c|X) = \text{softmax}(W^T_{SST}\cdot x)\\L(\Theta) = -\sum_c{\mathbb{I}(X, c) \log (P_r(c|X)) }$$ 2. 文本相似度均方误差：$$\text{Sim}(X_1, X_2) = \text{sigmoid}(w^T_{STS}\cdot x)\\L(\Theta) = (y-\text{Sim}(X_1, X_2))^2$$ 3. 对句分类前面比较常见，这个对句分类作者处理的方式比较特殊，用了18年作者自己提出的一种叫 SAN（stochastic answer network）的输出层构建方式，推理过程有点繁琐，给大家贴个图。 注意图中的$m$，$n$ 都是sequence length。 总结起来就是，作者得到query和premise分别的token-wise的表达之后，在他们两个之间做了一个attention，然后开辟了一个新的状态维度做RNN，从而得到多次预测结果，再做平均（类似于人推理时，多次思考才能得到最终的判断）。作者在后面证明了 SAN 结构能带来0.1%~0.5%的提升。 loss也是交叉熵：$$L(\Theta) = -\sum_c{\mathbb{I}(X, c) \log (P_r(c|X)) }$$ 4. 相关性排序作者的这个loss设计还是挺有意思的，不用简单的二分类来做这个任务，而是用learning2rank的范式，对于每个query $Q$ 采样 $N$ 个candidates，其中$A^+$是正确答案，其他的都是是错误答案。 负对数似然：$$\text{Rel}(Q,A)=\text{sigmoid}(w^T_{QNLI} \cdot x)\\L(\Theta)=-\frac{\text{exp}(\text{Rel}(Q,A^+))}{\sum_{A’\in{\mathcal{A}}}\text{exp}(Rel(Q,A’))}$$ 训练过程 训练过程就是把所有数据合并在一起，每个batch只有单一任务的数据，同时会带有一个task-type的标志，这样模型就知道该走哪一条loss计算的路径。 论文里并没有提及对于单个任务，之后还要不要再单独Fine-tune一下，不过参考github的FAQ，再FT一下，结果会更好。 实验实现细节Optimizer：Adamax（这个地方跟bert不太一样） lr：5e-5 batch size：32 max_num of epochs：5 SAN steps：5 warm-up：0.1 clip gradient norm：1 max seq length：512 GLUE结果 从Table 2 可以看出来，MT-DNN在每一项都超过了bert，而且数据越少的任务，提升越明显，对于QQP和MNLI来说，提升就没那么明显了。 Table 3中的ST-DNN名字很玄乎，其实与bert不同的就是用了文中的复杂了一点的输出模块和loss的设计，比如SAN，learning2rank这些，单独训练各个任务。可见都是有一定程度的提升。所以MT-DNN相对于bert的提升其实来自于 multi task 和 special output module 两个部分。 SNLI 和 SciTail 结果 在得到mult-task训练后的ckpt后，用这个weights去fine tune新的任务，结果和GLUE的保持一致，都有提升，且小数据集任务的提升更明显。 Domain 适应性结果这个结果比较有趣，笔者认为也是比较重要的点，MT-DNN得到的weights相对于bert的weights能在很少的数据下达到不错的效果，且数据越少，相对bert的提升就越大。(甚至23个训练样本就能达到82.1%的准确率，amazing啊。) Conclusion打个总结： MT-DNN的优点： 数据要求少 泛化能力强，不容易过拟合 MT-DNN的缺点： 实用性：实际应用中也许并不能找到特别合适的，且高质量的多任务 训练慢啊，MT-DNN作者用了4张v100，普通业务要不起这个条件，所以MT-DNN的定位其实类似于bert，训练好了就别乱动了，当pretrain-model用。 Further work 更深度的share weights 更有效的训练方法 用更可控的方式融入文本的语言结构（这点个人感觉不适用于现在大刀阔斧搞预训练模型的情况）]]></content>
      <categories>
        <category>NLP</category>
        <category>论文笔记</category>
        <category>多任务</category>
      </categories>
      <tags>
        <tag>多任务</tag>
        <tag>bert</tag>
        <tag>迁移学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【比赛】百度2019信息抽取比赛（有监督的三元组抽取）]]></title>
    <url>%2F2019%2F05%2F21%2Flic_ie%2F</url>
    <content type="text"><![CDATA[瓦砾最近参加了百度的信息抽取比赛，收获颇多，所以借由这个契机，在这给大家分享一下有监督信息抽取的一些方案。 信息抽取指的是从自然语言文本中抽取出结构化的信息知识，以便于存储、管理及使用，一般用三元组的方式来表示这种结构化的知识。由于一段文本往往包含了多个三元组，所以信息抽取任务可以理解成从一段自然文本输出一个有向图（节点为实体，边为关系）的过程。 信息抽取是构建知识图谱的基础，同时也能服务于检索，问答，对话等任务。与去年火爆NLP圈的阅读理解任务不同，信息抽取似乎还没能有一个较为官方和权威的监督数据集。这次百度提出的SKE数据集也是中文最大的相关数据集（大概20万的句子，40万的三元组）。 本文专注于讨论三元组抽取的监督学习部分。如果想了解其他无监督，远监督的方法，请移步 徐阿衡的博客。 竞赛任务 给定 schema 约束集合及句子 sent，其中 schema 定义了关系P以及其对应的主体S和客体O的类别，例如（S_TYPE:人物，P:妻子，O_TYPE:人物）、（S_TYPE:公司，P:创始人，O_TYPE:人物）等。 任务要求参评系统自动地对句子进行分析，输出句子中所有满足schema约束的SPO三元组知识 Triples=[(S1, P1, O1), (S2, P2, O2)…]。 输入/输出: (1) 输入:schema约束集合及句子sent (2) 输出:句子sent中包含的符合给定schema约束的三元组知识Triples 举例： 输入：《鬼影实录2》是托德·威廉姆斯执导，布赖恩·波兰德主演的恐怖片。 输出：（’鬼影实录2’, ‘主演’, ‘布赖恩·波兰德’） （’鬼影实录2’, ‘导演’, ‘托德·威廉姆斯’） 数据集介绍 50个schema的形式如图，需要注意的是，50个schema中有一个【成立日期】关系存在【机构】和【企业】两个主语类型，后期官方合并了这两个，所以其实是49个schema。 任务分析与一般的单句单三元组不同，该数据集每个样本有近一半是多三元组的。另外，这次的数据集主要来源于百度百科的infobox，所以本数据集的最大的特点是单核心词，多属性宾语。依旧举个栗子： 输入文本： 周杰伦（Jay Chou），1979年1月18日出生于台湾省新北市，毕业于淡江中学，中国台湾流行乐男歌手、音乐人、演员、导演、编剧等。 Infobox如下： 输出为三元组即为： （周杰伦，出生日期，1979年1月18日） （周杰伦，出生地，台湾省新北市） （周杰伦，毕业院校，淡江中学） 数据集中90%的数据都是一对一，或是这种一对多的形式。剩下的10%的数据相对就比较复杂了，可以理解成是一个有向图，其中，节点为文本中的实体，边为50个schema中的关系。 方案监督的深度学习方法可以分为两类： Pipeline：把实体识别和关系分类作为两个完全独立的过程，不会相互影响，关系的识别依赖于实体识别的效果。 Joint Model：实体识别和关系分类的过程融合在一个模型里。 Pipeline刚接触这个比赛的时候，第一直觉是分成实体识别（NER）和关系抽取（RE）两个任务做。看了一些论文和综述，确实最早都是这么做的，但很奇怪的是，之前的研究似乎都想当然地当成了两个独立的问题做，在已知NER结果的情况下做RE，而且大多并没有考虑多三元组的情况，所以对我们的这个任务帮助不大。这里不花太多的篇幅介绍。 NER的做法相对简单成熟，这里不赘述，重点讲一下RE的问题定义。 一般的做法是：假设主语宾语及其对应的类型已经找到，和句子一起输入到模型中，输出这两个输入实体之间的关系，实际上是做了个多标签分类（考虑到两个实体可能有多种关系），另外加一个【Nan】表示无关系。 这种Pipeline的方案显然有两个问题： 分步来做，会不可避免的引起 Error Propagation。 NER抽取出多个结果时，如何组合这些结果成对输入的问题：假设抽到了n个实体，主语宾语允许重叠的话，理论上需要尝试 $n^2​$ 个组合，不仅inference的时候会造成极大的计算成本，而且也会影响结果表现。 考虑到上面inference时候需要遍历组合，如何构建Nan的负样本，也是个头疼的问题。 最后，抱着不见黄河心不死的心态，我们团队还是实现了一版Pipeline的方案，最后的结果大概线上0.82左右，不算太差。但是，由于提升过于麻烦，加上衔接两个step的代码过于繁琐，最后还是弃了这个坑Orz。 Joint Model下面讲一下三元组抽取的 Joint Model 解决方案综述。 NovelTagging CopyRE Multihead sujianlin]]></content>
      <categories>
        <category>NLP</category>
        <category>比赛</category>
        <category>信息抽取</category>
      </categories>
      <tags>
        <tag>比赛</tag>
        <tag>信息抽取</tag>
        <tag>三元组</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【论文笔记】Attention is all you need]]></title>
    <url>%2F2019%2F01%2F27%2Ftransfomer%2F</url>
    <content type="text"><![CDATA[今年的NLP界被BERT整的明明白白，其中的基本结构 Transformer 一定要了解一下。 Abstract：一般来说，重要的sequence transduction模型都是基于包含encoder，decoder的复杂的rnn和cnn的。最好的模型是通过一个attention机制来连接encoder，decoder。比较普通！我们提出一个只靠attention的。叫Transformer，跟cnn，rnn完全没关系，很炫酷。 在两个翻译任务实验证明：我们的模型又快又好！！！ 高了2个BLEU； 用8个gpu训练3.5天，结果直接超过了当前的SOTA。 我们还证明，Transformer泛化性能贼好，在parsing上大小数据都比别的好 。 划重点：只靠Attention。 1. IntroductionRNN，LSTM，GRU在翻译和 LM（language model）领域搞了很多SOTA，很多研究花了很多心思在push Encoder-Decoder和Recurrent Language Model的边界。 可是，RNN的天性决定了训练的时候并行性差。尤其对长的sequence，内存限制影响batch examples了。很烦！有一些通过分解tricks和条件计算来提高efficiency的related work，后者也提高performance。但是问题依然存在。 Attention机制现在几乎干啥都必须了，可以不顾输入输出的distance地去model依赖。但是，除了极少cases，attention基本都和rnn绑定在了一起，很不机智！ So，咱提出一种不靠rnn，只靠attention的！！！并行性刚刚的，8个p100花了12 hours就达到翻译的new sota。 2. Background一些相关的研究都用了cnn来减少sequence计算。 这些模型里operations的数量随着输入输出距离的线性（convS2S）或是指数级（ByteNet）地增长。使得很难学习到较远的dependency。 在Transformer里，这是一个常数级的操作，虽然是以牺牲一定精度为代价，但是我们用Multi-Head Attention来抵消了，效果很好！ Self-attention，通过relate一个single sequence的不同位置来计算seq的表征。成功用在阅读理解，summarization等等。 基于attention的end2end的memory网络在很多简单QA和LM问题上比seq-aligned rnn（就是s2s吧）要好。但是啊，Transformer是第一个只靠self-attention来计算表征的。下面介绍一波Transformer！ 3. Model Architecture 其实，老瓦看到这个模型的一瞬间，心情是复杂，看着好简单，可是咋和以前那种有个序列，清清楚楚写着$(x_1, x_2, …, x_n)$什么的模型不太一样？不慌，老瓦来盘一盘。 3.1 Encoder and Decoder StacksEncoder：Encoder是由N=6相同的层组成的栈，每层都有两个子层。第一子层是一个multi-head self-attention mechanism（多头自注意力机制），第二子层是一个position-wise的全连接前馈层。每个子层后面都加了一个residual connection（冗余连接）+layer normalization（层正则），也就是说每个子层的输出都是 $\text{LayerNorm}(x+\text{Sublayer}(x))$ 。为了方便做加运算，所有子层的输出维度都是$d_{model}=512$。 Decoder：Decoder也是N=6层的。主要两个区别： 增加了一个子层，将encoder的输出当做输入。 修改了decoder栈的自注意力自层，来防止位置们去关注后续的位置。masking结合output的embedding都右移了一位这个事实，保证了位置i的预测只依赖于比i小的已知位置。 3.2 Attention既然文章名字叫Attention is all you need，attention的结构当然是其中的重中之重，理解了Attention就几乎理解了文章的一大半。 3.2.1 Basic Attention首先，得知道啥是attention。14年Sutskever大神祭出seq2seq之后，紧跟着Bahdanau和Luong就发了两篇attention用在seq2seq的论文，名字也好记，一个叫Bahdanau attention，一个叫Luong attention。这里以Bahdanau Attention为例，讲一讲计算attention的基本套路。 其实，attention机制和普通seq2seq的不同就是，要计算出一个包含上下文信息的context vector作为decoder每个位置的输入。计算attention时，一般有Query(Q)，Key(K)，Value(V) 三个输入。在上面这张图上，Q就是 $(s_0, s_1, … s_m)$ ，K和V就是$(h_0, h_1, …, h_n)$。attention机制一般的套路就是，用Q和K先算出一个权重的向量，再用这个权重的向量去element-wise地乘上V，就能得到Context Vector：$$ConVec(Q, K, V) = softmax(score(Q, K))V$$ 3.2.2 Scaled Dot-Product Attention明白了attention的套路，我们来看看论文里的attention是什么来头，先看看原文中这张无比清晰（但是看起来有点唬人）的图。 这又是Scaled Dot-Product又是Multi-Head的，一开始着实让老瓦感到有点慌，后来仔细一看，其实挺简单。 看过Luong那篇attention的文章的人都知道，score一般有三种算法：$$score(h_t, \overline{h}_s) =\begin{cases}h_t^{\top}\overline{h}_s &amp;dot \\h_t^{\top}W_a\overline{h}_s &amp;general\\v_a^{\top}tanh(W_a[h_t;\overline{h}_s]) &amp;concat\end{cases}$$其实文章里用的就是第一种，因为性价比高（效果还不错速度快），但是有个缺点当Q，K的维度比较大的时候，容易进到softmax的饱和区，作者就scale了一下（除以$\sqrt{d_k}$），解决了这个问题，这个就是所谓的Scaled Dot-Product Attention。粗暴有效。$$Attention(Q, K, V) = softmax(\frac{QK^{\top}}{\sqrt{d_k}})V$$基本上就完事了。 老瓦在看论文的时候一直不明白mask（只有在Decoder的input用到了）到底是具体怎么操作的，其实作者在后面有解释，就是把所有的非法连接的score都设置成负无穷，这样的softmax之后得到的权重向量就是零了($e^{-\infty}$)。完事。 3.2.3 Multi-Head Attention这下就讲到精髓了：多头注意力（不知道这样翻译会不会被打。。。）。可以这么理解，有好多人对attention权重的看法不太一样，所以我们就把这个任务给很多人一起做，最后取大家的共同意见，有点像CNN里好多个kernel的味道。 文章表示，比起直接用$d_{model}$的Q, K, V来说，将Q, K, V用不同的h个线性投影得到的h个$d_v$的context vector，再concat起来，过一个线性层的结果更好，可以综合不同位置的不同表征子空间的信息。$$\text{Multihead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, …, \text{head}_h)W^O\\\text{where head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$其中，$W_i^Q\in\mathbb{R}^{d_{model}\times d_k}$，$W_i^K\in\mathbb{R}^{d_{model}\times d_k}$，$W_i^V\in\mathbb{R}^{d_{model}\times d_v}$，然后$W^O\in\mathbb{R}^{hd_v\times d_{model}}$。 在文章里，设置了h=8个平行注意层（也就是头（head）2333）。对于每个层的$d_k=d_v=d_{\text{model}}/h=64$。因为每个头都减少了dimension，所以整体的computational cost和single-head full dimension的注意力机制是差不多的。 3.2.4 Applications of Attention in our model encoder-decoder attention: 模仿seq2seq模型的注意力机制 encoder 的 self-attention layer decoder 的self-attention layer：和上面的区别就是加了masking 3.3 Position-wise Feed-Forward Networks每个FFN包括两次线性变换，中间是ReLu的激活函数。$$\text{FFN} = \max(0, xW_1+b_1)W_2+b_2$$不同position的FFN是一样的，但是不同层是不同的。输入输出维度都是$d_{model}=512$，中间层的维度是$d_{ff}=2048$。 3.4 Embeddings and Softmax和其他seq transduction模型一样，也得用learned embeddings，learned 线性变换，softmax这些东西。两个embedding的权重是share的。embedding层，会把权重乘$\sqrt{d_{model}}​$。 3.5 Positional Encoding因为模型没有rnn或者cnn，为了用到sequence的顺序，作者引入了positional encoding（$dim=d_{model}$，便于相加）来inject一些相对位置的信息。$$PE(pos, 2i) = sin(pos/10000^{2i/d_{model}})\\PE(pos, 2i+1) = cos(pos/10000^{2i/d_{model}})$$作者测试用学习的方法来得到PE，最终发现效果差不多，所以最后用的是fixed的，而且sinusoidal的可以处理更长的sequence的情况。 用sinusoidal函数的另一个好处是可以用前面位置的值线性表示后面的位置。$$\sin(\alpha+\beta) = \sin\alpha\cos\beta+\cos\alpha\sin\beta\\\cos(\alpha+\beta) = \cos\alpha\cos\beta-\sin\alpha\sin\beta$$ 4. Why Self-Attention 之所以选择self-attention，主要因为三点： 每层的computational complexity； 可以被parallelize的计算量； 网络中long-range dependencies直接的path length（越短越能方便学到 long-range dependencies）。 5. Conclusion优点： 抛弃了RNN和CNN，提出了Transformer，算法的并行性非常好； Transformer的设计最大的带来性能提升的关键是将任意两个单词的距离是1，有效地解决了long dependency的问题。 缺点： Transformer不像CNN那样可以抽取局部特征，RNN + CNN + Transformer的结合可能会带来更好的效果； 位置信息其实在NLP中非常重要，Transformer中用的Position Embedding也不是一个最终的解决方案。]]></content>
      <categories>
        <category>NLP</category>
        <category>论文笔记</category>
        <category>Attention</category>
      </categories>
      <tags>
        <tag>Attention</tag>
        <tag>Transformer</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【论文笔记】A User Simulator for Task-Completion Dialogues]]></title>
    <url>%2F2019%2F01%2F16%2Fuser-simulator%2F</url>
    <content type="text"><![CDATA[基本框架（包含对话系统）： Abstract：做任务型bot的时候，强化学习（RL）很强，但是有一些困难： 需要和环境互动，已有的完整对话训练数据没用； 每个不同的任务都需要各自领域的标注数据； 收集人人对话或者人机对话需要领域知识。 但是啊： ====&gt;建造数据库又贵又花时间 ====&gt;只好模拟 ====&gt;user simulator诞生 ====&gt;Bot（agent）先用simulator去训练，搞定了simulator就可以上线，持续online learning Introduction:对话系统一般如 Figure 1. 所示。Dialongue Policy (DP)是一个任务型bot的核心。 一般来说传统的Dialogue Policy（DP）使用规则编程的，但有缺点： 对于复杂系统，难以设计 最优的policy也会变化，不好维护。因此，一般用强化学习训练DP。 为啥要user simulator ? Supervised Learning（SL）监督学习在任务型的bot里不行： 需要专家来标注大量数据； 大量专业的领域知识需要大量的数据来训练； 即使有大量训练数据，还是会有对话空间没有被搜索到。 相反的，RL很吊，不需要专家生成的数据，给一个奖励信号，agent就可以通过交互优化DP。 但是不幸的是：RL需要从环境来的很多sample，所以和真实用户从零开始训练不实际。 ====&gt; 所以需要User Simulators！ 一般的操作是：先在simulator上训练出一个比较好的效果，再部署到真实场景中，持续online learning。 Related Work很难判断user simulator好不好，没有Metric来判定，so没有标准方法，放手乱做。user simulation主要有这么几个类型： 从粒度上分： 在对话行为（dialogue-act）上进行操作的； 在对话文本（utterance）进行操作的 从方法的角度讲： 基于规则的（rule-based） 基于模型的（model-based） 以前的 bi-gram 模型 $P(a_u |a_m)$，基于上一个系统行为 $a_m$ 去预测下一个用户行为 $a_u$。这就很傻。(user有可能改goal，看的信息太少) 后面两个办法来处理： 看更长的对话历史 把user goal 放到user state modeling中 seq2seq端到端解决闲聊可以，任务型不太行。 本文用的叫 agenda-based user simulation 的架构，类似栈的结构通过进栈出栈来 model 状态转移和用户行为生成。很方便，显性encode了对话历史和用户目标（user goal）。 总结：文章 结合了rule-based和model-based 的方法: 在dialog-act level，用了agenda-based (rule)的方法； 在nlg部分，用了seq2seq (model)的方法。 Task-completion的对话系统（Dialogue system）任务型对话系统(以订票bot为例)，通过nl交互，去获取客户期望的信息，最终实现订票。以： 是否订票成功 是否电影满足要求 为标准，输出一个二进制结果，success or failure，评估系统。 数据：用Amazon Mechanical Turk（众包平台）收集的数据，内部标注，11个intent（i.e., inform，request，confirm-question，confirm-answer，etc），29个slot（i.e., movie-name，start-time，theater，numberofpeople，etc）。 一共标注了280个对话，对话平均11轮。 User SimulatorUser GoalTC Bot的user simulator第一步是生成user goal。agent不知道user goal，但是要帮助user来完成他的goal。user goal的定义分两个部分： inform_slots: 包含了constraints（C） request_slots: unk（R） 又可以分成： 必须有的slots（required slots） 选择有的slots（optional slots，i.e., ticket就是request slots 里的必须项）。 Goal是在labeled dataset里生成的，两个机制： 在第一个用户轮提取所有的slots。对于所有的slots，在所有的用户轮里提取第一次出现的。 每次跑对话的时候，就先sample一个user goal。 User Action第一轮的action sampling：要加一些限制（比如通常是request turn，至少一个inform slot，movie_name必须在，等等）。 如果不用NLG，NLU的话，还要加入噪声来模拟NLG和NLU过程产生的噪音，去训练DM部分。 Dialogue Status三个对化状态： no_outcome_yet success failure 具体情况具体讨论。 NLUIOB-format slots tags + Intent tags 最后的hidden layer判断 intent。 NLG基于Template的NLG：dialog-act 被found在模板中的，套模板句型。基于Model的NLG：没发现的，用model生成。（这一点感觉很扯，都没见过，咋生成，数据量肯定不够啊。） UsagesTask-completing Dialogue Setting：任务型Bot（订票），衡量 agent的Metrics为：1. success rate；2. average reward；3. average turns。KB-InfoBot（简单点，agent和user都只有request和inform）：问答Bot（电影信息） DiscussionRule-based的 user simulation 很safe，但是很耗时，因为要手动制定各种规则。两个优化方向： 包含user goal的改变（已做）实现Model-based 的simulator，优点是泛化性能好，缺点是1. 需要大量数据， 2. 万一有漏洞，RL agent 会抓住这个漏洞，假假的成功，你以为success了，其实都是RL agent抓住了loophole给你的假象。 总结：（根据《Agenda-Based User Simulation for bootstrapping a POMDP Dialogue System》by J. Schatzmann） 语义层的 User Simulation人机对话可以看成是状态转移（state transition）和对话行为（Dialog）的序列：用户根据状态 S (或加上机器人的 $a_m$，第一轮可能没有$a_m$)，采取行动 $a_u$, 把状态转移到$S’$。收到agent行动 $a_m$，再根据 $S’$ ，再把状态转移到 $S’’$。这个user行为可以被分解为三个模型：$P(a_u|S)$ 行为选择，$P(S’|a_u, S)$ 状态转移（用户行为），$P(S’’|a_m,S’)$ 状态转移（系统行为）。 基于Goal和Agent的状态表示用户状态（User State）由用户目标（Goal）及议程（Agenda）构成。 Goal由Constraint（C）（比如要市中心的啤酒酒吧）和Request（R）（比如酒吧电话是多少）构成。 Agenda是一个类似栈的结构，包含了在排队中的，用来引出目标中明确的信息的用户行为。在对话开始的时候，用数据库随机生成一个新的Goal。然后，从Goal里的内容来初始化Agenda，把Goal中的 C 都convert成 Inform动作，把Goal中的 R 都convert成 Request动作，再在最后加一个bye动作。有新的 $a_m​$ 的时候，新的user acts 会被压进栈，不需要的会被出栈。具体实现方法（框架和数据结构），后面分析miulab的simulator源码的时候会写。 上图描述了Agenda随着$a_m$，$a_u$的状态转移过程（进栈和出栈）。 上图描述了无NLU和NLG的训练。]]></content>
      <categories>
        <category>NLP</category>
        <category>论文笔记</category>
        <category>Bot</category>
      </categories>
      <tags>
        <tag>Chatbot</tag>
        <tag>User Simulator</tag>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前言]]></title>
    <url>%2F2019%2F01%2F11%2Ffirst-post%2F</url>
    <content type="text"><![CDATA[欢迎来到瓦特兰蒂斯。笔者是一个相信亚特兰蒂斯曾经存在的五岁抬头团—aka瓦砾。 不定期更新NLP的干货，刚开始写博客，希望自己能坚持下去，请大家多多指教。 给大家看一张俺和媳妇的美照：]]></content>
  </entry>
</search>
