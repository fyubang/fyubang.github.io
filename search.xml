<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[【论文笔记】Attention is all you need]]></title>
    <url>%2F2019%2F01%2F27%2Ftransfomer%2F</url>
    <content type="text"><![CDATA[今年的NLP界被BERT整的明明白白，其中的基本结构 Transformer 一定要了解一下。 Abstract：一般来说，重要的sequence transduction模型都是基于包含encoder，decoder的复杂的rnn和cnn的。最好的模型是通过一个attention机制来连接encoder，decoder。比较普通！我们提出一个只靠attention的。叫Transformer，跟cnn，rnn完全没关系，很炫酷。 在两个翻译任务实验证明：我们的模型又快又好！！！ 高了2个BLEU； 用8个gpu训练3.5天，结果直接超过了当前的SOTA。 我们还证明，Transformer泛化性能贼好，在parsing上大小数据都比别的好 。 划重点：只靠Attention。 1. IntroductionRNN，LSTM，GRU在翻译和 LM（language model）领域搞了很多SOTA，很多研究花了很多心思在push Encoder-Decoder和Recurrent Language Model的边界。 可是，RNN的天性决定了训练的时候并行性差。尤其对长的sequence，内存限制影响batch examples了。很烦！有一些通过分解tricks和条件计算来提高efficiency的related work，后者也提高performance。但是问题依然存在。 Attention机制现在几乎干啥都必须了，可以不顾输入输出的distance地去model依赖。但是，除了极少cases，attention基本都和rnn绑定在了一起，很不机智！ So，咱提出一种不靠rnn，只靠attention的！！！并行性刚刚的，8个p100花了12 hours就达到翻译的new sota。 2. Background一些相关的研究都用了cnn来减少sequence计算。 这些模型里operations的数量随着输入输出距离的线性（convS2S）或是指数级（ByteNet）地增长。使得很难学习到较远的dependency。 在Transformer里，这是一个常数级的操作，虽然是以牺牲一定精度为代价，但是我们用Multi-Head Attention来抵消了，效果很好！ Self-attention，通过relate一个single sequence的不同位置来计算seq的表征。成功用在阅读理解，summarization等等。 基于attention的end2end的memory网络在很多简单QA和LM问题上比seq-aligned rnn（就是s2s吧）要好。但是啊，Transformer是第一个只靠self-attention来计算表征的。下面介绍一波Transformer！ 3. Model Architecture 其实，老瓦看到这个模型的一瞬间，心情是复杂，看着好简单，可是咋和以前那种有个序列，清清楚楚写着$(x_1, x_2, …, x_n)$什么的模型不太一样？不慌，老瓦来盘一盘。 3.1 Encoder and Decoder StacksEncoder：Encoder是由N=6相同的层组成的栈，每层都有两个子层。第一子层是一个multi-head self-attention mechanism（多头自注意力机制），第二子层是一个position-wise的全连接前馈层。每个子层后面都加了一个residual connection（冗余连接）+layer normalization（层正则），也就是说每个子层的输出都是$\text{LayerNorm}(x+\text{Sublayer}(x))$。为了方便做加运算，所有子层的输出维度都是$d_{model}=512$。 Decoder：Decoder也是N=6层的。主要两个区别： 增加了一个子层，将encoder的输出当做输入。 修改了decoder栈的自注意力自层，来防止位置们去关注后续的位置。masking结合output的embedding都右移了一位这个事实，保证了位置i的预测只依赖于比i小的已知位置。 3.2 Attention既然文章名字叫Attention is all you need，attention的结构当然是其中的重中之重，理解了Attention就几乎理解了文章的一大半。 3.2.1 Basic Attention首先，得知道啥是attention。14年Sutskever大神祭出seq2seq之后，紧跟着Bahdanau和luong就发了两篇attention用在seq2seq的论文，名字也好记，一个叫Bahdanau attention，一个叫Luong attention。这里以Bahdanau Attention为例，讲一讲计算attention的基本套路。 其实，attention机制和普通seq2seq的不同就是，要计算出一个包含上下文信息的context vector作为decoder每个位置的输入。计算attention时，一般有Query(Q)，Key(K)，Value(V) 三个输入。在上面这张图上，Q就是$(s_0, s_1, … s_m)​$，K和V就是$(h_0, h_1, …, h_n)​$。attention机制一般的套路就是，用Q和K先算出一个权重的向量，再用这个权重的向量去element-wise地乘上V，就能得到Context Vector：$$ConVec(Q, K, V) = softmax(score(Q, K))V$$ 3.2.2 Scaled Dot-Product Attention明白了attention的套路，我们来看看论文里的attention是什么来头，先看看原文中这张无比清晰（但是看起来有点唬人）的图。 这又是Scaled Dot-Product又是Multi-Head的，一开始着实让老瓦感到有点慌，后来仔细一看，其实挺简单。 看过Luong那篇attention的文章的人都知道，score一般有三种算法：$$score(h_t, \overline{h}_s) =\begin{cases}h_t^{\top}\overline{h}_s &amp;dot \h_t^{\top}W_a\overline{h}_s &amp;general\v_a^{\top}tanh(W_a[h_t;\overline{h}_s]) &amp;concat\end{cases}$$其实文章里用的就是第一种，因为性价比高（效果还不错速度快），但是有个缺点当Q，K的维度比较大的时候，容易进到softmax的饱和区，作者就scale了一下（除以$\sqrt{d_k}​$），解决了这个问题，这个就是所谓的Scaled Dot-Product Attention。粗暴有效。$$Attention(Q, K, V) = softmax(\frac{QK^{\top}}{\sqrt{d_k}})V$$基本上就完事了。 老瓦在看论文的时候一直不明白mask（只有在Decoder的input用到了）到底是具体怎么操作的，其实作者在后面有解释，就是把所有的非法连接的score都设置成负无穷，这样的softmax之后得到的权重向量就是零了($e^{-\infty}$)。完事。 3.2.3 Multi-Head Attention这下就讲到精髓了：多头注意力（不知道这样翻译会不会被打。。。）。可以这么理解，有好多人对attention权重的看法不太一样，所以我们就把这个任务给很多人一起做，最后取大家的共同意见，有点像CNN里好多个kernel的味道。 文章表示，比起直接用$d_{model}$的Q, K, V来说，将Q, K, V用不同的h个线性投影得到的h个$d_v$的context vector，再concat起来，过一个线性层的结果更好，可以综合不同位置的不同表征子空间的信息。$$\text{Multihead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, …, \text{head}_h)W^O\\text{where head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$其中，$W_i^Q\in\mathbb{R}^{d_{model}\times d_k}$，$W_i^K\in\mathbb{R}^{d_{model}\times d_k}$，$W_i^V\in\mathbb{R}^{d_{model}\times d_v}$，然后$W^O\in\mathbb{R}^{hd_v\times d_{model}}$。 在文章里，设置了h=8个平行注意层（也就是头（head）2333）。对于每个层的$d_k=d_v=d_{\text{model}}/h=64$。因为每个头都减少了dimension，所以整体的computational cost和single-head full dimension的注意力机制是差不多的。 3.2.4 Applications of Attention in our model encoder-decoder attention: 模仿seq2seq模型的注意力机制 encoder 的 self-attention layer decoder 的self-attention layer：和上面的区别就是加了masking 3.3 Position-wise Feed-Forward Networks每个FFN包括两次线性变换，中间是ReLu的激活函数。$$\text{FFN} = \max(0, xW_1+b_1)W_2+b_2$$不同position的FFN是一样的，但是不同层是不同的。输入输出维度都是$d_{model}=512$，中间层的维度是$d_{ff}=2048$。 3.4 Embeddings and Softmax和其他seq transduction模型一样，也得用learned embeddings，learned 线性变换，softmax这些东西。两个embedding的权重是share的。embedding层，会把权重乘$\sqrt{d_{model}}$。 3.5 Positional Encoding因为模型没有rnn或者cnn，为了用到sequence的顺序，作者引入了positional encoding（$dim=d_{model}$，便于相加）来inject一些相对位置的信息。$$PE(pos, 2i) = sin(pos/10000^{2i/d_{model}})\PE(pos, 2i+1) = cos(pos/10000^{2i/d_{model}})$$作者测试用学习的方法来得到PE，最终发现效果差不多，所以最后用的是fixed的，而且sinusoidal的可以处理更长的sequence的情况。 用sinusoidal函数的另一个好处是可以用前面位置的值线性表示后面的位置。$$\sin(\alpha+\beta) = \sin\alpha\cos\beta+\cos\alpha\sin\beta\\cos(\alpha+\beta) = \cos\alpha\cos\beta-\sin\alpha\sin\beta$$ 4. Why Self-Attention 之所以选择self-attention，主要因为三点： 每层的computational complexity； 可以被parallelize的计算量； 网络中long-range dependencies直接的path length（越短越能方便学到 long-range dependencies）。 5. Conclusion优点： 抛弃了RNN和CNN，提出了Transformer，算法的并行性非常好； Transformer的设计最大的带来性能提升的关键是将任意两个单词的距离是1，有效地解决了long dependency的问题。 缺点： Transformer不像CNN那样可以抽取局部特征，RNN + CNN + Transformer的结合可能会带来更好的效果； 位置信息其实在NLP中非常重要，Transformer中用的Position Embedding也不是一个最终的解决方案。]]></content>
      <categories>
        <category>NLP</category>
        <category>Attention</category>
      </categories>
      <tags>
        <tag>Attention</tag>
        <tag>Transformer</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【论文笔记】A User Simulator for Task-Completion Dialogues]]></title>
    <url>%2F2019%2F01%2F16%2Fuser-simulator%2F</url>
    <content type="text"><![CDATA[基本框架（包含对话系统）： Abstract：做任务型bot的时候，强化学习（RL）很强，但是有一些困难： 需要和环境互动，已有的完整对话训练数据没用； 每个不同的任务都需要各自领域的标注数据； 收集人人对话或者人机对话需要领域知识。 但是啊： ====&gt;建造数据库又贵又花时间 ====&gt;只好模拟 ====&gt;user simulator诞生 ====&gt;Bot（agent）先用simulator去训练，搞定了simulator就可以上线，持续online learning Introduction:对话系统一般如 Figure 1. 所示。Dialongue Policy (DP)是一个任务型bot的核心。 一般来说传统的Dialogue Policy（DP）使用规则编程的，但有缺点： 对于复杂系统，难以设计 最优的policy也会变化，不好维护。因此，一般用强化学习训练DP。 为啥要user simulator ? Supervised Learning（SL）监督学习在任务型的bot里不行： 需要专家来标注大量数据； 大量专业的领域知识需要大量的数据来训练； 即使有大量训练数据，还是会有对话空间没有被搜索到。 相反的，RL很吊，不需要专家生成的数据，给一个奖励信号，agent就可以通过交互优化DP。 但是不幸的是：RL需要从环境来的很多sample，所以和真实用户从零开始训练不实际。 ====&gt; 所以需要User Simulators！ 一般的操作是：先在simulator上训练出一个比较好的效果，再部署到真实场景中，持续online learning。 Related Work很难判断user simulator好不好，没有Metric来判定，so没有标准方法，放手乱做。user simulation主要有这么几个类型： 从粒度上分： 在对话行为（dialogue-act）上进行操作的； 在对话文本（utterance）进行操作的 从方法的角度讲： 基于规则的（rule-based） 基于模型的（model-based） 以前的 bi-gram 模型 $P(a_u |a_m)$，基于上一个系统行为 $a_m$ 去预测下一个用户行为 $a_u$。这就很傻。(user有可能改goal，看的信息太少) 后面两个办法来处理： 看更长的对话历史 把user goal 放到user state modeling中 seq2seq端到端解决闲聊可以，任务型不太行。 本文用的叫 agenda-based user simulation 的架构，类似栈的结构通过进栈出栈来 model 状态转移和用户行为生成。很方便，显性encode了对话历史和用户目标（user goal）。 总结：文章 结合了rule-based和model-based 的方法: 在dialog-act level，用了agenda-based (rule)的方法； 在nlg部分，用了seq2seq (model)的方法。 Task-completion的对话系统（Dialogue system）任务型对话系统(以订票bot为例)，通过nl交互，去获取客户期望的信息，最终实现订票。以： 是否订票成功 是否电影满足要求 为标准，输出一个二进制结果，success or failure，评估系统。 数据：用Amazon Mechanical Turk（众包平台）收集的数据，内部标注，11个intent（i.e., inform，request，confirm-question，confirm-answer，etc），29个slot（i.e., movie-name，start-time，theater，numberofpeople，etc）。 一共标注了280个对话，对话平均11轮。 User SimulatorUser GoalTC Bot的user simulator第一步是生成user goal。agent不知道user goal，但是要帮助user来完成他的goal。user goal的定义分两个部分： inform_slots: 包含了constraints（C） request_slots: unk（R） 又可以分成： 必须有的slots（required slots） 选择有的slots（optional slots，i.e., ticket就是request slots 里的必须项）。 Goal是在labeled dataset里生成的，两个机制： 在第一个用户轮提取所有的slots。对于所有的slots，在所有的用户轮里提取第一次出现的。 每次跑对话的时候，就先sample一个user goal。 User Action第一轮的action sampling：要加一些限制（比如通常是request turn，至少一个inform slot，movie_name必须在，等等）。 如果不用NLG，NLU的话，还要加入噪声来模拟NLG和NLU过程产生的噪音，去训练DM部分。 Dialogue Status三个对化状态： no_outcome_yet success failure 具体情况具体讨论。 NLUIOB-format slots tags + Intent tags 最后的hidden layer判断 intent。 NLG基于Template的NLG：dialog-act 被found在模板中的，套模板句型。基于Model的NLG：没发现的，用model生成。（这一点感觉很扯，都没见过，咋生成，数据量肯定不够啊。） UsagesTask-completing Dialogue Setting：任务型Bot（订票），衡量 agent的Metrics为：1. success rate；2. average reward；3. average turns。KB-InfoBot（简单点，agent和user都只有request和inform）：问答Bot（电影信息） DiscussionRule-based的 user simulation 很safe，但是很耗时，因为要手动制定各种规则。两个优化方向： 包含user goal的改变（已做）实现Model-based 的simulator，优点是泛化性能好，缺点是1. 需要大量数据， 2. 万一有漏洞，RL agent 会抓住这个漏洞，假假的成功，你以为success了，其实都是RL agent抓住了loophole给你的假象。 总结：（根据《Agenda-Based User Simulation for bootstrapping a POMDP Dialogue System》by J. Schatzmann） 语义层的 User Simulation人机对话可以看成是状态转移（state transition）和对话行为（Dialog）的序列：用户根据状态 S (或加上机器人的 $a_m$，第一轮可能没有$a_m$)，采取行动 $a_u$, 把状态转移到$S’$。收到agent行动 $a_m$，再根据 $S’$ ，再把状态转移到 $S’’$。这个user行为可以被分解为三个模型：$P(a_u|S)$ 行为选择，$P(S’|a_u, S)$ 状态转移（用户行为），$P(S’’|a_m,S’)$ 状态转移（系统行为）。 基于Goal和Agent的状态表示用户状态（User State）由用户目标（Goal）及议程（Agenda）构成。 Goal由Constraint（C）（比如要市中心的啤酒酒吧）和Request（R）（比如酒吧电话是多少）构成。 Agenda是一个类似栈的结构，包含了在排队中的，用来引出目标中明确的信息的用户行为。在对话开始的时候，用数据库随机生成一个新的Goal。然后，从Goal里的内容来初始化Agenda，把Goal中的 C 都convert成 Inform动作，把Goal中的 R 都convert成 Request动作，再在最后加一个bye动作。有新的 $a_m​$ 的时候，新的user acts 会被压进栈，不需要的会被出栈。具体实现方法（框架和数据结构），后面分析miulab的simulator源码的时候会写。 上图描述了Agenda随着$a_m$，$a_u$的状态转移过程（进栈和出栈）。 上图描述了无NLU和NLG的训练。]]></content>
      <categories>
        <category>NLP</category>
        <category>Bot</category>
      </categories>
      <tags>
        <tag>Chatbot</tag>
        <tag>User Simulator</tag>
        <tag>Reinforcement Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[送给我❤️湘琦]]></title>
    <url>%2F2019%2F01%2F11%2Ffirst-post%2F</url>
    <content type="text"><![CDATA[欢迎来到赵邦邦和符居居的小窝。给大家看一张俺和媳妇的美照：]]></content>
  </entry>
</search>
